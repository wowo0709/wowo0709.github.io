---
layout: single
title: "[AITech] 2022년 1월 21일 학습 내용 정리"
categories: ['AI', 'AITech']
toc: true
toc_sticky: true
tag: ['확률/통계','마스터클래스']
---



<br>

## 강의 복습 내용

### 1. Pandas



<br>

### 2. CNN 기초

#### Convolution

* 일반적인 다층 신경망(MLP)은 각 뉴런들이 선형 모델과 활성 함수로 **모두 연결된(fully connected) 구조**이다. 

* **Convolution 연산**은 이와 달리 **커널(필터)**을 **입력 벡터 상에서 움직여가면서** 선형모델과 함성 함수가 적용되는 구조이다. 

  * 컨볼루션 연산의 수학적 의미는 신호를 **커널을 이용해 국소적으로 증폭 또는 감소**시켜서 정보를 추출 또는 필터링하는 것이다. 

    ![image-20220121111508711](https://user-images.githubusercontent.com/70505378/150459184-30c78040-ff98-4640-b683-466e7aec286c.png)

  * CNN에서 사용하는 연산은 엄밀히 말하면 convolution 연산이 아니고 **cross-correlation** 연산이다. 하지만 그 의미에서 convolution 연산과 큰 차이가 없기 때문에 옛날부터 convolution 연산으로 통칭한다. 

  * 커널은 정의역 내에서 움직여도 변하지 않고(**translation invariant**) 주어진 신호에 국소적(**local**)으로 적용한다. 

  * 컨볼루션 연산은 1차원 뿐 아니라 다양한 차원에서 계산 가능하다. 

    * 데이터의 성격에 따라 사용하는 커널이 달라진다. 

    ![image-20220121111857484](https://user-images.githubusercontent.com/70505378/150459187-30d17340-932f-42dd-971c-8be5a51f555f.png)

* **2D-Conv** 연산은 아래와 같이 나타낼 수 있다. 

  ![image-20220121112045046](https://user-images.githubusercontent.com/70505378/150459189-a4fd7a4d-ac15-407b-af85-04aa6693d073.png)

  * 컨볼루션 연산에서 사용하는 용어로 **스트라이드**와 **패딩**이라는 것이 있다. 
    * **스트라이드**: 컨볼루션 연산을 위해 한 번에 필터(커널)를 이동시키는 칸 수
    * **패딩**: 컨볼루션 결과의 크기를 조정하기 위해 입력 배열의 둘레를 확장하고 0으로 채우는 연산

  ![image-20220121112643098](https://user-images.githubusercontent.com/70505378/150459190-fef4ea77-2a75-406b-9ed2-69d9897d5350.png)

  * 결과적으로 입력의 크기 (OH, OW), 커널(필터)의 크기(FH, FW), 패딩의 폭 P, 스트라이드 크기 S를 안다면 출력의 크기는 다음과 같이 구할 수 있다. 

    ![image-20220121112842880](https://user-images.githubusercontent.com/70505378/150459192-59f8d0da-27c2-48b7-b372-e7f3b25d43b0.png)

* 채널이 여러 개인 **3D-Conv** 이상의 다차원 컨볼루션 연산의 경우 **커널의 채널 수와 입력의 채널 수가 같아야** 한다. (rank가 동일해야 함)

  * 이 경우 3차원 입력과 3차원 커널을 통해 출력의 채널 크기는 1이 되며, 채널의 크기를 Oc로 만들고 싶다면 커널을 Oc개 사용하면 된다. 

  ![image-20220121113425774](https://user-images.githubusercontent.com/70505378/150459193-aa40617d-fa13-45a1-a289-aef42b5e19ce.png)

#### Convolution의 역전파

* 컨볼루션 연산은 커널이 모든 입력 데이터에 공통으로 적용되기 때문에 **역전파를 계산할 때도 convolution 연산**이 나오게 된다. 

![image-20220121114159506](https://user-images.githubusercontent.com/70505378/150459195-87f0536f-3e1c-48d5-9a49-794ea273b650.png)



<br>

### 3. RNN 기초

#### 시퀀스 데이터

* 소리, 문자열, 주가 등의 데이터를 **시퀀스 데이터**로 분류한다. 

* 시퀀스 데이터는 **독립동등분포(i.i.d.)** 가정을 위배하기 때문에 **순서를 바꾸거나 과거 정보에 손실이 발생하면 데이터의 확률분포도 바뀐다.**

* 따라서 이전 시퀀스에 대한 정보를 가지고 앞으로 발생할 데이터의 확률 분포를 계산해야 하며, 이를 위해 조건부 확률을 이용할 수 있다. 

  ![image-20220121114516596](https://user-images.githubusercontent.com/70505378/150459169-72a12f32-2439-4e73-840a-559be2d27ff9.png)

  * 위 조건부 확률은 과거의 모든 정보를 이용하지만, 시퀀스 데이터를 분석할 때 **과거의 모든 정보들이 필요한 것은 아니다.**
    * 어떤 시점까지의 과거의 정보를 이용할 지는 데이터/모델링에 따라 달라진다. 

* 시퀀스 데이터를 다루기 위해서는 **길이가 가변적인 데이터**를 다룰 수 있는 모델이 필요하다. 

  * 이를 해결하기 위해 특정 구간 _tau_만큼의 과거 정보만을 이용하고, 그보다 더 전의 정보들은 **H<sub>t</sub>**라는 잠재변수로 인코딩해서 사용할 수 있다. 
    * 이렇게 함으로써 데이터의 길이를 고정할 수 있고, 과거의 모든 데이터를 활용하기 용이해진다. 

  ![image-20220121115240369](https://user-images.githubusercontent.com/70505378/150459172-8ac5d9c3-3ce5-49d9-9db9-8482e2071342.png)

  * 이 잠재변수 H<sub>t</sub>를 신경망을 통해 반복해서 사용하여 **시퀀스 데이터의 패턴을 학습**하는 모델이 **RNN**이다. 

    <img src="https://user-images.githubusercontent.com/70505378/150459173-7e1479bf-afb5-454a-8336-ef95b1282c8c.png" alt="image-20220121115433935" style="zoom:67%;" />

#### RNN(Recurrent Neural Network)

* 현재 정보만을 입력으로 사용하는 완전연결신경망은 과거의 정보를 다룰 수 없다. 

* RNN은 이전 순서의 잠재변수와 현재의 입력을 활용하여 모델링한다. 

  * W: t에 따라 불변/ X, H: t에 따라 가변

  ![image-20220121115906209](https://user-images.githubusercontent.com/70505378/150459174-3020b45d-4248-4ff1-b8fa-bf36b10fa114.png)

* **RNN의 역전파**는 잠재변수의 연결그래프에 따라 순차적으로 계산한다. (맨 마지막 출력까지 계산한 후에 역전파)

  * 이를 **BPTT(Backpropagation Through Time)**라 하며 RNN의 기본적인 역전파 방식이다. 

  ![image-20220121120052256](https://user-images.githubusercontent.com/70505378/150459175-d0158bca-a493-49b7-9272-adc6d1ca8496.png)

  * BPTT를 통해 RNN의 가중치 행렬의 미분을 계산해보면 아래와 같이 **미분의 곱**으로 이루어진 항이 계산된다. 

    * 그 중 빨간색 네모 안의 항은 불안정해지기 쉽다. 
    * 이는 거듭된 값들의 곱으로 인해 값이 너무 커지거나(기울기 폭발) 너무 작아져(기울기 소실) 과거의 정보를 제대로 전달해주지 못하기 때문이다. 

    ![image-20220121120521892](https://user-images.githubusercontent.com/70505378/150459177-ae598173-a0df-431f-a4a6-538baf34ae44.png)

  * 기울기 폭발/소실 문제를 해결하기 위해 역전파 과정에서 **길이를 끊는 것**이 필요하며, 이를 **TBPTT(Truncated BPTT)**라 한다. 

    ![image-20220121120714788](https://user-images.githubusercontent.com/70505378/150459180-30e736f3-3b17-4191-a09e-85417f3d37b5.png)

* 여러가지 문제로 Vanilla RNN으로는 긴 시퀀스를 처리하는데 한계가 있고, 이를 해결하기 위해 **LSTM**이나 **GRU**와 같은 발전된 형태의 네트워크를 사용한다. 

  ![image-20220121120933072](https://user-images.githubusercontent.com/70505378/150459183-54dfba71-66b6-4bf0-8d54-1c7fbaec5427.png)



<br>

<br>

## 피어세션 정리



<br>

<br>

## 과제 수행 과정/결과물



<br>

<br>

## 학습 회고



















