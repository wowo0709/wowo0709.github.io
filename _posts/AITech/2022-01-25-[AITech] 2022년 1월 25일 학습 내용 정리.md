---
layout: single
title: "[AITech] 2022년 1월 25일 학습 내용 정리"
categories: ['AI', 'AITech']
toc: true
toc_sticky: true
tag: ['파이토치','AutoGrad','DataLoader']
---



<br>

## 학습 내용 정리

### AutoGrad & Optimizer

딥러닝 모델의 구조는 **블록들의 연속**이다. 해당 블록은 하나의 연산을 수행하는 단일 층일 수도 있고, 여러 단일 층들이 모인 하나의 블록일 수도 있다. 

모델의 반복 구조를 설계하기 쉽게 하기 위하여, 파이토치에서는 여러 모듈을 제공한다. 

#### torch.nn.Module

* 딥러닝을 구성하는 Layer의 base class
* **Input, Output, Forward, Backward** 정의
* 학습의 대상이 되는 **parameter** 정의

**nn.Parameter**

* nn.Module 내에 attribute가 될 때는 **required_grad=True**로 지정하여 학습 대상으로 설정
* 하지만 이를 직접 지정해 줄 일은 거의 없다. 
  * 대부분의 layer에는 weights 값들이 지정되어 있음

```python
class MyLinear(nn.Module):
    def __init__(self, in_features, out_features, bias=True):
        super().__init__()
        self.in_features = in_features # 입력 피쳐 개수(입력 노드 개수)
        self.out_features = out_features # 출력 피쳐 개수(출력 노드 개수)
        
        self.weights = nn.Parameter( # 학습 parameter 설정(가중치)
                torch.randn(in_features, out_features))
        
        self.bias = nn.Parameter(torch.randn(out_features)) # 학습 parameter 설정(편향)
        
    def forward(self, x: Tensor):
        return x @ self.weights + self.bias # linear 연산
```

**Backward**

* **Layer에 있는 parameter들의 미분을 수행(그래디언트 값 전달)**
* Loss를 미분한 값으로 parameter 갱신

```python
for epoch in range(epochs):
    ...
    # 이전 그래디언트 값 초기화
    optimizer.zero_grad()
    # 모델 예측 값
    output = model(inputs)
    # 손실 함수 값 계산
    loss = criterion(outputs, labels)
    # 손실 함수 미분, 그래디언트 값 계산
    loss.backward()
    # 파라미터 갱신
    optimizer.step()
```

* backward 함수와 optimizer는 Module 레벨에서 직접 오버라이딩 할 수 있고, 직접 미분 수식을 써야 하므로 실제로 할 일은 거의 없지만 순서를 이해할 필요는 있다. 

```python
class LogisticRegression(nn.Module):
    def __init__(self, dim, lr=torch.scalar_tensor(0.01)):
        super(LR, self).__init__()
        # initialize parameters
        self.w = torch.zeros(dim, 1, dtype=torch.float).to(device)
        self.b = torch.scalar_tensor(0).to(device)
        self.grads = {"dw": torch.zeros(dim, 1, dtype=torch.float).to(device), 
                     "db": torch.scalar_tensor(0).to(device)}
        self.lr = lr.to(device)
        
    def forward(self, x):
        # compute forward
        z = torch.mm(self.w.T, x)
        a = self.sigmoid(z)
        return a
    
    def sigmoid(self, z):
        return 1/(1+torch.exp(-z))
    
    def backward(self, x, yhat, y):
        # 미분 수식을 직접 작성
        self.grads["dw"] = (1/x.shape[1])*torch.mm(x, (yhat-y).T)
        self.grads["db"] = (1/x.shape[1])(torch.sum(yhat-y))
        
    def optimize(self):
        # 파라미터 업데이트
        self.w = self.w - self.lr*self.grads["dw"]
        self.b = self.b - self.lr*self.grads["db"]
```

<br>

### Datasets & DataLoaders

`Dataset`과 `DataLoader` 모듈은 파이토치에서 데이터를 가져와서 모델에 먹이는 일련의 과정을 담당한다. 

![image-20220125115009860](https://user-images.githubusercontent.com/70505378/150939779-edf1891b-0b6f-44a5-bc15-5e494bd0d733.png)

* **Transforms**: 데이터 전처리, 증강, 타입 변환(텐서) 등의 역할
* **Dataset**: 초기화(init) 과정, 데이터 크기, 데이터 인덱싱 방법(mapping style, 어떤 형태로 반환할 지) 등의 역할
* **DataLoader**: 배치 생성, 배치 섞기, 데이터 샘플링 등의 역할

#### Dataset 클래스

* 데이터 입력 형태를 정의하는 클래스
* Image, Text, Audio 등에 따른 다른 입력 정의

```python
import torch
from torch.utils.data import Dataset

class CustomDataset(Dataset):
    def __init__(self, text, labels): # 초기 데이터 생성 방법 지정
        self.labels = labels
        self.data = text
        
    def __len__(self): # 데이터의 전체 길이
        return len(self.labels)
    
    def __getitem__(self, idx): # index 값을 주었을 때 반환 형식
        label = self.labels[idx]
        text = self.data[idx]
        sample = {"Text": text, "Class": label}
        return sample
```

**Dataset 클래스 생성 시 유의점**

* 데이터 형태에 따라 각 함수를 알맞게 정의함
* 모든 것을 데이터 생성 시점에 처리할 필요는 없음
  * Image의 Tensor 변환의 경우 학습이 필요한 시점에 변환
* 데이터 셋에 대한 표준화된 처리방법 제공 필요
* 최근에는 HuggingFace 등 표준화된 라이브러리 사용

#### DataLoader 클래스

* **Data의 Batch를 생성하고 학습 직전 Tensor로의 변환**이 메인 업무
* 병렬적인 데이터 전처리 코드의 고민 필요

```python
text = ["Happy", "Amazing", "Sad", "Unhappy", "Glum"]
labels = ["Positive", "Positive", "Negative", "Negative", "Negative"]

MyDataset = customDataset(text, labels) # Dataset 객체 생성
MyDataLoader = DataLoader(MyDataset, batch_size=2, shuffle=True) # DataLoader 객체 생성
next(iter(MyDataLoader))
# {'Text':['Glum', 'Sad'], 'Class': ['Negative', 'Negative']}

MyDataLoader = DataLoader(MyDataset, batch_size=2, shuffle=True)
for dataset in MyDataLoader:
    print(dataset)
# {'Text':['Glum', 'Sad'], 'Class': ['Negative', 'Negative']}
# {'Text':['Sad', 'Amazing'], 'Class': ['Negative', 'Positive']}
# {'Text': ['Happy'], 'Class': ['Positive']}
```

**DataLoader 파라미터**

* dataset: 데이터를 가져올 Dataset 객체로서 `map-style dataset` 또는 `iterable-style dataset`을 사용할 수 있다. 
  * map-style dataset: `__get_item()__`과 `__len()__` 프로토콜을 작성한 Dataset 객체
  * iterable-style dataset: `__iter__()` 프로토콜을 작성한 IterableDataset 객체
* batch_size, shuffle, sampler, batch_sampler, drop_last
  * sampler: 배치 생성 시 사용할 데이터의 인덱스를 generate하는 `torch.utils.data.Sampler` 객체
  * shuffle: True로 설정 시 자동으로 랜덤하게 데이터의 인덱스를 generate하는 Sampler를 사용한다. 
  * batch_sampler: 배치를 생성하는 방법이 정의된  `torch.utils.data.Sampler` 객체
  * batch_size: 인자 전달 시 batch_size 크기의 미니 배치를 생성
  * drop_last: 전체 데이터 크기와 배치의 크기가 나누어 떨어지지 않는다면 마지막 미니 배치는 버린다. 
  * 이 모든 parameter를 사용하기 위해서는 dataset 으로 map-style dataset을 전달해야 한다. 
* collate_fn: 배치를 생성하기 위한 데이터의 인덱스를 뽑은 후에 합치는(collate) 과정에서 적용할 함수
  * 서로 길이가 다른 sequential data의 길이를 맞춰주기 위한 padding을 할 때 주로 사용한다. 
* num_workers, pin_memory, timeout
  * num_workers: 멀티 프로세스 사용
  * pin_memory: 메모리를 불러올 때 pinned memory를 사용, CUDA-enabled GPU 사용 시 더 빠른 데이터 transfer 가능. 커스터마이징 시 cumstom batch 안에 pin_memory() 메서드 정의
  * timeout: 배치를 가져오는데 걸리는 시간의 timeout

```python
DataLoader(dataset, batch_size=1, shuffle=False, sampler=None,
           batch_sampler=None, num_workers=0, collate_fn=None,
           pin_memory=False, drop_last=False, timeout=0,
           worker_init_fn=None, *, prefetch_factor=2,
           persistent_workers=False)
```







<br>

<br>

## 참고 자료

* [Pytorch 설치 이후 Jupyter notebook에서 import torch 안 될 때](https://keepdev.tistory.com/51)
* [torch.utils.data document](https://pytorch.org/docs/stable/data.html)







<br>

<br>

## 회고

오늘은 파이토치에서의 편리한 역전파를 가능하게 해주는 Autograd, Optimizer에 대한 내용과 편리한 데이터 로드를 가능하게 해주는 Dataset, DataLoader 클래스에 대한 내용을 학습하였습니다. 

torch.nn.Module 단에서 intput, output, forward, backward에 대한 연산을 오버라이딩해서 커스터마이징할 수 있지만, 실제로 할 일은 거의 없고 메서드를 그대로 가져다 씁니다. 갱신이 필요한 파라미터들은 required_grad=True로 지정되어 있고 Tensor.backward() 로 미분을, optimizer.step()으로 파라미터 갱신을 수행합니다. 

Dataset 클래스는 데이터의 입력 형태를 정의해주는 업무를 담당하며, 데이터의 형태에 따라 다른 정의가 필요합니다. 또한 init(), len(), get_item() 등의 메서드를 정의해주어야 합니다. DataLoader에서는 Dataset 클래스를 이용해 배치를 만들고, 데이터의 타입을 텐서로 변환해주는 업무를 주로 수행합니다. DataLoader에 있는 여러 파라미터들을 이용하여 이를 커스터마이징 할 수 있습니다. 





<br>

<br>
