---
layout: single
title: "[AITech] 2022년 1월 19일 학습 내용 정리"
categories: ['AI', 'AITech']
toc: true
toc_sticky: true
tag: ['확률/통계','마스터클래스']
---



<br>

## 강의 복습 내용

### 1. 딥러닝 학습 방법 이해하기

* **신경망**은 선형 모델과 활성화 함수가 합성된 **비선형 모델**입니다. 

  * 신경망에서 각 뉴런을 이어주는 화살표(엣지)가 **가중치**에 해당합니다. 

  * 각 뉴런은 계산된 **XW**에 더해질 편향 값 **b**를 가지고 있습니다. 

  * 선형 모델에 의해 계산된 **O = XW + b**는 활성화 함수를 지나 비선형성을 얻게 됩니다. 

    * 활성화 함수에는 전통적으로 사용되어 온 **Sigmoid와 tanh**, 최근 많이 사용되는 **ReLU** 등이 있습니다. 

    ![image-20220120114123889](https://user-images.githubusercontent.com/70505378/150361220-72873270-e83c-41db-8239-594b4b71cc9a.png)

  * 다중 분류의 경우 최종 출력에 **소프트맥스 함수**를 적용시켜 **확률 벡터**로 변환할 수 있습니다. 

    * softmax 함수를 지난 벡터의 값은 **특정 클래스 k에 속할 확률**로 해석할 수 있습니다. 

    ![image-20220120114414683](https://user-images.githubusercontent.com/70505378/150361221-5dca2188-f2bc-46ab-997b-8e04c16c695d.png)

    * 추론을 할 때는 **원-핫 벡터**로 최댓값을 가진 주소만 1로 출력하는 연산을 사용해서 softmax를 사용하지 않습니다. 

* 다층 퍼셉트론(MLP)은 **신경망이 여러 층 합성된 함수**이다. 

  * 아래와 같이 **입력층-은닉층-출력층**을 거쳐 여러 번의 선형 계산과 활성화 함수 적용을 통해 입력 X로부터 출력 y를 찾아내는 것을 **순전파** 과정이라 한다. 

  ![image-20220120114550280](https://user-images.githubusercontent.com/70505378/150361225-9b7d007d-ee82-4d04-b395-b2e612eaf8ff.png)

  * 이론적으로는 2층 신경망으로도 임의의 연속 함수를 근사할 수 있다. 
  * 그러나 층이 깊을수록 **목적함수를 근사하는데 필요한 뉴런(노드)의 숫자가 훨씬 크게 줄어든다.** 층이 얇으면 한 층에 필요한 뉴런의 숫자가 기하급수적으로 늘어나고, 따라서 가중치의 수와 연산 횟수 또한 기하급수적으로 늘어나기 때문에 효율적이지 않다. 

* 딥러닝은 **역전파 알고리즘**을 이용하여 각 층에 사용된 파라미터 W, b를 학습한다. 

  * 각 층의 파라미터의 그레디언트 벡터는 윗 층부터 내려오며 역순으로 계산하게 된다. 

  ![image-20220120115148097](https://user-images.githubusercontent.com/70505378/150361229-f57fcdef-a681-47f6-805f-7e1a9a730d1e.png)

  * 역전파 알고리즘은 합성 함수 미분법인 **연쇄 법칙 기반 자동 미분**을 사용한다. 

<br>

### 2. 확률

딥러닝에서 확률론이 필요한 이유는 다음과 같다. 

* 기계학습에서 사용되는 **손실 함수들의 작동 원리**는 **데이터 공간을 통계적으로 해석해서 유도**하게 된다. 
* 예를 들어, 회귀 분석에서 사용되는 L2-노름은 **예측오차의 분산을 가장 최소화하는 방향으로 학습**하도록 유도하고, 분류 문제에서 사용되는 교차 엔트로피는 **모델 예측의 불확실성을 최소화하는 방향으로 학습**하도록 유도한다. 
* 이렇듯 분산 및 불확실성을 최소화하기 위해서는 **측정하는 방법**을 알아야 한다. 

#### 확률 분포

* 데이터 공간을 (x,y), 데이터 공간에서 데이터를 추출하는 분포를 D라 합니다. 

  * 이 때 데이터는 확률 변수로 **(x, y) ~ D**로 표기합니다. 

* 확률 변수는 **이산형 확률변수**와 **연속형 확률변수**로 구분되고, 이를 구분하는 기준은 데이터 공간의 범위가 아닌 D(데이터 분포)에 의해 결정된다. 

  * 즉, 데이터 공간의 범위가 실수 전체더라도 실제로는 -0.5와 0.5에서만 분포한다면 이는 이산형 확률변수이다. 

* 이산형 확률변수는 **확률변수가 가질 수 있는 경우의 수의 확률을 모두 더해서 모델링**한다. (확률 질량 함수)

  ![image-20220120182242018](https://user-images.githubusercontent.com/70505378/150361230-e8f5541b-5137-4eb6-b1aa-361ce8bca867.png)

* 연속형 확률변수는 **데이터 공간에 정의된 확률변수의 밀도 위에서의 적분을 통해 모델링**한다. (확률 밀도 함수)

  * 밀도는 **누적확률분포의 변화율**을 모델링하며, 확률로 해석하면 안 된다. 

  ![image-20220120182325072](https://user-images.githubusercontent.com/70505378/150361232-fd68a6e4-cff7-4873-b1fa-623d50d31572.png)

* **결합 분포 P(x, y)**는 D를 모델링한다. 

  * P(x, y)는 D의 실제 타입과 상관없이 결정할 수 있다. D는 이론적으로 존재하는 확률분포로, 사전에 알 수 없기 때문에 D가 실제로 이산이든 연속이든 P(x, y)는 이산/확률로 모델링할 수 있다. 

* **P(x)**는 입력 x에 대한 **주변확률분포**로 y에 대한 정보를 주지는 않는다. 

  * 주변확률분포 P(x)는 결합분포 P(x,y)에서 유도 가능하다. 

  ![image-20220120183027992](https://user-images.githubusercontent.com/70505378/150361233-f6fa3b27-e38d-4415-9c2e-7e8a817d89d7.png)

#### 조건부 확률

* **조건부 확률 P(y\|x)**는 입력 변수 x에 대해 정답이 y일 확률을 의미한다. 

  * 연속확률분포의 경우, P(y\|x)는 확률이 아닌 **밀도**로 해석한다. 

* 로지스틱 회귀에서 사용했던 선형 모델과 소프트맥스 함수의 결합은 **데이터에서 추출된 패턴을 기반으로 확률을 해석**하는데 사용된다. 

* 분류 모델에서 softmax(Wk + b)은 **데이터 x로부터 추출된 특징 패턴 k(x)와 가중치 행렬 W를 통해 조건부 확률 P(x,y)를 계산**한다.

* 회귀 문제의 경우 **조건부기대값 E[y\|x]**를 추정한다.   

  * 조건부기대값은 E\|\|y - f(x)\|\|<sub>2</sub>을 최소화하는 함수 f(x)와 일치한다. 
  * 무엇을 사용할 지는 **통계적 모양과 목적에 따라 다르다!!**

  ![image-20220120183844971](https://user-images.githubusercontent.com/70505378/150361236-b6b970f1-944c-49d9-8acc-6ccf4b59b818.png)

  * 기대값이란?

    * **기대값은 데이터를 대표하는 통계량**이면서 동시에 확률분포를 통해 **다른 통계적 범함수를 계산**하는데 사용된다. 

    ![image-20220120184229330](https://user-images.githubusercontent.com/70505378/150361238-9727549c-d0c0-4760-aabb-09fa50ddb6a7.png)

    * 기대값을 이용해 분산, 첨도, 공분산 등 여러 통계량을 계산할 수 있다. 

    ![image-20220120184306228](https://user-images.githubusercontent.com/70505378/150361242-aeb554a6-e41d-41d8-8a08-5c3fbafd35b0.png)

* **딥러닝**은 다층신경망을 이용하여 **데이터로부터 특징패턴 k를 추출한다.**

  * 특징패턴을 학습하기 위해 **어떤 손실함수를 사용할 지는 기계학습 문제와 모델에 의해 결정**된다. 

#### 몬테카를로 샘플링

* 기계학습의 많은 문제들은 **확률분포 P(x)를 명시적으로 모를 때가 대부분**이다. 
* 확률분포 P(x)를 모를 때 데이터를 이용하여 기대값을 계산하려면 **몬테카를로 샘플링 방법**을 사용해야 한다. 
  * `x~P(x)`: x는 확률분포 P(x)를 따른다.
  * `i.i.d.`: 독립적이고 같은 확률분포를 가진다. (independent identically distribution)

![image-20220120184516631](https://user-images.githubusercontent.com/70505378/150361246-f7420f76-97bd-4559-8276-535a5a3c8145.png)

* 몬테카를로 샘플링은 **독립추출**만 보장된다면 대수의 법칙에 의해 **수렴성을 보장**한다. 

  * 몬테카를로 샘플링은 기계학습에서 매우 다양하게 응용되는 방법이다. 

* 예제: 함수 f(x) = e<sup>-x<sup>2</sup></sup>의 [-1,1] 상에서 적분값을 어떻게 구할까?

  * 구간 [-1,1]의 길이는 2이며 함수가 좌우 대칭이다. 따라서 적분값을 2로 나누면 기대값 계산과 같고, 몬테카를로 방법을 사용할 수 있다. 

  ![image-20220120185848704](https://user-images.githubusercontent.com/70505378/150361248-ea83a030-df00-4459-aa48-38ebbf4aa94b.png)

```python
# python code
import numpy as np

def mc_int(fun, low, high, sample_size=100, repeat=10):
    int_len = np.abs(high-low)
    stat = []
    for _ in range(repeat): # 샘플링 과정 여러번 반복
        x = np.random.uniform(low=low, high=high, size=sample_size) # 샘플링
        fun_x = fun(x) # f(xi)
        int_val = int_len * np.mean(fun_x) # 2 * (sum(f(x))/N)
        stat.append(int_val)
    return np.mean(stat), np.std(stat)

def f_x(x):
    return np.exp(-x**2)

print(mc_int(f_x, low=-1, high=1, sample_size=10000, repeat=100))
# (1.4937142278833102, 0.004055490097645274)
```



<br>

### 3. 통계

#### 모수

* **통계적 모델링은 적절한 가정 위에서 확률분포를 추정**하는 것이 목표이며, 기계학습과 통계학이 공통적으로 추구하는 목표이다. 

* 그러나 유한한 개수의 데이터에서 모집단의 분포를 정확하게 알아내는 것은 불가능하므로, **근사적으로 확률분포를 추정**해야 한다. 

  * 예측 모형의 목적인 분포를 정확하게 맞추는 것보다 **데이터와 추정 방법의 불확실성을 고려해서 위험을 최소화**하는 것이다. 

* **데이터가 특정 확률분포를 따른다고 선험적으로 가정한 후 그 분포를 결정하는 모수를 추정**하는 방법을 **모수적 방법론**이라 한다. 

* **특정 확률분포를 가정하지 않고 데이터에 따라 모델의 구조 및 모수의 개수가 바뀌면** **비모수 방법론**이라 한다. 

  * 기계학습의 많은 방법론은 비모수 방법론에 속한다. 

* 기계적으로 확률분포를 가정해서는 안 되며, **데이터를 생성하는 원리**를 먼저 고려해야 한다.

  * 모수를 추정한 후에는 반드시 검정 과정이 필요하다. 

* 예를 들어, 정규분포의 모수는 평균 μ와 분산 σ<sup>2</sup>으로 이를 추정하는 통계량은 다음과 같다. 

  * 표본 분산을 구할 때 N이 아니라 N-1로 나누는 이유는 불편(unbiased) 추정량을 구하기 위해서이다. 

  ![image-20220120221103646](https://user-images.githubusercontent.com/70505378/150361250-2f69f80b-68bc-407f-92bb-e0a300096a3b.png)

  * **통계량(평균, 분산 등)의 확률 분포**를 **표집분포**라 한다. 

    * 표집분포는 표본분포와 다릅니다!!
    * 표본평균의 표집분포는 N이 커질수록 정규분포 N(μ, σ<sup>2</sup>/N)를 따른다. 이를 **중심극한정리**라 하며, 모집단의 분포가 정규분포를 따르는지와의 여부는 상관없다. 

    ![image-20220120221404354](https://user-images.githubusercontent.com/70505378/150361256-980085e5-b413-4c35-9643-75c3bcf96990.png)

#### 최대가능도 추정법

* 표본평균이나 표본분산은 중요한 통계량이지만 **확률분포마다 사용하는 모수가 다르므로 적절한 통계량이 달라진다.**

* 이론적으로 가장 가능성이 높은 모수를 추정하는 방법 중 하나는 **최대가능도 추정법**이다. 

  * 확률 밀도(질량) 함수가 모수가 주어졌을 때 x에 대한 함수라면, 최대가능도 함수는 x가 주어졌을 때 모수에 대한 함수이다. 
  * 가능도 함수는 모수 _theta_를 따르는 분포가 _x_를 관찰할 가능성을 뜻하지만, 확률로 해석해서는 안 된다. 

  ![image-20220120221745254](https://user-images.githubusercontent.com/70505378/150361258-766ecdc1-cf13-44b8-b902-85ed616fb8dc.png)

* 데이터 집합 X가 **독립적으로 추출되었을 경우 로그 가능도**를 최적화한다. 

  ![image-20220120221912176](https://user-images.githubusercontent.com/70505378/150361262-ac0153b3-cf57-4c30-9b97-ac398f643569.png)

  * 데이터의 숫자가 매우 많을 때, 컴퓨터의 precision으로는 0~1 사이의 값을 여러번 곱하는 연산은 큰 오차를 갖게 된다. 
  * 데이터가 독립일 경우, 로그를 사용하면 가능도의 곱셈을 로그가능도의 덧셈으로 바꿀 수 있기 때문에 컴퓨터로 연산이 가능하다. 
  * 미분 연산 시 로그 가능도를 사용하면 연산량을 O(n^2)에서 O(n)으로 줄일 수 있다. 
  * 대개의 손실함수의 경우 **음의 로그가능도**를 최적화(최소화)한다. 

#### 예제1: 정규분포

* 정규분포를 따르는 확률변수 X(연속확률변수)로부터 독립적인 표본 \{x1, ..., xn\}을 얻었을 때 최대가능도 추정법을 이용하여 모수를 추정한다. 

![image-20220120222932223](https://user-images.githubusercontent.com/70505378/150361264-13bd442f-2a9f-44f4-9bb9-52813be21fd5.png)

#### 예제2: 카테고리 분포

* 카테고리 분포를 따르는 확률변수 X(이산확률변수)로부터 독립적인 표본 \{x1, ..., xn\}을 얻었을 때 최대가능도 추정법을 이용하여 모수를 추정한다. 
  * **p<sub>k</sub>**는 k번째 차원에서 1일 확률이며, 모수입니다. 

![image-20220120225334321](https://user-images.githubusercontent.com/70505378/150361186-6a6a77ee-ff39-4a78-90ff-4019a904e41e.png)

#### 두 확률분포 사이의 거리

* 최대 가능도 추정법을 이용해 기계학습 모델을 학습할 수 있다. 
* 딥러닝 모델의 가중치를 θ = (W(1), ..., w(L))이라 표기했을 때 분류 문제에서 **소프트맥스 벡터는 카테고리 분포의 모수 (p1, ..., pk)**를 모델링한다. 
* 원핫벡터로 표현한 **정답 레이블 y = (y1, ..., yk)를 관찰데이터**로 이용해 **확률분포인 소프트맥스 벡터의 로그가능도를 최적화**합니다. 

![image-20220120230950310](https://user-images.githubusercontent.com/70505378/150361193-f8f36621-bc61-4fa0-8887-6f8fef898496.png)

* 기계학습에서 사용되는 손실함수들은 **모델이 학습하는 확률분포와 데이터에서 관찰되는 확률분포의 거리를 통해 유도한다.**

  * 데이터 공간에 두 개의 확률분포 P(x), Q(x)가 있을 경우 **두 확률분포 사이의 거리**를 계산할 때 다음과 같은 함수들을 이용한다. 

    * 총변동 거리(Total Variation Deistance, TV)
    * 쿨백-라이블러 발산(Kullback-Leibler Divergence, KL)
    * 바슈타인 거리(Wasserstein Distance)

  * 쿨백-라이블러 발산은 다음과 같이 정의하며, 

    ![image-20220120231424581](https://user-images.githubusercontent.com/70505378/150361195-527d9f60-37d4-4f68-8716-ab10c2232f40.png)

  * 다음과 같이 분해할 수 있다. 

    ![image-20220120231449046](https://user-images.githubusercontent.com/70505378/150361200-eae86f55-621a-4163-a354-d5245a2bd834.png)

  * 분류 문제에서 정답 레이블을 P, 모델 예측을 Q라 하면 **최대가능도 함수를 최적화하는 것은 쿨백-라이블러 발산을 최소화**하는 것과 같다. 

    * 즉, 손실 함수를 최소화한다는 것은 데이터의 분포와 모델이 예측한 분포 간 거리를 최소화한다는 것과 같다. 

<br>

### 4. 베이지안 통계학

#### 조건부 확률

* **베이즈 정리**는 조건부 확률을 이용하여 **정보를 갱신하는 방법**을 알려준다. 

  ![image-20220120232125534](https://user-images.githubusercontent.com/70505378/150361203-5b67aabc-4716-4514-9186-155231a60612.png)

* **사후확률**을 구할 때는 적어도 **사전확률**과 **가능도**가 주어져야 한다. 가능도에는 P(D | θ)와 P(D | ¬θ)가 있다. 

  * 두 가능도를 알 때, **Evidence**는 다음과 같이 구할 수 있다. 

  ![image-20220120232535181](https://user-images.githubusercontent.com/70505378/150361208-0a03eab5-d979-4872-8f4b-711196615bf1.png)

* 조건부 확률의 시각화
  * 데이터의 성격에 따라 1종오류와 2종오류 중 어떤 것의 감소가 중요한 지 결정한다. 
  * 예를 들어, 의료 데이터에서는 2종 오류(0이라고 예측했을 때 틀린 경우)는 매우 심각한 문제이므로, 2종 오류를 줄이기 위해 노력한다. 

![image-20220120232938536](https://user-images.githubusercontent.com/70505378/150361210-e47c713b-eb18-42d8-89df-5252f9ddcb51.png)

#### 베이즈 정리를 통한 정보의 갱신

* 베이즈 정리를 통해 새로운 데이터가 들어왔을 때 **앞서 계산한 사후 확률을 사전 확률로 사용**하여 **갱신된 사후확률을 계산**할 수 있다. 

![image-20220120233356864](https://user-images.githubusercontent.com/70505378/150361214-4edf9c47-611f-40f2-9944-43c38e7a9421.png)

![image-20220120233514235](https://user-images.githubusercontent.com/70505378/150361216-feb9acbb-b63d-4a28-85da-4b1d68c64b25.png)

#### 인과관계

* 조건부 확률은 유용한 통계적 해석을 제공하지만 **인과관계**를 추론할 때 함부로 사용해서는 안 된다. 
  * 데이터가 많아져도, 조건부 확률만 가지고 인과관계를 추론하는 것은 불가능하다. 
* 인과관계는 **데이터 분포의 변화에 강건한 예측모형**을 만들 때 필요하다. 

![image-20220120233658999](https://user-images.githubusercontent.com/70505378/150361218-3538e844-3130-40c6-b19f-a81176fdcda6.png)

* 인과관계를 알아내기 위해서는 **중첩요인의 효과를 제거**하고, 원인에 해당하는 변수만의 인과관계를 계산해야 한다. 



<br>

<br>

## 피어세션 정리

오늘 학습한 주된 내용인 통계론/확률론 및 베이즈 통계학은 친숙하지 않은 내용이라서인지 이해하는 것이 쉽지는 않았다. 

그래서 피어세션 중에도 해당 강의에 대한 토론이 활발하게 이루어졌고 용어 및 이해의 모호함을 더욱 명확하게 바꿀 수 있었다. 

개인적으로는, 강의 내용 자체는 쉽지 않았지만 '머신러닝 이론'에 대한 이해를 할 수 있게 해준다는 점에서 확률론/통계론의 미학을 조금이나마 느낀 것 같다. 특히 모델의 손실 함수를 정하는 것이 통계론에 기반한 선택이라는 점이 놀라웠다. 또한 손실 함수를 최소화 한다는 것은 곧 로그 가능도의 최적화이고, 이는 실제 데이터의 분포와 모델이 예측한 분포 사이의 거리를 최소화하는 것이라는 점을 자연스레 연결하며 알게 된 것이 매우 인상적이었다. 

대학원을 가고자 하는 학도의 입장에서 머신러닝 이해를 위한 확률론/통계론을 더 많이 공부해보고 싶다. 

<br>

<br>

## 과제 수행 과정/결과물



<br>

<br>

## 학습 회고

오늘 처음으로 진행한 마스터 클래스에서의 내용을 적어보고자 한다. 

**마스터 클래스**

**Part1. 인공지능 공부하기**

* 수학 너무 어려워요. 쉽게 공부하는 방법이 있나요?
  * 머리로는 어렵지만 손으로 익힐 수 있다. 
  * 똑똑하게 익숙해지는 방법 -> 많이 보는 것보다 많이 사용하는 것이 더 좋다!
  * 용어의 정의를 일단 외우고, 이후에는 예제를 찾아봅니다. 
* 여러 모델들의 수학적 원리를 모두 이해하고 있어야 하나요?
  * 모두 이해하는 것은 어렵지만 적어도 원리를 이해하는데 필요한 기초는 갖춰야 합니다. 
  * 무엇이 기초인가? -> 선형대수/확률론/통계학은 필수!
    * 기업 및 대학원 면접 때 정말 많이 물어본다. 
    * 알고리즘이랑 최적화 내용도 같이 공부하면 시너지가 좋다. 
  * 머신러닝 이론을 공부하고 싶다면 해석학/위상수학까지!
  * 기초 자체를 공부하기 보다 머신러닝에서 어떻게 활용되는지 검색해본다. 
    * 예) 분류 문제에서 왜 cross-entropy를 손실함수로 사용하는가?
* ML 엔지니어는 수학을 어느 정도 알아야 할까요?
  * 필요한 걸 공부해서 빠르게 따라잡을 수 있을만큼 알아야 한다!
* 엔지니어는 무엇을 하는 사람일까?
  * Define -> Brainstorm -> Implement -> Analyze
* 추천 시스템 공부할 대 알아야 할 내용이 있나요?
  * Dive into Deep Learning 16장 내용
  * MAB(Multi-Armed Bandit)도 중요한 테크닉!

**Part2. 인공지능 대학원 관심있어요!**

* AI 분야에서 학석박 간에 어떤 차이가 있을까요?
  * 분야마다 다르겠지만 전문성이 다르고, 대중화되지 않은 영역은 학위과정이 중요할 수 있다. 
    * 잘 알려진 대중화된 분야는 엔지니어링 능력이 더 중요할 수 있다. 
    * Seeing/Listening/Speaking/Drawing/Understanding like human -> Well-known
    * Learning/Reasoning like human -> Not-known
  * 더 깊은 이해도를 가질 수 있다는 것이 중요
* 인공지능 분야에서 대학원이 필수일까요?
  * 자신이 하려는 분야가 잘 알려지지 않은 분야라면 가는 것이 유리하다!

**Part3. FAQ**

* AI 분야에서 가장 중요한 수학은?
  * 선형대수학: 필수 중의 필수. 코드를 이해하는 데 필수. 
  * 통계 및 확률: 머신러닝 원리/이론을 이해하는데 필수. 
    * 왜 이 손실함수를 사용? 왜 이렇게 최적화?
* 대학원생과 견줄만한 활동?
  * 좋은 논문을 많이 쓰는 것이 중요
  * 오픈소스에 기여 
  * 새로운 기술에 대한 친화도, 활용한 경험 및 능력
  * 이외에는 면접으로 승부해야 함
* 수학이든 모델이든 line-by-line으로 구현해보는 연습을 하는 것이 좋은 지?
  * 대부분의 업무에서는 딥러닝 프레임워크에서 가져다 쓴다. 
  * 다만, 새로운 것을 구현해야 하는 등의 Problem Solving이 필요하다면 line-by-line으로 이해하는 것이 새로운 내용을 Follow up하고 만드는 데 핵심적이다. 
* 기계학습 알고리즘이 잘 할 수 있는 것과 아닌 것이라 
  * 충분한 데이터에서 루틴한 패턴을 추출할 수 있다면 잘 할 수 있을 것
  * 아니라면 다른 알고리즘이 더 잘 할 수 있다. 이를 비교할 수 있는 시점이 올 것. 
* 모르는/낯선 개념을 잘 이해하는 방식?
  * 모르는 내용은 키워드를 뽑아서 구글링, 정리 및 이해(30분 정도)
* AI 엔지니어로 현업에 나가기 위해 준비하면 좋을 것? 
  * MLOps 공부!!!































