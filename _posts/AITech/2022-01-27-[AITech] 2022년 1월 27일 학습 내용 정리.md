---
layout: single
title: "[AITech] 2022년 1월 27일 학습 내용 정리"
categories: ['AI', 'AITech']
toc: true
toc_sticky: true
tag: ['파이토치','saveModel','checkpoint', 'monitoringTools']
---



<br>

## 학습 내용 정리

### 모델 불러오기

요즘 딥러닝에서는 **좋은 성능을 보이는 Backbone 모델**(VGG, ResNet, BERT 등)을 가져와 본인의 데이터 셋으로 Transfer learning(Fine Tuning)시키는 과정이 일반적이다. 

또한 본인의 모델을 다른 사람들과 공유하고 싶을 수도 있다. 

이런 경우에, 모델 또는 파라미터를 파일로 저장할 수 있다. 

#### Save

* **torch.save(model, path)**

  * 사용 목적
    * **모델 자체(+파라미터)** 또는 **파라미터**만을 저장
    * 모델 학습 중간 과정의 저장을 통해 가장 좋은 성능을 보이는 모델을 선택(EarlyStopping)
    * 만들어진 모델을 외부 연구자와 공유하여 학습 재연성 향상
  * 예시 코드

  ```python
  '''모델의 파라미터만!'''
  for param_tensor in model.state_dict(): # state_dict: 모델의 파라미터를 표시
      print(param_tensor, "\t", model.state_dict()[param_tensor].size())
  '''
  Model's state_dict:
  layer1.0.weight 	 torch.Size([16, 3, 3, 3])
  layer1.0.bias 	 torch.Size([16])
  layer1.1.weight 	 torch.Size([16])
  layer1.1.bias 	 torch.Size([16])
  layer1.1.running_mean 	 torch.Size([16])
  layer1.1.running_var 	 torch.Size([16])
  layer1.1.num_batches_tracked 	 torch.Size([])
  layer2.0.weight 	 torch.Size([32, 16, 3, 3])
  layer2.0.bias 	 torch.Size([32])
  layer2.1.weight 	 torch.Size([32])
  layer2.1.bias 	 torch.Size([32])
  layer2.1.running_mean 	 torch.Size([32])
  layer2.1.running_var 	 torch.Size([32])
  layer2.1.num_batches_tracked 	 torch.Size([])
  layer3.0.weight 	 torch.Size([64, 32, 3, 3])
  layer3.0.bias 	 torch.Size([64])
  layer3.1.weight 	 torch.Size([64])
  layer3.1.bias 	 torch.Size([64])
  layer3.1.running_mean 	 torch.Size([64])
  layer3.1.running_var 	 torch.Size([64])
  layer3.1.num_batches_tracked 	 torch.Size([])
  fc1.weight 	 torch.Size([1000, 576])
  fc1.bias 	 torch.Size([1000])
  fc2.weight 	 torch.Size([1, 1000])
  fc2.bias 	 torch.Size([1])
  '''
      
  torch.save(model.state_dict(), # 모델의 파라미터를 저장
            os.path.join(MODEL_PATH, "model.pt"))
  
  new_model = TheModelClass()
  new_model.load_state_dict(torch.load(os.path.join(MODEL_PATH, "model.pt"))) # 같은 형태의 모델에서 파라미터만 load
  
  '''모델의 구조 전체!'''
  torch.save(model, os.path.join(MODEL_PATH, "model.pt")) # 모델 구조 save
  model = torch.load(os.path.join(MODEL_PATH, "model.pt")) # 모델 구조 load
  ```

  ✋ **모델 구조 살펴보기: torchsummary.summary**

  ```python
  from torchsummary import summary
  summary(model, (3,224,224))
  '''
  ----------------------------------------------------------------
          Layer (type)               Output Shape         Param #
  ================================================================
              Conv2d-1         [-1, 16, 111, 111]             448
         BatchNorm2d-2         [-1, 16, 111, 111]              32
                ReLU-3         [-1, 16, 111, 111]               0
           MaxPool2d-4           [-1, 16, 55, 55]               0
              Conv2d-5           [-1, 32, 27, 27]           4,640
         BatchNorm2d-6           [-1, 32, 27, 27]              64
                ReLU-7           [-1, 32, 27, 27]               0
           MaxPool2d-8           [-1, 32, 13, 13]               0
              Conv2d-9             [-1, 64, 6, 6]          18,496
        BatchNorm2d-10             [-1, 64, 6, 6]             128
               ReLU-11             [-1, 64, 6, 6]               0
          MaxPool2d-12             [-1, 64, 3, 3]               0
            Dropout-13                  [-1, 576]               0
             Linear-14                 [-1, 1000]         577,000
             Linear-15                    [-1, 1]           1,001
  ================================================================
  Total params: 601,809
  Trainable params: 601,809
  Non-trainable params: 0
  ----------------------------------------------------------------
  Input size (MB): 0.57
  Forward/backward pass size (MB): 5.53
  Params size (MB): 2.30
  Estimated Total Size (MB): 8.40
  ----------------------------------------------------------------
  '''
  ```

#### Checkpoints

* **Checkpoints**

  * 사용 목적

    * 학습의 결과물(loss, metric 등)을 저장하며 최선의 결과를 선택
    * 일반적으로 epoch, loss, metric을 함께 저장하여 확인
    * colab에서 지속적인 학습을 위해 필요

  * 예시 코드

    ```python
    for e in range(1, EPOCHS+1):
        epoch_loss = 0
        epoch_acc = 0
        for X_batch, y_batch in dataloader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device).type(torch.cuda.FloatTensor)
            
            optimizer.zero_grad()        
            y_pred = model(X_batch)
                   
            loss = criterion(y_pred, y_batch.unsqueeze(1))
            acc = binary_acc(y_pred, y_batch.unsqueeze(1))
            
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
            epoch_acc += acc.item()
            
            
        torch.save({
            'epoch': e,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': epoch_loss,
            }, f"saved/checkpoint_model_{e}_{epoch_loss/len(dataloader)}_{epoch_acc/len(dataloader)}.pt")
            
    
        print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(dataloader):.5f} | Acc: {epoch_acc/len(dataloader):.3f}')
    ```

#### Pretrained model Transfer learning

* **Pretrained model Transfer learning**

  * 사용 목적

    * 일반적으로 대용량 데이터셋으로 만들어진 모델의 성능이 좋다. 
    * 성능이 좋은 모델을 가져와 본인의 데이터셋으로 재학습
    * backbone architecture가 잘 학습된 모델에서 일부분만 변경하여 학습을 수행(freezing)
    * 현재의 DL에서 가장 일반적인 학습 방법

  * 참고 사이트

    * [github: pytorch-image-models](https://github.com/rwightman/pytorch-image-models#introduction): PyTorch로 설계된 이미지 분류 모델 backbones
    * [HuggingFace](https://huggingface.co/models): NLP의 사실상 표준

  * 예시 코드

    ```python
    from torch import nn
    from torchvision import models
    
    class MyNewNet(nn.Module):   
        def __init__(self):
            super(MyNewNet, self).__init__()
            self.vgg19 = models.vgg19(pretrained=True) # pretrained vgg model
            self.linear_layers = nn.Linear(1000, 1)    # 본인의 목적에 맞게 layer를 추가
    
    
        # Defining the forward pass    
        def forward(self, x):
            x = self.vgg19(x)        
            return self.linear_layers(x)
        
        
    for param in my_model.parameters(): # freezing layers
        param.requires_grad = False
    for param in my_model.linear_layers.parameters(): # 마지막 layer만 학습 수행
        param.requires_grad = True
    ```

<br>

### Monitoring tools for PyTorch

모델을 학습시키는 동안 학습 현황을 확인할 수 있는 많은 모니터링 도구들 중 대표적인 `TensorBoard`와 `weightandbiases`에 대해 살펴본다. 

#### Tensorboard

* **Tensorboard**

  * 특징

    * TensorFlow의 프로젝트로 만들어진 시각화 도구
    * 학습 그래프, metric, 학습 결과의 시각화 지원
    * PyTorch도 연결 가능 -> DL 시각화 핵심 도구

  * 데이터

    * scalar: metric 등 상수값의 연속을 표시
    * graph: 모델의 computational graph 표시
    * histogram: weight 등 값의 분포를 표현
    * Image/Text: 예측 값과 실제 값을 비교 표시
    * mesh: 3d 형태의 데이터를 표현하는 도구

  * 예시 코드

    ```python
    # Tensorboard 기록을 위한 directory 생성
    import os
    logs_base_dir = "logs"
    os.makedirs(logs_base_dir, exist_ok=True)
    
    # 기록 생성 객체 SummaryWriter 생성
    from torch.utils.tensorboard import SummaryWriter
    import numpy as np
    # add_scalar: scalar 값을 기록
    # Loss/train: loss category에 train 값
    # n_iter: x 축의 값
    writer = SummaryWriter(exp)
    for n_iter in range(100):
        writer.add_scalar('Loss/train', np.random.random(), n_iter)
        writer.add_scalar('Loss/test', np.random.random(), n_iter)
        writer.add_scalar('Accuracy/train', np.random.random(), n_iter)
        writer.add_scalar('Accuracy/test', np.random.random(), n_iter)
    # 값 기록(disk에 쓰기)
    writer.flush()
    # jupyter/colab command(같은 명령어를 콘솔에서도 사용 가능)
    %load_ext tensorboard
    %tensorboard --logdir "logs"
    ```

#### Weight & Biases

* **Weight & Biases**

  * 특징

    * 머신러닝 실험을 원활히 지원하기 위한 상용도구(유료, 기본 기능은 무료로 사용 가능)
    * 협업, code versioning, 실험 결과 기록 등 제공
    * MLOps의 대표적인 툴로 저변 확대 중
    * 하나의 프로젝트로 관리하기 때문에 코드나 실험 결과 공유 시 매우 유용

  * 사용 과정

    1. [weight & biases 사이트 접속](https://wandb.ai/site)

    2. 가입 후 API 키 확인(Settings -> API keys)

    3. 새로운 프로젝트 생성하기(Profile -> Create new project)

       * 프로젝트 이름은 모델과 연결 과정에서 사용됨

    4. wandb 연결

       * wanb.init()을 호출하면 API key를 입력

       ```python
       !pip install wandb -q
       
       import wandb
       wandb.init(project="my-test-project", entity='wowo0709') # 프로젝트명, 닉네임
       ```

    5. config 설정

       * 프로젝트 템플릿으로 따로 파일로 관리하면 매우 편리!

       ```python
       EPOCHS = 100
       BATCH_SIZE = 32
       LEARNING_RATE = 0.001
       
       config={"epochs": EPOCHS, "batch_size": BATCH_SIZE, "learning_rate" : LEARNING_RATE}
       wandb.config = config
       # wandb.init(project="my-test-project", config=config)
       ```

    6. 학습 기록

       ```python
       for e in range(1, EPOCHS+1):
           epoch_loss = 0
           epoch_acc = 0
           for X_batch, y_batch in train_dataset:
               X_batch, y_batch = X_batch.to(device), y_batch.to(device).type(torch.cuda.FloatTensor)
               optimizer.zero_grad()        
               y_pred = model(X_batch)
                      
               loss = criterion(y_pred, y_batch.unsqueeze(1))
               acc = binary_acc(y_pred, y_batch.unsqueeze(1))
               
               loss.backward()
               optimizer.step()
               
               epoch_loss += loss.item()
               epoch_acc += acc.item()
               
               
           train_loss = epoch_loss/len(train_dataset)
           train_acc = epoch_acc/len(train_dataset)
           print(f'Epoch {e+0:03}: | Loss: {train_loss:.5f} | Acc: {train_acc:.3f}')
           # 학습 기록!
           wandb.log({'accuracy': train_acc, 'loss': train_loss})
       ```

       

       ![image-20220127135843298](https://user-images.githubusercontent.com/70505378/151295520-afa06789-313a-4a2f-9f0d-2bfde985c7e8.png)





<br>

<br>

## 참고 자료







<br>

<br>

## 회고

금일 파이토치에 대한 학습은 크게 **모델 불러오기**와 **모니터링 도구**로 나눌 수 있다. 

모델 불러오기의 경우 '모델 save/load, checkpoint, pretrained model transfer learning'으로 나누어 학습하였고, 각각을 수행하기 위한 간단한 핵심 코드, 그리고 그 사용 이유에 대하여 학습하였다. 

모니터링 도구의 경우 기존에 익숙하던 'Tensorboard'에 더하여 최근 MLOps의 대표적인 툴로 저변을 확대 중인 'Weight & Biases'에 대하여 둘러봤는데, 학습 기록을 프로젝트로 관리한다는 점에서 협업 시 매우 유용하게 사용할 수 있을 것 같다는 생각이 들었다. 



<br>

<br>
