---
layout: single
title: "[Computer Vision] 5(3). 강력한 객체 탐지 모델 - Faster R-CNN"
categories: ['AI', 'ComputerVision']
---



<br>

# Faster R-CNN

속도를 주요 이점으로 갖는 YOLO에 비해 Faster R-CNN의 장점은 성능에 있다. 또한 이 모델은 4~5 fps의 속도를 보인다.

Faster R-CNN은 R-CNN, Fast R-CNN을 거쳐 점진적으로 설계되었다. 

<br>

### Faster R-CNN의 일반 아키텍처

---

YOLO는 SSD(single shot detection, 단발성 탐지기)로 간주되는데, 그 이름에서도 알 수 있듯이 이미지의 각 픽셀을 한 번에 분석한다. 따라서 속도가 매우 빠르다. 

더 정확한 결과를 얻기 위해 Faster R-CNN은 두 단계로 작동한다. 
1. **관심 영역(Region of Interest, RoI)**을 추출한다. RoI는 이미지에서 객체를 포함할 수 있는 영역을 말한다. 
2. **분류 단계(탐지 단계)**. 각 RoI에 대해 합성곱 네트워크의 입력에 맞춰 정사각형으로 크기를 조정한다. 그런 다음 CNN을 이용해 RoI를 분류한다. 

![KakaoTalk_20210818_113104420](https://user-images.githubusercontent.com/70505378/129832501-7f24ec73-98d6-45df-add5-47be90a6faf6.png)

<br>

**1단계: 영역 제안**

![KakaoTalk_20210818_111338971](https://user-images.githubusercontent.com/70505378/129832515-c7387724-039f-46b7-a50e-7b22637da4f7.png)

관심 영역은 **영역 제안 네트워크(region proposal network, RPN)**를 사용해 생성된다. RoI를 생성하기 위해 영역 제안 네트워크는 합성곱 계층을 사용한다. RPN 아키텍처는 YOLO 아키텍처와 상당히 많음 유사점을 가진다. 
- 앵커 박스를 사용한다. Faster R-CNN 논문에서 9개 크기의 앵커 박스(3개의 세로로 긴 직사각형, 3개의 가로로 긴 직사각형, 3개의 정사각형)를 사용했다. 
- 특징 볼륨을 생성하기 위해 어떤 백본이라도 사용할 수 있다. 
- 그리드를 사용하며 그리드 크기는 특징 볼륨의 크기에 따라 달라진다. 
- 마지막 계층은 숫자를 출력하는데, 이 숫자를 사용해서 앵커 박스를 객체에 맞는 적절한 경계 상자로 정교화할 수 있다. 

<br>

RPN의 단계별 절차는 다음과 같다. 
1. 네트워크는 입력으로 이미지를 받아들이고 여러 합성곱 계층을 적용한다. 
2. 특징 볼륨을 출력한다. 합성곱 필터가 특징 볼륨에 적용된다. 필터 크기는 3x3xD이며 여기에서 D는 특징 볼륨의 깊이이다. 
3. 특징 볼륨의 각 위치에서 이 필터는 중간 단계로 1xD의 벡터를 생성한다. 
4. 두 형제 1x1 합성곱 계층은 객체성 점수와 경계 상자 좌표를 계산한다. k개의 경계 상자마다 두 개의 객체성 점수가 있다. 또한 앵커 박스의 좌표를 개선하기 위해 사용될 4개의 부동소수점도 있다. 

사후 처리 단계 이후 최종 출력은 RoI 리스트다. 이 단계에서는 객체의 클래스에 대한 어떤 정보도 생성되지 않고 그 위치에 대한 정보만 생성된다. 다음 분류 단계에서 객체를 분류하고 경계 상자를 개선한다. 

<br>

**2단계: 분류**

![KakaoTalk_20210818_112930443](https://user-images.githubusercontent.com/70505378/129832540-0d788c32-753e-419b-8d08-3214418411a0.png)

분류 단계에서는 최종 경계 상자를 출력하고, 이전 단계(RPN)의 RoI 리스트와 입력 이미지에서 계산된 특징 볼륨을 두 개의 입력으로 받는다. 

분류 단계에서는 이전 영역 제안 단계에서 계산한 특징 맵을 재사용함으로써 다음 두가지 이점을 갖는다. 
- **가중치를 공유한다**: 다른 CNN을 사용하려면 두 개의 백본(하나는 RPN, 다른 하나는 분류 부분)에 대한 가중치를 저장해야 한다. 
- **계산을 공유한다**: 하나의 입력 이미지에 대해 두 개 대신 한 개의 특징 볼륨만 계산한다. 이 연산이 전체 네트워크에서 가장 비용이 많이 들기 때문에 두 번 실행하지 않아도 된다는 점에서 계산 성능이 향상된다. 

<br>

_Fast R-CNN 아키텍처_

Faster R-CNN의 두번째 단계는 첫번째 단계에서 특징 맵과 함께 RoI 리스트를 받는다. 각 RoI에 대해 합성곱 계층이 적용돼 클래스 예측과 **경계 상자 개선** 정보를 얻는다.

단계별 절차는 다음과 같다. 
1. RPN으로부터 특징 맵과 RoI를 받는다. 원본 이미지 좌표계로 생성된 RoI는 특징 맵 좌표계로 변환된다. 
2. 각 RoI 크기를 조정(RoI 풀링)해 완전 연결 계층의 입력과 맞춘다. 
3. 완전 연결 계층을 적용한다. 여기에서 특징 벡터를 얻는다. 
4. 두 개의 서로 다른 합성곱 계층을 적용한다. 하나는 분류(cls)를 처리하고 다른 하나는 RoI 개선(rgs)을 처리한다. 

최종 결과는 클래스 점수와 경계 상자 개선을 위한 부동소수점 숫자이며, 이 결과를 사후 처리해서 모델의 최종 출력을 생성한다. 

완전 연결 계층의 입력은 고정된 크기이기 때문에 이 문제를 해결하기 위해 **관심영역 풀링** 기법을 사용한다. 

<br>

_RoI 풀링_

**관심 영역 풀링(RoI pooling)**은 다양한 크기의 특징 맵의 일부를 취하고 이를 고정된 크기로 변환한다. 

<br>

<br>

### Faster R-CNN 훈련

---

**RPN 훈련시키기**

RPN의 입력은 이미지이고 출력은 RoI 리스트다. 앞에서 봤듯이 각 이미지에는 HxWxk 개의 제안 영역이 있다. (H와 W는 특징 맵의 크기를 나타내고 k는 앵커 박스의 개수이다) 이 단계에서는 아직 객체의 클래스를 고려하지 않는다.

모든 제안 영역을 한 번에 훈련시키기는 어렵다. 이미지의 대부분은 배경이기 때문에 대부분의 제안 영역은 '배경'을 예측하도록 훈련된다. 그 결과 이 네트워크는 항상 배경을 예측하도록 학습한다. 대신 샘플링 기법을 선호한다. 

256개의 실제 앵커 박스의 미니 배치가 구성되며 이 중 128개는 양성(객체를 포함)이고 다름 128개는 음성(배경만 포함)이다. 이미지에서 음성 샘플이 128개보다 적으면 가능한 양성 샘플이 모두 사용되며 배치는 음성 샘플로 채워진다. 

<br>

**RPN 손실**

손실은 두 개의 항으로 구성된다. 

![KakaoTalk_20210818_122422812](https://user-images.githubusercontent.com/70505378/129832568-016a36ec-3477-4c20-a1aa-25483f836eef.png)

- i는 훈련 배치에서 앵커 박스의 인덱스이다. 
- p<sub>i</sub>는 해당 앵커 박스가 객체일 확률이다. p<sub>i</sub>* 는 실제 확률로 앵커 박스가 '양성'이면 1이고 그렇지 않으면 0이다. 
- t<sub>i</sub>는 좌표 개선을 나타내는 벡터이며, t<sub>i</sub>* 는 실제 좌표이다. 
- N<sub>cls</sub>는 훈련 미니 배치에서 실제 앵커 상자의 개수다. 
- N<sub>cls</sub>는 가능한 앵커 위치 개수다. 
- L<sub>cls</sub>는 두 클래스(객체와 배경)에 대한 로그 손실이다. 
- **람다**는 이 손실의 두 부분의 균형을 맞추기 위한 밸런싱 매개변수다. 

<br>

마지막으로 L<sub>reg</sub>(t<sub>i</sub>, t<sub>i</sub>* ) = R(t<sub>i</sub> - t<sub>i</sub>* )로 구성되며, 여기에서 R은 다음같이 정의되는 '평활' L1 손실 함수이다. 


```python
def smoothL1(x):
    if abs(x) < 1: return 0.5 * x**2
    else: abs(x) - 0.5
```

이 함수는 L2 손실을 대체할 함수로 도입되었다. 오차가 너무 중요한 경우 L2 손실이 너무 커져서 훈련이 불안정해진다. 

YOLO와 마찬가지로 p<sub>i</sub>* 항 덕분에 객체를 포함한 앵커 박스에 대해서만 회귀 손실이 사용된다. 이 손실은 N<sub>cls</sub>, N<sub>cls</sub> 두 부분으로 나뉜다. 이 두 값을 **정규화 항**이라고 하며, 미니 배치 크기를 바꾸더라도 이 손실은 균형을 잃지 않는다. 

<br>

요약하면 YOLO와 유사하게 손실은 다음에 패널티를 부과한다. 
- 첫번째 항으로 객체성 분류에서의 오차
- 두번째 항으로 경계 상자 개선에서의 오차

그러나 YOLO 손실과 반대로 RPN은 관심 영역만 예측하기 때문에 객체 클래스를 다루지 않는다. 

<br>

**Fast R-CNN 손실**

Faster R-CNN의 RPN 다음 단계는 Fast R-CNN 단계이다. 따라서 그 손실은 종종 Fast R-CNN 손실이라고 한다. Fast R-CNN 손실의 공식이 RPN 손실과 다르지만, 근본적으로 매우 비슷하다. 

![KakaoTalk_20210818_120559115](https://user-images.githubusercontent.com/70505378/129832583-25b00281-101f-4285-b5a1-da0e95b64f28.png)

- L<sub>cls</sub>: 실제 클래스 u와 클래스 확률 p 사이의 로그 손실
- L<sub>Loc</sub>(t<sup>n</sup>, v): RPN 손실의 L<sub>reg</sub>와 동일
- **람다**[u >= 1]는 u >= 1일 때 1이고 그 외의 경우는 0

Fast R-CNN을 훈련하는 동안 id=0으로 설정해 항상 배경 클래스를 사용한다. 실제로 ROI가 배경 영역을 포함할 수 있고 이를 배경으로 분류하는 일은 중요하다. 람다 항은 배경 상자에 대해 경계 상자 오차에 패널티를 부과하는 것을 피한다. 

<br>

**훈련 계획**

논문에서 추천하는 훈련 절차는 **4-단계 교대 훈련**이다. 

1. RPN을 훈련해 허용할 만한 RoI를 예측한다. 
2. 훈련된 RPN의 출력을 사용해 분류 파트를 훈련시킨다. RPN과 분류 부분이 별도로 훈련됐기 때문에 훈련이 끝나면 두 부분이 서로 다른 합성곱 가중치를 갖게 된다. 
3. RPN의 CNN을 분류 부분의 CNN으로 교체해 합성곱 가중치를 공유하게 만든다. 공유된 CNN 가중치를 고정시킨다. RPN의 마지막 계층을 다시 훈련한다. 
4. 분류 부분의 마지막 계층을 RPN의 출력을 사용해 다시 훈련한다. 

이 절차가 끝나면 합성곱 가중치를 공유하는 두 부분으로 이루어진 훈련된 네트워크를 얻게 된다. 

<br>

<br>

### 텐서플로 객체 탐지 API

---

텐서플로 객체 탐지 API는 수많은 기여자와 텐서플로 팀에 의해 관리되고 있는 Faster R-CNN의 구현을 제공한다. 

객체 탐지 API는 텐서플로 핵심 라이브러리에 포함되지는 않지만, [별도의 깃허브 저장소](https://github.com/tensorflow/models/tree/master/research/object_detection)에서 내려받을 수 있다. 

<br>

**사전 훈련된 모델 사용하기**

객체 탐지 API에는 COCO 데이터셋에서 훈련된 몇 가지 사전 훈련된 모델이 포함되어 있다. 이들은 모두 Faster R-CNN을 기반으로 하지만 서로 다른 매개변수와 백본을 사용한다. 이 차이가 추론 속도와 성능에 영향을 미친다. 일반적으로 추론 시간과 mAP는 트레이드 오프 관계이다. 

**맞춤 데이터셋에서 훈련하기**

COCO 데이터셋에 포함되지 않는 객체를 탐지하기 위해 모델을 훈련시킬 수 있다. 그러려면 객체 클래스 당 최소 1000개의 샘플을 확보해야 한다. 또한 훈련 데이터셋을 생성하기 위해 훈련 이미지 주위에 경계 상자를 그리고(이미지 크롭핑) 직접 주석을 달아야(이미지 라벨링)한다. 
