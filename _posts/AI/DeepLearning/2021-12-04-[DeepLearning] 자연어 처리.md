---
layout: single
title: "[Deep Learning] 자연어 처리"
categories: ['AI', 'DeepLearning']
toc: true
toc_sticky: true
tag: ['형태소분석','품사태깅','구문분석','의미분석','Word2Vec','CBOW','Skipgram']
---

<br>

## 자연어 처리의 분석 단계

![image-20211204160531908](https://user-images.githubusercontent.com/70505378/144702197-b4bec116-9831-4e5d-b83d-6cc1ce1b9c24.png)

### 형태소 분석

입력된 문자열을 분석하여 형태소 단위로 분해하는 과정

* 자연어 처리의 **가장 기본적인 분석 작업**
* **단어**(한국어의 경우 어절)에 대하여, **형태소**들을 인식
* 불규칙한 활용이나 축약, 탈락 현상이 일어난 경우에 원형 복원
* **어휘 분석**이라고도 함
* **여러 사전 정보 이용**
* **규칙 기반 분석**
* 언어별 난이도 차이
  * 영어, 프랑스어, 중국어: 쉬움
  * 한국어, 일본어, 아랍어: 어려움

👍 **예시**

영어 형태소 분석 예시

<img src="https://user-images.githubusercontent.com/70505378/144702198-184c039b-6c23-4640-a8ed-62f4ed6b9dd8.png" alt="image-20211204161259999" style="zoom:67%;" />

한국어 형태소 분석 예시

* **교착어**의 특성 때문에 영어에 비해 복잡
* **중의성** 포함
  * 예: **'나는'**
    * 나(대명사, I) + 는(조사)
    * 나(동사, produce) + 는(어미)
    * 나(동사, fly) + 는(어미)
* 접두사 접미사 처리

<img src="https://user-images.githubusercontent.com/70505378/144702199-e6993972-3111-44bf-8fc8-30e63a6ff3b0.png" alt="image-20211204161546047" style="zoom:67%;" />

한국어 형태소 분석 과정

![image-20211204161642693](https://user-images.githubusercontent.com/70505378/144702200-46bcebfc-5247-4715-a317-92269f07a505.png)

<br>

#### 품사 태깅

문장의 각 단어에 **품사 및 문법적 기능**에 대한 태그를 붙이는 것

![image-20211204162053826](https://user-images.githubusercontent.com/70505378/144702201-ca0399e6-2af4-42df-8bf3-161d50d4e811.png)

**품사 태깅 방법**

* 규칙 기반 방법
  * 수작업으로 만든 규칙 사용
  * 일관성 있는 규칙을 많이 만들어내기는 어렵지만, 만들어진 규칙은 비교적 쉽게 사용 가능
* 기계학습 기반 방법
  * 품사 태깅이 되어있는 말뭉치를 학습 데이터로 사용하여 분류기를 학습하여 사용
  * 통계적 기계학습 알고리즘, SVM, 딥러닝 신경망 등 사용

#### 개체명 인식

텍스트에서 **인명, 지명, 기관명, 시간, 날짜, 화폐, 퍼센티지**와 같은 **개체명**을 인식하여 텍스트에 해당 객체명 태그를 달아주는 것

정보검색, 정보추출, 질의응답에서 중요한 역할

![image-20211204162108647](https://user-images.githubusercontent.com/70505378/144702202-f89025e6-096c-4fd3-9d7f-8e328383a873.png)

**개체명 인식 기법**

* 규칙 기반의 방법
  * 개체명을 인식하기 위해 사전들과 규칙들 이용
  * 사전의 종류: 개체명 사전, 결합 단어 사전 등
  * 개체명 인식 규칙: 단어구성 규칙, 문맥 규칙 등
* 기계학습 기반의 방법
  * 단어의 품사 정보, 문자 유형, 주변 단어 정보, 사전 정보 등의 특징들을 이용하여 개체명을 부류로 간두하여 분류기 개발

<br>

### 구문 분석

* 구문에 따라 문장이 가지는 구문 구조를 분석하여, 문장을 구성하는 **문자열(단어)**들이 **문장에서 어떤 역할**을 하는지 결정
  * **구문(syntax)**: 문장이나 구절을 만드는 규칙
  * '나는 책을 읽는다' = 나는(주어) + 책을(목적어) + 읽는다(서술어)
* 구문 분석 결과는 **파스 트리** 형태로 표현
* 파싱(parsing)
  * 문장을 구문 분석하여 파스 트리를 만들어내는 작업

**구문분석 접근 방법**

* 규칙 기반 구문 분석
* 기계학습 기반 구문 분석

<br>

#### 규칙 기반 구분 분석

문맥 자유 문법 형태의 **구구조 문법**을 이용하여 문장 분석

* 구구조 문법: 문장의 문법을 N, V, Adj, DetP 등의 품사 기호와, NP, VP, AdjP 등의 구 기호를 사용하여 문맥 자유 문법 형태로 기술

  ![image-20211204162621902](https://user-images.githubusercontent.com/70505378/144702205-89253389-b19d-4d8d-a0e2-8ee653250f40.png)

**파서** 또는 **구문 분석기** 이용

**파싱 기법 분류**

* 하향식 파싱

  * 문장 시작 기호 S에서 시작
  * 생성 규칙을 반복 적용하여 주어진 문장을 얻는 과정을 통해 구문의 구조 파악

  ![image-20211204162939972](https://user-images.githubusercontent.com/70505378/144702206-9183e72d-e5d1-4117-af61-ec5ce68e4182.png)

* 상향식 파싱

  * 문장에서 시작하여 문장 시작 기호 S 방향으로 파싱 트리를 생성
  * 생성 규칙의 오른족에 대응하는 부분을 해당 규칙의 왼쪽 부분으로 변화하는 과정 반복

  ![image-20211204163037134](https://user-images.githubusercontent.com/70505378/144702207-cf788684-ebf4-4a48-8819-bc2dc7c049a4.png)

**중의성으로 인한 구문 분석의 어려움**

* 구조적 중의성

  * 하나의 **문장**이 다수의 구조로 해석될 수 있는 문장

  ![image-20211204163224328](https://user-images.githubusercontent.com/70505378/144702210-54f93fe0-4b4b-4f72-a03a-c64bedb25d25.png)

* 어휘적 중의성

  * 하나의 **단어**가 **여러 품사**로 사용될 때 발생하는 다수 구조로 해석될 수 있는 성질

  ![image-20211204163306109](https://user-images.githubusercontent.com/70505378/144702211-1ad8254a-11ab-4e0f-aad4-148307e599e2.png)

<br>

#### 기계학습 기반 구문분석

* 구문 분석이 된 학습 데이터를 사용하여 구문 분석을 하는 모델을 학습을 통해 구축
* 사람이 구구조 문법을 정의할 필요가 없음
* 구문 분석된 정보를 포함한 말뭉치를 만들어서 제공
* 높은 신뢰도의 말뭉치 구축 필요
* SVM, 조건부 랜덤 필드(CRF) 모델, 딥러닝 신경망 등

**말뭉치(Corpus)**

* 문장 하나하나를 **구문 분석**하여 **말뭉치**로 구성해 놓은 것
* 구문 구조를 **트리 형태**로 표현

![image-20211204163600454](https://user-images.githubusercontent.com/70505378/144702212-a886840e-bb1c-4cee-a7f8-d7a6b7019f1c.png)

<br>

### 의미 분석

* **형태소 분석**과 **구문 분석** 결과를 해석하여 **문장이 가진 의미**를 파악하는 작업
* 형태소 각각의 의미에 대한 지식 표현 필요
* 담화가 이루어지는 상황에 대한 세계 모델과 상식에 대한 지식 필요
* 일반적인 상황에 대한 담화를 이해하는 것은 어려움
* **담화 환경의 제약**을 통해 만족스러운 정도의 시스템 구축 가능
  * 검색 기반 질의 응답
  * 지식 기반 질의 응답
  * 혼합형 질의 응답

**단어 의미 중의성 해소**

특정 문장에 등장하는 단어가 어떤 의미인지 판별하는 작업

![image-20211204163813748](https://user-images.githubusercontent.com/70505378/144702213-7f7919be-b0cb-4856-9509-bc526dbc3059.png)

**화행 분석**

대화 중의 발화가 어떤 종류인지 파악하는 것

설명, 의견, 동의, 거부, 감사, 예-아니오 질문, 주관식 질문, 혼잣말, 재확인 등으로 분류

**문맥 함의**

문장에 표면적으로 나타난 사실 이외에 함의된 사실을 파악하는 것

![image-20211204163908803](https://user-images.githubusercontent.com/70505378/144702214-247a8c11-f815-4b6f-a696-852268a08be1.png)`

**의미 분석**

* 통사적으로 옳으나 의미적으로 틀린 문장
  * 돌이 걸어간다, 바람이 달린다 등
* 모호성
  * 말이 많다. (horse, speech)

<br>

### 화용 분석

실제 **상황적 맥락**, 즉 말하는 이와 듣는 이의 관계, 시간과 장소, 주제를 고려하여 **문장이 실세계와 가지는 연관 관계를 분석**하는 것

<br>

<br>

## 단어의 실수 벡터 표현

### One-Hot 벡터

단어 별로 하나의 좌표 축을 대응시킨 공간에서, 해당되는 단어 위치에만 1을 설정하고 나머지에는 0을 설정하여, 공간 상에 단어 표현

![image-20211204164107263](https://user-images.githubusercontent.com/70505378/144702185-f5f40e87-52cc-47cb-bc7a-0a6c53ad6fe6.png)

**단점**

* 단어 간의 유사도를 계산하기 어려움
* 단어가 많아질 경우 메모리 공간이 너무 커짐

### Word2Vec 또는 단어 임베딩

단어의 의미를 충분히 잘 나타내도록 단어를 공간 상의 실수 벡터로 표현

![image-20211204164240453](https://user-images.githubusercontent.com/70505378/144702188-02befa79-434d-43f2-93aa-e0a50a0e9fc4.png)

유사한 의미의 단어가 벡터 공간 상에서 근처에 위치

![image-20211204164258647](https://user-images.githubusercontent.com/70505378/144702190-464ca902-e5b4-423f-93b8-4c26146f456a.png)

#### CBOW 모델

`CBOW(Continuous Bag of Words)` 모델은 V차원의 one-hot 벡터로 표현된 단어를 N차원의 실수 벡터로 바꾸는 역할을 한다. 

**입력에 주변 단어**들이 주어질 때, **출력에서는 해당 단어**가 나타날 확률이 높아지도록 학습한다. 

이 때 단어를 표현하는 실수 벡터는 **학습된 모델에서 해당 단어의 노드에 연결된 가중치**이다. 

![image-20211204164526477](https://user-images.githubusercontent.com/70505378/144702192-3b1dc133-dd0c-4d86-ac30-0a60b4df632c.png)

#### Skip-gram 모델

`Skip-gram` 모델은 CBOW와 대칭적인 구조를 보인다. 

**입력에 학습 대상이 되는 단어**가 주어질 때, **출력에서는 해당 단어의 주위 단어**들이 나타날 확률이 높아지도록 학습

이 때 단어를 표현하는 실수 벡터는 **학습된 모델에서 해당 단어의 노드에 연결된 가중치**이다. 

![image-20211204164659790](https://user-images.githubusercontent.com/70505378/144702193-f1ccfde2-1d6b-4d7d-8f1c-a0687d783c00.png)

#### 계층적 소프트맥스와 반례 표본 추출

단어 임베딩 시 소프트맥스 계산할 때, 분모에서 모든 노드에 대한 계산 결과값이 필요하기 때문에 매우 높은 계산 비용이 발생한다. 

이를 해결하기 위해 두가지 방법을 사용할 수 있다. 

**계층적 소프트맥스**

소프트맥스를 이진트리로 근사하여 계산시간을 절감한다. 

![image-20211204164931597](https://user-images.githubusercontent.com/70505378/144702194-9618a945-fac2-4a77-a6dc-ef82a2f7518b.png)

**반례 표본 추출**

* 소프트맥스를 사용할 때 매번 많은 출력 벡터를 계산하는 대신, **표본을 추출해서 이들에 한해서만 계산**하는 방법

* 표본 구성

  * 학습 데이터에 주어진 **전체 출력 단어들 wi**와 **소수의 반례 단어들 wn<sub>j</sub> 에 한해 오차 함수 E를 최소화하도록 학습

  ![image-20211204165202389](https://user-images.githubusercontent.com/70505378/144702195-26b8a08e-ec0e-4492-b2d5-e4fbbe8b185e.png)

* 오차 함수가 소프트맥스의 값을 직접 사용하지 않기 때문에, 일반 소프트맥스를 사용하는 경우에 비하여 매우 짧은 시간에 모델 학습 가능
