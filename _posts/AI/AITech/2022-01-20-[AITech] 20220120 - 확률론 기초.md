---
layout: single
title: "[AITech] 20220120 - 확률론 기초"
categories: ['AI', 'AITech']
toc: true
toc_sticky: true
tag: ['확률분포', '조건부확률', '몬테카를로샘플링']
---



<br>

## 강의 복습 내용

### 확률

딥러닝에서 확률론이 필요한 이유는 다음과 같다. 

* 기계학습에서 사용되는 **손실 함수들의 작동 원리**는 **데이터 공간을 통계적으로 해석해서 유도**하게 된다. 
* 예를 들어, 회귀 분석에서 사용되는 L2-노름은 **예측오차의 분산을 가장 최소화하는 방향으로 학습**하도록 유도하고, 분류 문제에서 사용되는 교차 엔트로피는 **모델 예측의 불확실성을 최소화하는 방향으로 학습**하도록 유도한다. 
* 이렇듯 분산 및 불확실성을 최소화하기 위해서는 **측정하는 방법**을 알아야 한다. 

#### 확률 분포

* 데이터 공간을 (x,y), 데이터 공간에서 데이터를 추출하는 분포를 D라 합니다. 

  * 이 때 데이터는 확률 변수로 **(x, y) ~ D**로 표기합니다. 

* 확률 변수는 **이산형 확률변수**와 **연속형 확률변수**로 구분되고, 이를 구분하는 기준은 데이터 공간의 범위가 아닌 D(데이터 분포)에 의해 결정된다. 

  * 즉, 데이터 공간의 범위가 실수 전체더라도 실제로는 -0.5와 0.5에서만 분포한다면 이는 이산형 확률변수이다. 

* 이산형 확률변수는 **확률변수가 가질 수 있는 경우의 수의 확률을 모두 더해서 모델링**한다. (확률 질량 함수)

  ![image-20220120182242018](https://user-images.githubusercontent.com/70505378/150361230-e8f5541b-5137-4eb6-b1aa-361ce8bca867.png)

* 연속형 확률변수는 **데이터 공간에 정의된 확률변수의 밀도 위에서의 적분을 통해 모델링**한다. (확률 밀도 함수)

  * 밀도는 **누적확률분포의 변화율**을 모델링하며, 확률로 해석하면 안 된다. 

  ![image-20220120182325072](https://user-images.githubusercontent.com/70505378/150361232-fd68a6e4-cff7-4873-b1fa-623d50d31572.png)

* **결합 분포 P(x, y)**는 D를 모델링한다. 

  * P(x, y)는 D의 실제 타입과 상관없이 결정할 수 있다. D는 이론적으로 존재하는 확률분포로, 사전에 알 수 없기 때문에 D가 실제로 이산이든 연속이든 P(x, y)는 이산/확률로 모델링할 수 있다. 

* **P(x)**는 입력 x에 대한 **주변확률분포**로 y에 대한 정보를 주지는 않는다. 

  * 주변확률분포 P(x)는 결합분포 P(x,y)에서 유도 가능하다. 

  ![image-20220120183027992](https://user-images.githubusercontent.com/70505378/150361233-f6fa3b27-e38d-4415-9c2e-7e8a817d89d7.png)

#### 조건부 확률

* **조건부 확률 P(y\|x)**는 입력 변수 x에 대해 정답이 y일 확률을 의미한다. 

  * 연속확률분포의 경우, P(y\|x)는 확률이 아닌 **밀도**로 해석한다. 

* 로지스틱 회귀에서 사용했던 선형 모델과 소프트맥스 함수의 결합은 **데이터에서 추출된 패턴을 기반으로 확률을 해석**하는데 사용된다. 

* 분류 모델에서 softmax(Wk + b)은 **데이터 x로부터 추출된 특징 패턴 k(x)와 가중치 행렬 W를 통해 조건부 확률 P(x,y)를 계산**한다.

* 회귀 문제의 경우 **조건부기대값 E[y\|x]**를 추정한다.   

  * 조건부기대값은 E\|\|y - f(x)\|\|<sub>2</sub>을 최소화하는 함수 f(x)와 일치한다. 
  * 무엇을 사용할 지는 **통계적 모양과 목적에 따라 다르다!!**

  ![image-20220120183844971](https://user-images.githubusercontent.com/70505378/150361236-b6b970f1-944c-49d9-8acc-6ccf4b59b818.png)

  * 기대값이란?

    * **기대값은 데이터를 대표하는 통계량**이면서 동시에 확률분포를 통해 **다른 통계적 범함수를 계산**하는데 사용된다. 

    ![image-20220120184229330](https://user-images.githubusercontent.com/70505378/150361238-9727549c-d0c0-4760-aabb-09fa50ddb6a7.png)

    * 기대값을 이용해 분산, 첨도, 공분산 등 여러 통계량을 계산할 수 있다. 

    ![image-20220120184306228](https://user-images.githubusercontent.com/70505378/150361242-aeb554a6-e41d-41d8-8a08-5c3fbafd35b0.png)

* **딥러닝**은 다층신경망을 이용하여 **데이터로부터 특징패턴 k를 추출한다.**

  * 특징패턴을 학습하기 위해 **어떤 손실함수를 사용할 지는 기계학습 문제와 모델에 의해 결정**된다. 

#### 몬테카를로 샘플링

* 기계학습의 많은 문제들은 **확률분포 P(x)를 명시적으로 모를 때가 대부분**이다. 
* 확률분포 P(x)를 모를 때 데이터를 이용하여 기대값을 계산하려면 **몬테카를로 샘플링 방법**을 사용해야 한다. 
  * `x~P(x)`: x는 확률분포 P(x)를 따른다.
  * `i.i.d.`: 독립적이고 같은 확률분포를 가진다. (independent identically distribution)

![image-20220120184516631](https://user-images.githubusercontent.com/70505378/150361246-f7420f76-97bd-4559-8276-535a5a3c8145.png)

* 몬테카를로 샘플링은 **독립추출**만 보장된다면 대수의 법칙에 의해 **수렴성을 보장**한다. 

  * 몬테카를로 샘플링은 기계학습에서 매우 다양하게 응용되는 방법이다. 

* 예제: 함수 f(x) = e<sup>-x<sup>2</sup></sup>의 [-1,1] 상에서 적분값을 어떻게 구할까?

  * 구간 [-1,1]의 길이는 2이며 함수가 좌우 대칭이다. 따라서 적분값을 2로 나누면 기대값 계산과 같고, 몬테카를로 방법을 사용할 수 있다. 

  ![image-20220120185848704](https://user-images.githubusercontent.com/70505378/150361248-ea83a030-df00-4459-aa48-38ebbf4aa94b.png)

```python
# python code
import numpy as np

def mc_int(fun, low, high, sample_size=100, repeat=10):
    int_len = np.abs(high-low)
    stat = []
    for _ in range(repeat): # 샘플링 과정 여러번 반복
        x = np.random.uniform(low=low, high=high, size=sample_size) # 샘플링
        fun_x = fun(x) # f(xi)
        int_val = int_len * np.mean(fun_x) # 2 * (sum(f(x))/N)
        stat.append(int_val)
    return np.mean(stat), np.std(stat)

def f_x(x):
    return np.exp(-x**2)

print(mc_int(f_x, low=-1, high=1, sample_size=10000, repeat=100))
# (1.4937142278833102, 0.004055490097645274)
```



<br>
