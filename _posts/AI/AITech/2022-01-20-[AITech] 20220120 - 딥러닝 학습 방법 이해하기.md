---
layout: single
title: "[AITech] 20220120 - 딥러닝 학습 방법 이해하기"
categories: ['AI', 'AITech']
toc: true
toc_sticky: true
tag: []
---



<br>

## 강의 복습 내용

### 딥러닝 학습 방법 이해하기

* **신경망**은 선형 모델과 활성화 함수가 합성된 **비선형 모델**입니다. 

  * 신경망에서 각 뉴런을 이어주는 화살표(엣지)가 **가중치**에 해당합니다. 

  * 각 뉴런은 계산된 **XW**에 더해질 편향 값 **b**를 가지고 있습니다. 

  * 선형 모델에 의해 계산된 **O = XW + b**는 활성화 함수를 지나 비선형성을 얻게 됩니다. 

    * 활성화 함수에는 전통적으로 사용되어 온 **Sigmoid와 tanh**, 최근 많이 사용되는 **ReLU** 등이 있습니다. 

    ![image-20220120114123889](https://user-images.githubusercontent.com/70505378/150361220-72873270-e83c-41db-8239-594b4b71cc9a.png)

  * 다중 분류의 경우 최종 출력에 **소프트맥스 함수**를 적용시켜 **확률 벡터**로 변환할 수 있습니다. 

    * softmax 함수를 지난 벡터의 값은 **특정 클래스 k에 속할 확률**로 해석할 수 있습니다. 

    ![image-20220120114414683](https://user-images.githubusercontent.com/70505378/150361221-5dca2188-f2bc-46ab-997b-8e04c16c695d.png)

    * 추론을 할 때는 **원-핫 벡터**로 최댓값을 가진 주소만 1로 출력하는 연산을 사용해서 softmax를 사용하지 않습니다. 

* 다층 퍼셉트론(MLP)은 **신경망이 여러 층 합성된 함수**이다. 

  * 아래와 같이 **입력층-은닉층-출력층**을 거쳐 여러 번의 선형 계산과 활성화 함수 적용을 통해 입력 X로부터 출력 y를 찾아내는 것을 **순전파** 과정이라 한다. 

  ![image-20220120114550280](https://user-images.githubusercontent.com/70505378/150361225-9b7d007d-ee82-4d04-b395-b2e612eaf8ff.png)

  * 이론적으로는 2층 신경망으로도 임의의 연속 함수를 근사할 수 있다. 
  * 그러나 층이 깊을수록 **목적함수를 근사하는데 필요한 뉴런(노드)의 숫자가 훨씬 크게 줄어든다.** 층이 얇으면 한 층에 필요한 뉴런의 숫자가 기하급수적으로 늘어나고, 따라서 가중치의 수와 연산 횟수 또한 기하급수적으로 늘어나기 때문에 효율적이지 않다. 

* 딥러닝은 **역전파 알고리즘**을 이용하여 각 층에 사용된 파라미터 W, b를 학습한다. 

  * 각 층의 파라미터의 그레디언트 벡터는 윗 층부터 내려오며 역순으로 계산하게 된다. 

  ![image-20220120115148097](https://user-images.githubusercontent.com/70505378/150361229-f57fcdef-a681-47f6-805f-7e1a9a730d1e.png)

  * 역전파 알고리즘은 합성 함수 미분법인 **연쇄 법칙 기반 자동 미분**을 사용한다. 

<br>
