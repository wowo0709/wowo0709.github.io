---
layout: single
title: "[AITech][NLP] 20220310 - Part 2) RNN, LSTM, and GRU"
categories: ['AI', 'AITech']
toc: true
toc_sticky: true
tag: []
---



<br>

_**ë³¸ í¬ìŠ¤íŒ…ì€ KAIST 'ì£¼ì¬ê±¸' ê°•ì‚¬ ë‹˜ì˜ ê°•ì˜ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤. **_

# RNN, LSTM, and GRU

ì´ë²ˆ ê°•ì˜ëŠ” `RNN` ê³¼ `LSTM`, `GRU`ì— ëŒ€í•œ ë‚´ìš©ì…ë‹ˆë‹¤. 

## Basics of Recurrent Neural Networks (RNNs)

ë‹¤ë“¤ RNNì— ëŒ€í•´ì„œëŠ” ì§€ê²¹ë„ë¡ ë§ì´ ë“¤ìœ¼ì…¨ì„ ê²ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ì¥í™©í•˜ê²Œ ì„¤ëª…í•˜ì§€ ì•Šê³ , RNNì„ ê³µë¶€í•¨ì— ìˆì–´ ì¤‘ìš”í•˜ê³  í•µì‹¬ì ì¸ ë‚´ìš©ë“¤ì„ í‚¤ì›Œë“œ ìœ„ì£¼ë¡œ ì •ë¦¬í•´ë³´ê² ìŠµë‹ˆë‹¤. 

![image-20220317140619792](https://user-images.githubusercontent.com/70505378/158777963-92e8f91f-a889-4082-99ec-c0da2de37a4c.png)

### Sequence Data

* ì†Œë¦¬, ë¬¸ìì—´, ì£¼ê°€ ë“±ì˜ ë°ì´í„°ë¥¼ **ì‹œí€€ìŠ¤ ë°ì´í„°**ë¡œ ë¶„ë¥˜í•œë‹¤. 

* ì‹œí€€ìŠ¤ ë°ì´í„°ëŠ” **ë…ë¦½ë™ë“±ë¶„í¬(i.i.d.)** ê°€ì •ì„ ìœ„ë°°í•˜ê¸° ë•Œë¬¸ì— **ìˆœì„œë¥¼ ë°”ê¾¸ê±°ë‚˜ ê³¼ê±° ì •ë³´ì— ì†ì‹¤ì´ ë°œìƒí•˜ë©´ ë°ì´í„°ì˜ í™•ë¥ ë¶„í¬ë„ ë°”ë€ë‹¤.**

* ë”°ë¼ì„œ ì´ì „ ì‹œí€€ìŠ¤ì— ëŒ€í•œ ì •ë³´ë¥¼ ê°€ì§€ê³  ì•ìœ¼ë¡œ ë°œìƒí•  ë°ì´í„°ì˜ í™•ë¥  ë¶„í¬ë¥¼ ê³„ì‚°í•´ì•¼ í•˜ë©°, ì´ë¥¼ ìœ„í•´ ì¡°ê±´ë¶€ í™•ë¥ ì„ ì´ìš©í•  ìˆ˜ ìˆë‹¤. 

  ![image-20220121114516596](https://user-images.githubusercontent.com/70505378/150459169-72a12f32-2439-4e73-840a-559be2d27ff9.png)

  * ìœ„ ì¡°ê±´ë¶€ í™•ë¥ ì€ ê³¼ê±°ì˜ ëª¨ë“  ì •ë³´ë¥¼ ì´ìš©í•˜ì§€ë§Œ, ì‹œí€€ìŠ¤ ë°ì´í„°ë¥¼ ë¶„ì„í•  ë•Œ **ê³¼ê±°ì˜ ëª¨ë“  ì •ë³´ë“¤ì´ í•„ìš”í•œ ê²ƒì€ ì•„ë‹ˆë‹¤.**
    * ì–´ë–¤ ì‹œì ê¹Œì§€ì˜ ê³¼ê±°ì˜ ì •ë³´ë¥¼ ì´ìš©í•  ì§€ëŠ” ë°ì´í„°/ëª¨ë¸ë§ì— ë”°ë¼ ë‹¬ë¼ì§„ë‹¤. 

* ì‹œí€€ìŠ¤ ë°ì´í„°ë¥¼ ë‹¤ë£¨ê¸° ìœ„í•´ì„œëŠ” **ê¸¸ì´ê°€ ê°€ë³€ì ì¸ ë°ì´í„°**ë¥¼ ë‹¤ë£° ìˆ˜ ìˆëŠ” ëª¨ë¸ì´ í•„ìš”í•˜ë‹¤. 

  * ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ íŠ¹ì • êµ¬ê°„ _tau_ë§Œí¼ì˜ ê³¼ê±° ì •ë³´ë§Œì„ ì´ìš©í•˜ê³ , ê·¸ë³´ë‹¤ ë” ì „ì˜ ì •ë³´ë“¤ì€ **H<sub>t</sub>**ë¼ëŠ” ì ì¬ë³€ìˆ˜ë¡œ ì¸ì½”ë”©í•´ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. 
    * ì´ë ‡ê²Œ í•¨ìœ¼ë¡œì¨ ë°ì´í„°ì˜ ê¸¸ì´ë¥¼ ê³ ì •í•  ìˆ˜ ìˆê³ , ê³¼ê±°ì˜ ëª¨ë“  ë°ì´í„°ë¥¼ í™œìš©í•˜ê¸° ìš©ì´í•´ì§„ë‹¤. 
    * _tau_ êµ¬ê°„ ë§Œí¼ì˜ ê³¼ê±° ì •ë³´ë¥¼ ì´ìš©í•˜ëŠ” ëª¨ë¸ì„ **Auto Regressive Model**ì´ë¼ í•˜ê³ , í˜„ì¬ ì‹œì ì˜ ì…ë ¥ê³¼ ì¸ì½”ë”©ëœ ì ì¬ ì •ë³´ë¥¼ ì´ìš©í•˜ëŠ” ëª¨ë¸ì„ **Latent Autoregressive Model**ì´ë¼ í•œë‹¤. 

  ![image-20220121115240369](https://user-images.githubusercontent.com/70505378/150459172-8ac5d9c3-3ce5-49d9-9db9-8482e2071342.png)

  * ì´ ì ì¬ë³€ìˆ˜ H<sub>t</sub>ë¥¼ ì‹ ê²½ë§ì„ í†µí•´ ë°˜ë³µí•´ì„œ ì‚¬ìš©í•˜ì—¬ **ì‹œí€€ìŠ¤ ë°ì´í„°ì˜ íŒ¨í„´ì„ í•™ìŠµ**í•˜ëŠ” ì ì¬ íšŒê·€ ëª¨ë¸ì´ **RNN**ì´ë‹¤. 

    <img src="https://user-images.githubusercontent.com/70505378/150459173-7e1479bf-afb5-454a-8336-ef95b1282c8c.png" alt="image-20220121115433935" style="zoom:67%;" />

### RNN(Recurrent Neural Network)

* í˜„ì¬ ì •ë³´ë§Œì„ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ì™„ì „ì—°ê²°ì‹ ê²½ë§ì€ ê³¼ê±°ì˜ ì •ë³´ë¥¼ ë‹¤ë£° ìˆ˜ ì—†ë‹¤. 

* RNNì€ ì´ì „ ìˆœì„œì˜ ì ì¬ë³€ìˆ˜ì™€ í˜„ì¬ì˜ ì…ë ¥ì„ í™œìš©í•˜ì—¬ ëª¨ë¸ë§í•œë‹¤. 

  * W: tì— ë”°ë¼ ë¶ˆë³€/ X, H: tì— ë”°ë¼ ê°€ë³€

  ![image-20220121115906209](https://user-images.githubusercontent.com/70505378/150459174-3020b45d-4248-4ff1-b8fa-bf36b10fa114.png)

* **RNNì˜ ì—­ì „íŒŒ**ëŠ” ì ì¬ë³€ìˆ˜ì˜ ì—°ê²°ê·¸ë˜í”„ì— ë”°ë¼ ìˆœì°¨ì ìœ¼ë¡œ ê³„ì‚°í•œë‹¤. (ë§¨ ë§ˆì§€ë§‰ ì¶œë ¥ê¹Œì§€ ê³„ì‚°í•œ í›„ì— ì—­ì „íŒŒ)

  * ì´ë¥¼ **BPTT(Backpropagation Through Time)**ë¼ í•˜ë©° RNNì˜ ê¸°ë³¸ì ì¸ ì—­ì „íŒŒ ë°©ì‹ì´ë‹¤. 

  ![image-20220121120052256](https://user-images.githubusercontent.com/70505378/150459175-d0158bca-a493-49b7-9272-adc6d1ca8496.png)

  * BPTTë¥¼ í†µí•´ RNNì˜ ê°€ì¤‘ì¹˜ í–‰ë ¬ì˜ ë¯¸ë¶„ì„ ê³„ì‚°í•´ë³´ë©´ ì•„ë˜ì™€ ê°™ì´ **ë¯¸ë¶„ì˜ ê³±**ìœ¼ë¡œ ì´ë£¨ì–´ì§„ í•­ì´ ê³„ì‚°ëœë‹¤. 

    * ê·¸ ì¤‘ ë¹¨ê°„ìƒ‰ ë„¤ëª¨ ì•ˆì˜ í•­ì€ ë¶ˆì•ˆì •í•´ì§€ê¸° ì‰½ë‹¤. 
    * ì´ëŠ” ê±°ë“­ëœ ê°’ë“¤ì˜ ê³±ìœ¼ë¡œ ì¸í•´ ê°’ì´ ë„ˆë¬´ ì»¤ì§€ê±°ë‚˜(ê¸°ìš¸ê¸° í­ë°œ) ë„ˆë¬´ ì‘ì•„ì ¸(ê¸°ìš¸ê¸° ì†Œì‹¤) ê³¼ê±°ì˜ ì •ë³´ë¥¼ ì œëŒ€ë¡œ ì „ë‹¬í•´ì£¼ì§€ ëª»í•˜ê¸° ë•Œë¬¸ì´ë‹¤. 

    ![image-20220121120521892](https://user-images.githubusercontent.com/70505378/150459177-ae598173-a0df-431f-a4a6-538baf34ae44.png)

  * ê¸°ìš¸ê¸° í­ë°œ/ì†Œì‹¤ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì—­ì „íŒŒ ê³¼ì •ì—ì„œ **ê¸¸ì´ë¥¼ ëŠëŠ” ê²ƒ**ì´ í•„ìš”í•˜ë©°, ì´ë¥¼ **TBPTT(Truncated BPTT)**ë¼ í•œë‹¤. 

    ![image-20220121120714788](https://user-images.githubusercontent.com/70505378/150459180-30e736f3-3b17-4191-a09e-85417f3d37b5.png)

* ì—¬ëŸ¬ê°€ì§€ ë¬¸ì œë¡œ Vanilla RNNìœ¼ë¡œëŠ” ê¸´ ì‹œí€€ìŠ¤ë¥¼ ì²˜ë¦¬í•˜ëŠ”ë° í•œê³„ê°€ ìˆê³ , ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ **LSTM**ì´ë‚˜ **GRU**ì™€ ê°™ì€ ë°œì „ëœ í˜•íƒœì˜ ë„¤íŠ¸ì›Œí¬ë¥¼ ì‚¬ìš©í•œë‹¤. 

### Types of RNNs

* **One-to-one**: Standard Neural Networks
* **One-tomany**: Image Captioning
* **Many-to-one**: Setiment Classification
* **Sequence-to-sequence**: Machine Translation
* **Many-to-Many**: Video classification on frame level

![image-20220317141100058](https://user-images.githubusercontent.com/70505378/158777966-d93824ec-f7e6-4c9c-a8e7-517d5fc0fde1.png)









<br>

## LSTM & GRU

### LSTM

LSTM(Long Short Term Memory)ì€ Vanilla RNNì˜ í•œê³„ì¸ Long-term memoryë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì „ë‹¬í•˜ê¸° ìœ„í•´ ê³ ì•ˆëœ ëª¨ë¸ì…ë‹ˆë‹¤. 

![image-20220208183002864](https://user-images.githubusercontent.com/70505378/152983127-b220110a-625d-4706-96b6-dfa66117b563.png)

LSTMì˜ êµ¬ì¡°ëŠ” ë³µì¡í•´ë³´ì´ì§€ë§Œ **3ê°œì˜ Gateì™€ 1ê°œì˜ Cell** ë¶€ë¶„ë§Œ ì´í•´í•˜ë©´ ë©ë‹ˆë‹¤. ì „ì²´ì ì¸ input-output ê´€ê³„ë¶€í„° ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. 

* `i`: input gate, cell stateì— ì „ë‹¬í•  ì •ë³´ ìƒì„±
* `f`: forget gate,cell stateì— ì „ë‹¬í•˜ì§€ ì•Šê³  ë²„ë¦´ ì •ë³´ ìƒì„±
* `o`: output gate, ë‹¤ìŒ time stepì— ì „ë‹¬í•  hidden state ìƒì„±
* `g`: gate gate(update cell), í˜„ì¬ time stepì˜ cell state ìƒì„±

(ifogë¼ê³  ì™¸ìš°ë©´ ì‰½ìŠµë‹ˆë‹¤ ğŸ˜Š)

![image-20220317174937832](https://user-images.githubusercontent.com/70505378/158777968-6059baf9-d99a-4784-ab07-86d67b384f36.png)

input `x`ì™€ `h`ì˜ ê¸¸ì´ë¥¼ ëª¨ë‘ **h**ë¼ í•œë‹¤ë©´, LSTMì˜ ì „ì²´ ê°€ì¤‘ì¹˜ Wì˜ íŒŒë¼ë¯¸í„° ê°œìˆ˜ëŠ” **4h \* 2h = 8h<sup>2</sup>**ìœ¼ë¡œ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. `W * [h, x]`ë¡œ êµ¬í•œ  ì¶œë ¥ í–‰ë ¬ì˜ ê¸¸ì´ëŠ” **4h**ê°€ ë˜ê³ , ì´ëŠ” ê°ê° i/f/o/g gateì— **h** ì”© ë¶„ë°°ë©ë‹ˆë‹¤. ê°ê°ì˜ gateëŠ” ê°€ì¤‘ì¹˜ì— activationì„ ì ìš©í•˜ì—¬ outputì„ ë§Œë“¤ê²Œ ë©ë‹ˆë‹¤. 

ì´ ë•Œ ì£¼ëª©í•  ê²ƒì€, ë‹¹ì—°í•˜ê²Œë„ ì…ë ¥ hidden state `h`ì™€ ì¶œë ¥ hidden state `o`ì˜ ì°¨ì›ì´ ë™ì¼í•˜ê²Œ **h**ë¼ëŠ” ê²ƒì…ë‹ˆë‹¤. ë˜í•œ ë§Œì•½ biasë„ ì¡´ì¬í•œë‹¤ë©´, LSTMì˜ ì „ì²´ parameterì˜ ê°œìˆ˜ëŠ” **4h \* (2h+1) = 8h<sup>2</sup> + 4h** ê°œê°€ ë  ê²ƒì…ë‹ˆë‹¤ (ë‹¨, ì—¬ê¸°ì„œ '2h'ëŠ” 'xì˜ ê¸¸ì´+hì˜ ê¸¸ì´'ì¸ ê²ƒì„ ì£¼ì˜í•´ì£¼ì„¸ìš”). 

ì¶”ê°€ì ìœ¼ë¡œ activation functionì— ëŒ€í•œ ì–˜ê¸°ë¥¼ ì¡°ê¸ˆ ë” í•´ë³´ê² ìŠµë‹ˆë‹¤. i/f/o gateì˜ activationì¸ **sigmoid**ëŠ” ë‹¤ë“¤ ì˜ ì•„ì‹œë‹¤ì‹œí”¼ ê°’ì„ í™•ë¥ ë¡œ ë§¤í•‘í•´ì£¼ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. ì…ë ¥ ì •ë³´ì—ì„œ ëª‡ í¼ì„¼íŠ¸ì˜ ì •ë³´ë¥¼ ì¶œë ¥ìœ¼ë¡œ ì „ë‹¬í•  ì§€(ë˜ëŠ” ë²„ë¦´ì§€)ì— ëŒ€í•œ í™•ë¥  ê°’ì„ ìƒì„±í•´ì¤ë‹ˆë‹¤. Gate gate(update cell)ì˜ ê²½ìš° activationìœ¼ë¡œ **tanh**ë¥¼ ì‚¬ìš©í–ˆëŠ”ë°, tanhëŠ” ê°’ì„ -1 ~ 1 ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ë§¤í•‘í•´ì£¼ëŠ” í•¨ìˆ˜ë¡œì„œ ì…ë ¥ ì •ë³´ë¡œë¶€í„° ìœ ì˜ë¯¸í•œ ì •ë³´ë“¤ì„ ë½‘ì•„ë‚´ëŠ” ìš©ë„ë¡œ ì‚¬ìš©ëœë‹¤ê³  í•©ë‹ˆë‹¤. 

<br>

**Forget Gate**

Forget gateëŠ” previous output(hidden state) `h(t-1)`ê³¼ input `x(t)`ë¥¼ ì´ìš©í•´ ë§Œë“  ì •ë³´ `f(t)`ë¡œ **ì–´ë–¤ ì •ë³´ë¥¼ ë²„ë¦´ì§€** ê²°ì •í•©ë‹ˆë‹¤. 

![image-20220208183636794](https://user-images.githubusercontent.com/70505378/152983134-3eb40930-bec3-4e26-a2b2-f4f91aba8155.png)

**Input Gate**

Input gateëŠ” ë‘ ê°€ì§€ ì •ë³´ë¥¼ ìƒì„±í•˜ê³  ì´ìš©í•©ë‹ˆë‹¤. 

* `C'(t)`: Previous output(hidden state) `h(t-1)`ê³¼ input `x(t)`ë¥¼ ì´ìš©í•´ **í˜„ì¬ cell stateì— ì €ì¥í•  ì •ë³´ í›„ë³´**ë“¤ì„ ë§Œë“¤ì–´ëƒ…ë‹ˆë‹¤. 
* `i(t)`: Previous output(hidden state) `h(t-1)`ê³¼ input `x(t)`ë¥¼ ì´ìš©í•´ **ì •ë³´ í›„ë³´ë“¤ ì¤‘ ì–´ë–¤ ì •ë³´ë¥¼ ì €ì¥í•  ì§€** ì„ íƒí•©ë‹ˆë‹¤. 

ìµœì¢…ì ìœ¼ë¡œ ë§Œë“¤ì–´ì§„ ì •ë³´ `C'(t)`ì™€ `i(t)`ë¥¼ ì´ìš©í•˜ì—¬ **í˜„ì¬ cell state `C(t)`ì— ì „ë‹¬í•  ì •ë³´**ë¥¼ ë§Œë“¤ì–´ëƒ…ë‹ˆë‹¤. 

![image-20220208185133992](https://user-images.githubusercontent.com/70505378/152983138-0da04fb5-8dfb-4f2c-98c0-cc2c1e2d0d07.png)

**Update Cell**

Update cellì€ forget gateì™€ input gateì—ì„œ ë§Œë“¤ì–´ì§„ ì •ë³´ë“¤ `f(t)`, `C'(t)`, `i(t)`ê³¼ previous cell state `C(t-1)`ë¥¼ ì´ìš©í•´ **í˜„ì¬ cell state `C(t)`**ë¥¼ ë§Œë“¤ì–´ëƒ…ë‹ˆë‹¤. 

Cell stateì—ëŠ” ì™¸ë¶€ì—ëŠ” ë…¸ì¶œë˜ì§€ ì•ŠëŠ” **ì‹œê°„ 0~t ê¹Œì§€ì˜ ì •ë³´ë“¤ì´ ì¸ì½”ë”©**ë˜ì–´ ìˆìŠµë‹ˆë‹¤. 

![image-20220208185224572](https://user-images.githubusercontent.com/70505378/152983142-0bbd5d2a-faa2-40b8-9fd4-403abd974c04.png)

**Output Gate**

Output gateëŠ” ë¨¼ì € previous output(hidden state) `h(t-1)`ê³¼ input `x(t)`ë¥¼ ì´ìš©í•´ ë°–ìœ¼ë¡œ ë‚´ë³´ë‚¼(ì¶œë ¥í• ) ì •ë³´ í›„ë³´ `o(t)`ë¥¼ ë§Œë“¤ì–´ëƒ…ë‹ˆë‹¤. 

ê·¸ë¦¬ê³  ë§Œë“¤ì–´ì§„ `o(t)`ì™€ cell state `C(t)`ë¥¼ ì´ìš©í•´ **ë°–ìœ¼ë¡œ ë‚´ë³´ë‚¼ output(í˜„ì¬ hidden state) `h(t)`**ë¥¼ ë§Œë“¤ì–´ëƒ…ë‹ˆë‹¤. 

![image-20220208185257047](https://user-images.githubusercontent.com/70505378/152983145-eced45e3-2a3b-47cc-939b-019be3a377f4.png)

<br>

LSTMì˜ êµ¬ì¡°ë¥¼ ìš”ì•½í•´ì„œ ë‚˜íƒ€ë‚´ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. 

![image-20220208185800051](https://user-images.githubusercontent.com/70505378/152983146-670d9350-5fe1-4d82-a782-d6366e976e89.png)

<br>

### GRU

GRU(Gated Recurrent Unit)ëŠ” LSTMì˜ ê²½ë¸í™”ëœ í˜•íƒœë¼ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ë§Œ, ë†€ëê²Œë„ **GRUëŠ” ë” ì ì€ íŒŒë¼ë¯¸í„°ë¡œ ë†’ì€ í•™ìŠµ ì†ë„ì™€ ì¼ë°˜í™” ì„±ëŠ¥ì„ ë³´ì´ë©´ì„œ LSTMë³´ë‹¤ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ìì£¼ ë³´ì—¬ì¤ë‹ˆë‹¤.**

![image-20220208190144437](https://user-images.githubusercontent.com/70505378/152983149-67d746ac-fad8-4233-be3e-7bd8ddf4418e.png)

GRUëŠ” 2ê°œì˜ gate(**reset gate** and **update gate**)ë§Œì„ ì‚¬ìš©í•˜ë©°, **cell state** ì—†ì´ **hidden state**ë§Œì„ ì‚¬ìš©í•©ë‹ˆë‹¤. 

<br>

### Backpropagation in LSTM(GRU)

ë§ˆì§€ë§‰ìœ¼ë¢° ì™œ LSTMì´ gradient vanishing(exploding) ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆëŠ”ì§€ ë³´ê² ìŠµë‹ˆë‹¤. 

inputì— ë°˜ë³µì ìœ¼ë¡œ ë™ì¼í•œ W<sub>hh</sub>ë¥¼ ê³±í•˜ëŠ” Vaniila RNNê³¼ ë‹¬ë¦¬, LSTM(GRU)ì€ ê° time stepë§ˆë‹¤ cell stateì— ë³„ê°œì˜ f(forget gate)ì™€ elementwise multiplicationì„ ìˆ˜í–‰í•˜ê³  ì´í›„ addition ì—°ì‚°ë§Œì´ ìˆ˜í–‰ë˜ê¸° ë•Œë¬¸ì— gradient vanishing(exploding) ë¬¸ì œë¡œë¶€í„° ììœ ë¡­ìŠµë‹ˆë‹¤. 

ë”°ë¼ì„œ ë” ê¸´ sequenceì— ëŒ€í•´ì„œë„ gradientë¥¼ ì˜ ìœ ì§€í•˜ë©´ì„œ í•™ìŠµí•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤. 

![image-20220317182052761](https://user-images.githubusercontent.com/70505378/158777959-33bd8a84-a80b-48b0-9e4b-f439ab7b7253.png)



<br>

## ì‹¤ìŠµ) Word-level language modeling with RNN

ì´ë²ˆ ì‹¤ìŠµì—ì„œëŠ” ë°ì´í„° í† í°í™” ë° ì „ì²˜ë¦¬ì— ëŒ€í•œ ë‚´ìš©ì€ ë‹¤ë£¨ì§€ ì•ŠìŠµë‹ˆë‹¤. í•´ë‹¹ ë‚´ìš©ì´ ê¶ê¸ˆí•˜ì‹  ë¶„ë“¤ì´ ì œ ì´ì „ í¬ìŠ¤íŒ…ì¸ [Part 1) Bag or Words & Word Embedding]ì„ ì°¸ê³ í•˜ì‹œê¸¸ ë°”ëë‹ˆë‹¤. 

### ëª¨ë¸ ì•„í‚¤í…ì³ ì¤€ë¹„

* `RNNModel`: Embedding, RNN module, Projection ë¥¼ í¬í•¨í•œ ì»¨í…Œì´ë„ˆ ëª¨ë“ˆ. ë‹¤ìŒê³¼ ê°™ì´ ì´ì „ hidden stateì™€ inputì„ ë°›ì•„ ë‹¤ìŒ í† í°ì˜ log probabilityì™€ ë‹¤ìŒ hidden stateë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

![image-20220317183305124](https://user-images.githubusercontent.com/70505378/158785190-bff94862-b358-4f95-ae7e-d4b58b79184c.png)



<br>

ëª¨ë¸ì˜ forward ìˆœì„œëŠ” ë‹¤ìŒê³¼ ê°™ì´ êµ¬ì„±ë©ë‹ˆë‹¤. 

* `input`ì„ embedding layerì™€ dropout layerì— ì°¨ë¡€ë¡œ í†µê³¼ì‹œì¼œ embedded vectorë¥¼ ì–»ìŒ
* `embedded vector`ë¥¼ rnn layerì— í†µê³¼ì‹œì¼œ `output`ê³¼ `next_hidden`ì„ ì–»ìŒ
* `output`ì„ dropout layerì™€ projection layer(hidden dimension -> vocab_size)ë¥¼ í†µê³¼ì‹œí‚¨ í›„ softmaxë¥¼ ì ìš©í•˜ì—¬ `log_prob` ë¥¼ ì–»ìŒ

```python
class RNNModel(nn.Module):
    def __init__(self, 
        rnn_type: str,
        vocab_size: int,
        embedding_size: int=200,
        hidden_size: int=200,
        num_hidden_layers: int=2,
        dropout: float=0.5
    ):
        super().__init__()
        self.rnn_type = rnn_type
        self.vocab_size = vocab_size
        self.hidden_size = hidden_size
        self.num_hidden_layer = num_hidden_layers
        assert rnn_type in {'LSTM', 'GRU', 'RNN_TANH', 'RNN_RELU'}

        # ì •ìˆ˜ í˜•íƒœì˜ idë¥¼ ê³ ìœ  ë²¡í„° í˜•ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚´ê¸° ìœ„í•˜ì—¬ í•™ìŠµ ê°€ëŠ¥í•œ Embedding Layerë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        self.embedding = nn.Embedding(vocab_size, embedding_size)

        # Dropoutì€ RNN ì‚¬ìš©ì‹œ ë§ì´ ì“°ì…ë‹ˆë‹¤.
        self.dropout = nn.Dropout(dropout)

        if rnn_type.startswith('RNN'):
            # Pytorchì—ì„œ ì œê³µí•˜ëŠ” ê¸°ë³¸ RNNì„ ì‚¬ìš©í•´ ë´…ì‹œë‹¤.
            nonlinearity = rnn_type.split('_')[-1].lower()
            self.rnn = nn.RNN(
                embedding_size, 
                hidden_size, 
                num_hidden_layers,
                batch_first=True, 
                nonlinearity=nonlinearity,
                dropout=dropout
            )
        else:
            # Pytorchì˜ LSTMê³¼ GRUë¥¼ ì‚¬ìš©í•´ ë´…ì‹œë‹¤.
            self.rnn = getattr(nn, rnn_type)(
                embedding_size,
                hidden_size,
                num_hidden_layers,
                batch_first=True,
                dropout=dropout
            )

        # ìµœì¢…ì ìœ¼ë¡œ ë‚˜ì˜¨ hidden stateë¥¼ ì´ìš©í•´ ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡í•˜ëŠ” ì¶œë ¥ì¸µì„ êµ¬ì„±í•©ì‹œë‹¤.
        self.projection = nn.Linear(hidden_size, vocab_size)

    def forward(
        self, 
        input: torch.Tensor,
        prev_hidden: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]
    ):
        """ RNN ëª¨ë¸ì˜ forward í•¨ìˆ˜ êµ¬í˜„
        ìœ„ì˜ ê·¸ë¦¼ê³¼ __init__ í•¨ìˆ˜ ë‚´ ì£¼ì„ì„ ì°¸ê³ í•˜ì—¬ forward í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ì„¸ìš”.

        Hint 1: RNN ëª¨ë¸ì—ì„  Dropoutì„ ê³³ê³³ì— ì ìš©í•˜ëŠ” ê²ƒì´ ì„±ëŠ¥ì´ ì¢‹ë‹¤ê³  ì•Œë ¤ì ¸ ìˆìŠµë‹ˆë‹¤.
                ì˜ˆë¥¼ ë“¤ì–´, Embedding ì´í›„ì™€ Projection ì „ì—ë„ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        Hint 2: ìµœì¢… í™•ë¥ ê°’ì„ êµ¬í•˜ê¸° ìœ„í•´ì„œ Projection ì´í›„ì— F.log_softmaxë¥¼ ì‚¬ìš©í•˜ë©´ ë©ë‹ˆë‹¤.

        Arguments:
        input -- í† í°í™” ë° ë°°ì¹˜í™”ëœ ë¬¸ì¥ë“¤ì˜ í…ì„œ
                    dtype: torch.long
                    shape: [batch_size, sequence_lentgh]
        prev_hidden -- ì´ì „ì˜ hidden state
                    dtype: torch.float
                    shape: RNN, GRU - [num_layers, batch_size, hidden_size]
                           LSTM - ([num_layers, batch_size, hidden_size], [num_layers, batch_size, hidden_size])

        Return:
        log_prob -- ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡í•œ í™•ë¥ ì— logë¥¼ ì·¨í•œ ê°’
                    dtype: torch.float
                    shape: [batch_size, sequence_length, vocab_size]
        next_hidden -- ì´í›„ì˜ hidden state
                    dtype: torch.float
                    shape: RNN, GRU - [num_layers, batch_size, hidden_size]
                           LSTM - ([num_layers, batch_size, hidden_size], [num_layers, batch_size, hidden_size])
        """
        ### YOUR CODE HERE
        ### ANSWER HERE ###
        emb = self.dropout(self.embedding(input))
        output, next_hidden = self.rnn(emb, prev_hidden)
        log_prob = self.projection(self.dropout(output)).log_softmax(dim=-1)

        ### END YOUR CODE
        
        assert list(log_prob.shape) == list(input.shape) + [self.vocab_size]
        assert prev_hidden.shape == next_hidden if self.rnn_type != 'LSTM' \
          else prev_hidden[0].shape == next_hidden[0].shape == next_hidden[1].shape
        
        return log_prob, next_hidden
    
    def init_hidden(self, batch_size: int):
        """ ì²« hidden stateë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ """
        weight = self.projection.weight
        
        if self.rnn_type == 'LSTM':
            return (weight.new_zeros(self.num_hidden_layer, batch_size, self.hidden_size),
                    weight.new_zeros(self.num_hidden_layer, batch_size, self.hidden_size))
        else:
            return weight.new_zeros(self.num_hidden_layer, batch_size, self.hidden_size)
    
    @property
    def device(self):   # í˜„ì¬ ëª¨ë¸ì˜ deviceë¥¼ ë°˜í™˜í•˜ëŠ” í”„ë¡œí¼í‹°
        return self.projection.weight.device
    
    
    
rnn_type = 'LSTM'      # 'LSTM', 'GRU', 'RNN_TANH', 'RNN_RELU'
vocab_size = len(corpus.dictionary)
model = RNNModel(rnn_type, vocab_size=vocab_size)
```







<br>

### ëª¨ë¸ í•™ìŠµ

**ë°°ì¹˜í™”**

ì „ì²´ ë§ë­‰ì¹˜ì— ëŒ€í•´ RNN ê³„ì‚°ì„ í•˜ì—¬ ê¸°ìš¸ê¸°(Gradient)ë¥¼ ì—­ì „íŒŒí•˜ëŠ” ê²ƒì€ ì‹œê°„ë„ ì˜¤ë˜ê±¸ë¦´ ë¿ë§Œì´ ì•„ë‹ˆë¼ ë³‘ë ¬í™”ê°€ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ì´ë¥¼ ë°°ì¹˜ í¬ê¸° ë§Œí¼ ì˜ë¼ ê°ê°ì„ ë³„ê°œì˜ í•™ìŠµ ìƒ˜í”Œë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.

í˜„ì¬ ë°ì´í„°ì…‹ì€ í•œ ì¤„ë¡œ ê¸¸ê²Œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

```
[ a b c d e <eos> f g h i j k l m n <eos> o p q r s <eos> t u v w x y z <eos> ]
```

ì´ë¥¼ batch_size = `4`ë¡œ ë‚˜ëˆ„ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

```
[[ a b c d e <eos> ], 
 [ f g h i j k ],
 [ l m n <eos> o p ],
 [ r s <eos> t u v ]]
```

ê°œìˆ˜ê°€ ë¶€ì¡±í•˜ì—¬ ë°°ì¹˜ë¥¼ ëª» ì±„ìš´ ë¶€ë¶„ì€ ì˜ë¼ë²„ë¦½ë‹ˆë‹¤.

**Backpropagation through Time (BPTT)**

ë°°ì¹˜í™”ë¥¼ í•˜ì˜€ìŒì—ë„ ë¶ˆêµ¬í•˜ê³  í•˜ë‚˜ì˜ ìƒ˜í”Œì´ ë„ˆë¬´ ê¸¸ì–´ RNN ì—­ì „íŒŒë¥¼ í•˜ê¸°ì—ëŠ” ë‚œì ì´ ë§ìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•˜ì—¬ í•™ìŠµì— **BPTT**ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. BPTTëŠ” **í•œë²ˆì— sequence_length ë§Œí¼ì— ëŒ€í•´ì„œë§Œ ì—­ì „íŒŒë¥¼ ìˆ˜í–‰í•´ì„œ ì „ì²´ Sequenceë¥¼ í•™ìŠµ**ì‹œí‚¤ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

ì´ë¥¼ ìœ„í•˜ì—¬ í•œ ê° ìƒ˜í”Œì„ sequence_length ë‚˜ëˆ„ì–´ ì¤ë‹ˆë‹¤. ë°°ì¹˜í™”ëœ ë°ì´í„° ì…‹ì„ sequence_length = `2`ë¡œ ë‚˜ëˆ„ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. ì•„ë˜ì™€ ê°™ì€ ë°ì´í„°ì—ì„œ ëª¨ë¸ì€ [a, b]ë¥¼ í†µê³¼ì‹œí‚¤ê³  í•™ìŠµ í•œ ë’¤ [c, d]ë¥¼ í†µê³¼ì‹œí‚¤ê³  í•™ìŠµ, [e, \<eos\>]ë¥¼ í†µê³¼ì‹œí‚¤ê³  í•™ìŠµ...ì˜ ìˆœì„œë¡œ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤. 

```
[[[ a b ], [ c d ], [ e <eos> ]],
 [[ f g ],  [ h i ], [ j k ]],
 [[ l m ], [ n <eos> ], [ o p ]], 
 [[ r s ], [ <eos> t ], [ u v ]]]
```

í˜„ì¬ shapeëŠ” (batch_size, num_sample, sequence_length) ì…ë‹ˆë‹¤. **BPTTëŠ” num_sample ë¶€ë¶„ì„ ìˆœíšŒí•˜ë©´ì„œ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°**í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ì´ë¥¼ `reshape`í•˜ì—¬ **(num_sample, batch_size, sequence_length)**ë¡œ êµ¬ì„±í•˜ë©´ í¸ë¦¬í•©ë‹ˆë‹¤.

```
[[[ a b ], [ f g ], [ l m ], [ r s ]],
 [[ c d ], [ h, i ], [ n <eos> ], [ <eos> t ]],
 [[ e <eos> ], [ j k ], [ o p ], [ u v ]]]
```

ì´ë•Œ ì²«ë²ˆì§¸ ìƒ˜í”Œì¸ `[[ a b ], [ f g ], [ l m ], [ r s ]]`ëŠ” ê° ë°°ì¹˜ì˜ ì²«ë²ˆì§¸ sequenceì´ê³ , ë‘ë²ˆì§¸ ìƒ˜í”Œì¸ `[[ c d ], [ h, i ], [ n <eos> ], [ <eos> t ]]`ëŠ” ê° ë°°ì¹˜ì˜ ë‘ë²ˆì§¸ sequence, ê·¸ë¦¬ê³  ë§ˆì§€ë§‰ ìƒ˜í”Œì¸ `[[ e <eos> ], [ j k ], [ o p ], [ u v ]]]`ëŠ” ê° ë°°ì¹˜ì˜ ë§ˆì§€ë§‰ ë¶€ë¶„ì´ë¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

ìœ„ ëª¨ì–‘ìœ¼ë¡œ ë³´ë©´, ê° ë°°ì¹˜ì— ëŒ€í•œ í•™ìŠµì´ ìœ„ì—ì„œ ì•„ë˜ë¡œ ê°€ë©° ì´ë£¨ì–´ì§€ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ í•™ìŠµ ì½”ë“œ êµ¬í˜„ ì‹œ `for batch in data`ì™€ ê°™ì´ ì‘ì„±í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤. 

<br>

ì—¬ê¸°ì„œëŠ” ì¼ë ¬ë¡œ êµ¬ì„±ëœ dataë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ë°°ì¹˜í™”ëœ í˜•íƒœë¡œ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. 

```python
def bptt_batchify(
    data: torch.Tensor,
    batch_size: int,
    sequence_length: int
):
    ''' BPTT ë°°ì¹˜í™” í•¨ìˆ˜
    í•œ ì¤„ë¡œ ê¸¸ê²Œ êµ¬ì„±ëœ ë°ì´í„°ë¥¼ ë°›ì•„ BPTTë¥¼ ìœ„í•´ ë°°ì¹˜í™”í•©ë‹ˆë‹¤.
    batch_size * sequence_lengthì˜ ë°°ìˆ˜ì— ë§ì§€ ì•Šì•„ ë’¤ì— ë‚¨ëŠ” ë¶€ë¶„ì€ ì˜ë¼ë²„ë¦½ë‹ˆë‹¤.
    ì´ í›„ ë°°ìˆ˜ì— ë§ê²Œ ì¡°ì ˆëœ ë°ì´í„°ë¡œ BPTT ë°°ì¹˜í™”ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.

    Arguments:
    data -- í•™ìŠµ ë°ì´í„°ê°€ ë‹´ê¸´ í…ì„œ
            dtype: torch.long
            shape: [data_lentgh]
    batch_size -- ë°°ì¹˜ í¬ê¸°
    sequence_length -- í•œ ìƒ˜í”Œì˜ ê¸¸ì´

    Return:
    batches -- ë°°ì¹˜í™”ëœ í…ì„œ
               dtype: torch.long
               shape: [num_sample, batch_size, sequence_length]

    '''
    ### YOUR CODE HERE
    ### ANSWER HERE ###
    length = data.numel() // (batch_size * sequence_length) * (batch_size * sequence_length)
    batches = data[:length].reshape(batch_size, -1, sequence_length).transpose(0, 1)

    ### END YOUR CODE

    return batches
```

**ëª¨ë¸ í•™ìŠµ**

RNN ê³„ì—´ ëª¨ë¸ì˜ í•™ìŠµì€ ì´ì „ê¹Œì§€ ë‹¤ë¤„ì™”ë˜ image modelì˜ í•™ìŠµ ì½”ë“œì™€ëŠ” ë‹¤ë¦…ë‹ˆë‹¤. í° ê³¼ì •ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. 

* **optimizerë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤**

* RNN modelì— (batch, hidden)ì„ ì „ë‹¬í•˜ì—¬ (output, hidden)ì„ ë°›ìŠµë‹ˆë‹¤. 

  * BPTT í•™ìŠµì„ ìœ„í•´ ë§¤ batchë§ˆë‹¤ `hidden.detach()`ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤. 

* ë§ˆì§€ë§‰ ì˜ˆì¸¡ì„ ì œì™¸í•œ outputê³¼ ì²«ë²ˆì§¸ ë‹¨ì–´ë¥¼ ì œì™¸í•œ batch ì‚¬ì´ì˜ nll_lossë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. 

  * https://pytorch.org/docs/stable/generated/torch.nn.functional.nll_loss.html?highlight=nll_loss#torch.nn.functional.nll_loss 

    ```python
    >>> # input is of size N x C = 3 x 5
    >>> input = torch.randn(3, 5, requires_grad=True)
    >>> # each element in target has to have 0 <= value < C
    >>> target = torch.tensor([1, 0, 4])
    >>> output = F.nll_loss(F.log_softmax(input), target)
    >>> output.backward()
    ```

* `model.zero_grad()`, `loss.backward()`ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤. 

* `clip_grad_norm_` í•¨ìˆ˜ë¥¼ ì´ìš©í•´ gradientë¥¼ ê¸°ìš¸ê¸° í­ì£¼ë¥¼ ë°©ì§€í•˜ê³  ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. 

  * gradientë¥¼ ì¡°ì •í•˜ëŠ” ê³¼ì •ì´ í•„ìš”í•˜ê¸° ë•Œë¬¸ì— optimizerë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²ƒì…ë‹ˆë‹¤. 

```python
import math
from tqdm.notebook import tqdm
from torch.nn.utils import clip_grad_norm_

def train(
    model: RNNModel,
    data: torch.Tensor, # Shape: (num_sample, batch_size, sequence_length)
    lr: float
):
    model.train()
    batch_size = data.shape[1]
    total_loss = 0.

    hidden = model.init_hidden(batch_size)
    # tqdmì„ ì´ìš©í•´ ì§„í–‰ë°”ë¥¼ ë§Œë“¤ì–´ ë´…ì‹œë‹¤.
    progress_bar = tqdm(data, desc="Train")
    for bid, batch in enumerate(progress_bar, start=1):
        batch = batch.to(model.device) # RNN Modelì— ì •ì˜í–ˆë˜ device í”„ë¡œí¼í‹°ë¥¼ ì‚¬ìš©
        
        # íŠ¹ì´ì : optimizerë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ!!
		
        '''
        train ì‹œì—ëŠ” hiddenì„ detachí•´ì£¼ëŠ” ê³¼ì •ì´ í•„ìš”í•©ë‹ˆë‹¤. 
        ì´ëŠ” BPTTë¥¼ ìœ„í•œ ê²ƒìœ¼ë¡œ, ì´ì „ batch ì‹œ ê°±ì‹ ë‡ ê°€ì¤‘ì¹˜ëŠ” í˜„ì¬ batchì˜ gradientì— ì˜í•´ ê°±ì‹ ë˜ì§€ ì•Šê³  í˜„ì¬ì˜ ê°€ì¤‘ì¹˜ë§Œ ê°±ì‹ ë˜ë„ë¡ í•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤. 
        ì‰½ê²Œ ë§í•´, back propagationì„ ëŠì–´ì£¼ëŠ” ê²ƒì…ë‹ˆë‹¤. 
        '''
        output, hidden = model(batch, hidden)
        if model.rnn_type == 'LSTM':
            hidden = tuple(tensor.detach() for tensor in hidden)
        else:
            hidden = hidden.detach()

        # ì†ì‹¤ í•¨ìˆ˜ëŠ” Negative log likelihoodë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.
        '''
        ìœ„ì—ì„œ ë³¸ ì˜ˆì‹œë¡œë¶€í„° ì•„ë˜ ì‹ì„ ì„¤ëª…í•´ë³´ê² ìŠµë‹ˆë‹¤. ì•„ë˜ì™€ ê°™ì€ dataê°€ ìˆìŠµë‹ˆë‹¤. 
        
        [[[ a b ], [ f g ], [ l m ], [ r s ]],
 		[[ c d ], [ h, i ], [ n <eos> ], [ <eos> t ]],
 		[[ e <eos> ], [ j k ], [ o p ], [ u v ]]]
 		
 		ìš°ë¦¬ëŠ” ê° batch ë‹¨ìœ„ë¡œ forë¬¸ì„ ëŒë¦¬ê³  ìˆìœ¼ë‹ˆ ì²˜ìŒ batchëŠ” [[ a b ], [ f g ], [ l m ], [ r s ]] ì´ê² ì£ . 
 		ì—¬ê¸°ì„œ [a b] -> [b c], [f g] -> [g h], [l m] -> [m n], [r s] -> [s <eos>]ë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµë˜ì–´ì•¼ í•©ë‹ˆë‹¤. 
 		
 		ì´ ë•Œ 
 		output: [[ b c ], [ g h ], [ m n ], [ s <eos> ]] -> size: (batch_size, sequence_length, proj_size(vocab_size)) (outputì˜ b, c ë“±ì˜ ì›ì†ŒëŠ” ê° vocabì´ ë‹¤ìŒ ë‹¨ì–´ê°€ ë  ì ìˆ˜ê°’, ì¦‰ vocab_size í¬ê¸°ì˜ ë²¡í„°ì„)
 		batch: [[ a b ], [ f g ], [ l m ], [ r s ]] -> size: (batch_size, sequence_length)
 		ì™€ ê°™ì´ ë©ë‹ˆë‹¤. 
 		
 		ë”°ë¼ì„œ outputì˜ ë§ˆì§€ë§‰ ì¶œë ¥ê³¼ batchì˜ ì²˜ìŒ ì…ë ¥ì€ ë¹„êµí•  ëŒ€ìƒì´ ì—†ê¸° ë•Œë¬¸ì—, ì•„ë˜ì™€ ê°™ì´ ì¸ë±ì‹±ì„ í•´ì¤ë‹ˆë‹¤. 
        '''
        loss = F.nll_loss(output[:, :-1, :].transpose(1, 2), batch[:, 1:])
        
        model.zero_grad()
        loss.backward()
        
        # backwardëœ gradientë¥¼ ì¡°ì •í•˜ëŠ” ê³¼ì •ì´ í•„ìš”í•˜ê¸° ë•Œë¬¸ì— optimizer.step()ì„ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. 
        # clip_grad_norm_ì„ í†µí•´ ê¸°ìš¸ê¸° í­ì£¼ (Gradient Exploding) ë¬¸ì œë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
        clip_grad_norm_(model.parameters(), 0.25)
        for param in model.parameters():
            param.data.add_(param.grad, alpha=-lr)
        
        total_loss += loss.item()
        current_loss = total_loss / bid

        # PerplexityëŠ” ê³„ì‚°ëœ Negative log likelihoodì˜ Exponential ì…ë‹ˆë‹¤.
        progress_bar.set_description(f"Train - loss {current_loss:5.2f} | ppl {math.exp(current_loss):8.2f} | lr {lr:02.2f}", refresh=False)
```







<br>

### ëª¨ë¸ í‰ê°€

Evaluation ìš© ì½”ë“œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤. 

```python
@torch.no_grad()
def evaluate(
    model: RNNModel,
    data: torch.Tensor
):
    ''' ëª¨ë¸ í‰ê°€ ì½”ë“œ
    ëª¨ë¸ì„ ë°›ì•„ í•´ë‹¹ ë°ì´í„°ì— ëŒ€í•´ í‰ê°€í•´ í‰ê·  Loss ê°’ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    ìœ„ì˜ Train ì½”ë“œë¥¼ ì°¸ê³ í•˜ì—¬ ì‘ì„±í•´ë³´ì„¸ìš”.

    Arguments:
    model -- í‰ê°€í•  RNN ëª¨ë¸
    data -- í‰ê°€ìš© ë°ì´í„°
            dtype: torch.long
            shape: [num_sample, batch_size, sequence_length]

    Return:
    loss -- ê³„ì‚°ëœ í‰ê·  Loss ê°’
    '''
    
    model.eval()

    ### YOUR CODE HERE
    ### ANSWER HERE ###
    total_loss = 0.
    hidden = model.init_hidden(data.shape[1])
    
    for batch in data:
        batch = batch.to(model.device)
		
        # eval ì‹œì—ëŠ” ê°€ì¤‘ì¹˜ ê°±ì‹ ì´ ì¼ì–´ë‚˜ì§€ ì•Šê¸° ë•Œë¬¸ì— hidden.detach()ê°€ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. 
        output, hidden = model(batch, hidden)
        total_loss += F.nll_loss(output[:, :-1, :].transpose(1, 2), batch[:, 1:]).item()
    
    loss = total_loss / len(data)
    ### END YOUR CODE

    return loss
```

<br>

### ë¬¸ì¥ ìƒì„±

```python
from tqdm.notebook import trange

num_words = 1000
temperature = 1.0

hidden = model.init_hidden(1)
input = torch.randint(vocab_size, (1, 1), dtype=torch.long).to(device)
outputs = []

for i in trange(num_words, desc="Generation"):
    with torch.no_grad():
        log_prob, hidden = model(input, hidden)

    weights = (log_prob.squeeze() / temperature).exp()
    token_id = torch.multinomial(weights, 1)
    outputs.append(token_id.item())
    input = token_id.unsqueeze(0)

outputs = [corpus.dictionary.id2token[token_id] for token_id in outputs]

with open('generate.txt', 'w') as fd:
    fd.write(' '.join(outputs).replace('<eos>', '\n'))
    
'''
de Molina Minister , shows as part of the production of universe Catherine <unk> by one of the original case staff primaries in Jacob to the site that Innis 's ceiling .
Attached of the Australian organization was dressed formed by become currants of science on the fans , China . Russian initially felt bills feet of knowledge 's .....
'''
```

**Q.** ìœ„ ì½”ë“œì—ì„œ `temperature`ì˜ ì—­í• ì€?

temperatureëŠ” ë¬¸ì¥ ìƒì„±ì—ì„œ ë‹¤ì–‘ì„±ì„ ì¡°ì •í•œë‹¤. 

temperatureê°€ 1ë³´ë‹¤ ë†’ì•„ì§€ê²Œ ë˜ë©´ ìµœì¢… í™•ë¥  ê°’ì´ ì ì°¨ í‰íƒ„í•´ì§€ê³  ë¬´í•œëŒ€ë¡œ ê°€ë©´ ê· ì¼ ë¶„í¬ê°€ ëœë‹¤. ì´ëŠ” ì›ë˜ë¼ë©´ í™•ë¥ ì´ ë‚®ì€ í† í°ì´ ì¢€ ë” ì˜ ë½‘íˆê²Œ ëœë‹¤ëŠ” ëœ»ì´ê³ , ì´ëŠ” ë‹¤ì–‘í•œ ë¬¸ì¥ì„ ìƒì„±í•˜ëŠ”ë° ë„ì›€ì„ ì¤€ë‹¤. ê·¸ëŸ¬ë‚˜ ì´ë ‡ê²Œ ìƒì„±ëœ ë¬¸ì¥ì€ ë¬¸ë²•ì ìœ¼ë¡œ ë¶ˆì•ˆí•˜ê±°ë‚˜ ì–´ìƒ‰í•  í™•ë¥  ì—­ì‹œ ë†’ì•„ì§€ê²Œ ëœë‹¤. 

ë°˜ëŒ€ë¡œ temperatureê°€ 1ë³´ë‹¤ ë‚®ìœ¼ë©´ ì›ë˜ ë½‘í í™•ë¥ ì´ ë†’ì€ í† í°ì´ ë” ì˜ ë½‘íˆê²Œ ë˜ë©°, ì´ëŠ” ë‹¤ì–‘ì„±ì´ ë‚®ì•„ì§€ëŠ” ê²°ê³¼ë¡œ ê·€ê²°ëœë‹¤. ê·¸ëŸ¬ë‚˜ ì´ë ‡ê²Œ ìƒì„±ëœ ë¬¸ì¥ì´ ì¢€ ë” ì•ˆì •ì ì´ë‹¤.

















<br>

<br>

# ì°¸ê³  ìë£Œ

* 
