---
layout: single
title: "[AITech] 2022년 1월 28일 학습 내용 정리"
categories: ['AI', 'AITech']
toc: true
toc_sticky: true
tag: ['파이토치', '실습','MultiGPU', 'HyperparameterTuning', 'TransferLearning', 'TrobleShooting']
---



<br>

## 학습 내용 정리

### Multi-GPU 학습

#### 개념 정리

* `Single VS Multi`: 1개 VS 2개 이상
* `GPU VS Node`: GPU VS 컴퓨터
* `Single Node Single GPU`: 컴퓨터 1대에 GPU 1개
* `Single Node Multi GPU`: 컴퓨터 1대에 GPU 여러 개
* `Multi Node Multi GPU`: 컴퓨터 여러 대에 GPU 여러 대

#### Model Parallel

다중 GPU에 학습을 분산하는 방법에는 **모델을 나누는 방법**과 **데이터를 나누는 방법**이 있다. 

모델을 나누는 것은 비교적 예전부터 사용해온 기법(AlexNet)이지만, 모델의 병목이나 파이프라인의 어려움으로 인해 모델 병렬화는 곡난이도 과제이다. 

![image-20220128111218264](https://user-images.githubusercontent.com/70505378/151489744-cbbf842e-76e7-4dec-9281-932bcd8e3764.png)

* 예시 코드

```python
class ModelParallelResNet50(ResNet):
    def __init__(self, *args, **kwargs):
        super(ModelParallelResNet50, self).__init__(
        	Bottleneck, [3, 4, 6, 3], num_classes=num_classes, *args, **kwargs)
        
        self.seq1 = nn.Sequential(
        	self.conv1, self.bn1, self.relu, self.maxpool, self.layer1, self.layer2
        ).to('cuda:0')
        
        self.seq2 = nn.Sequential(
        	self.layer3, self.layer4, self.avgpool,
        ).to('cuda:1')
        
        self.fc.to('cuda:1')
        
    def forward(self, x):
        x = self.seq2(self.seq1(x).to('cuda:1'))
        return self.fc(x.view(x.size(0), -1))
```





#### Data Parallel

Data Parallel 기법은 데이터를 나눠 GPU에 할당한 후 결과의 평균을 취하는 방법입니다. 

![image-20220128112840914](https://user-images.githubusercontent.com/70505378/151489746-44d3d6e0-0f29-449f-b502-ed1e06867d4b.png)

위 그림을 보면 'Forward 시 분배가 일어나고 Backward가 완료된 후 취합'하는 것이 아니라, **중간에 Forward의 결과를 하나의 GPU가 취합한 후 gradient를 계산하고, 다시 분배하는 과정**이 일어나게 됩니다. 

이는 **Global Interpreter Lock**이라고 하는 파이썬의 멀티 프로세싱 상의 제약 사항 때문이라고 합니다. 

위와 같은 Data Parallel 기법은 파이토치에서 제공하는 DataParallel 클래스를 사용하여 간단히 구현할 수 있습니다. 

```python
parallel_model = torch.nn.DataParallel(model) # 이게 전부!!

# Forward ~ Loss Computation
predictions = parallel_model(inputs) # Forward pass on multi-GPUs
loss = loss_function(predictions, labels) # Compute loss function

# Gradient Backward propagation
loss.mean().backward() # Average GPU-losses + backward pass
optimizer.step() # Optimizer step

predictions = parallel_model(inputs) # Forward pass with new parameters
```

그런데 `DataParallel` 클래스는 위에서 말했듯이, 단순히 데이터를 분배한 후 평균을 취하고 다시 분배를 해주는 동작을 수행합니다. 

이는 **GPU 사용 불균형 문제**나 **Batch 사이즈 감소(취합하는 하나의 GPU의 병목)** 등의 문제를 야기합니다. 

<br>

이를 해결하는 방법으로 `DistributedDataParallel` 클래스를 사용할 수 있고, 해당 클래스는 **각 CPU마다 개별 process를 생성하여 GPU에 할당**함으로써 **중간에 취합하는 과정을 없앨 수 있습니다.**

사용하는 방법은 조금 더 복잡하지만 뛰어난 병렬화 효과를 볼 수 있습니다. 

```python
train_sampler = torch.utils.data.distributed.DistributedSampler(train_data)
shuffle = False
pin_memory = True

trainloader = torch.utils.data.DataLoader(train_data, batch_size=20, shuffle=True
										pin_memory=pin_memory, num_workers=3,
										shuffle=shuffle, sampler=train_sampler)

def main():
    n_gpus = torch.cuda.device_count()
    torch.multiprocessing.spawn(main_worker, nprocs=n_gpus, args=(n_gpus, ))
    
def main_worker(gpu, n_gpus):
    image_size = 224
    batch_size = 512
    num_worker = 8
    epochs = ...
    
    batch_size = int(batch_size / n_gpus)
    num_worker = int(num_worker / n_gpus)
    # 멀티 프로세싱 통신 규약 정의
    torch.distributed.init_process_group(
    		backend='nccl’ , init_method='tcp://127.0.0.1:2568’ , world_size=n_gpus, rank=gpu)
    
    model = MODEL
    # Distributed data parallel 정의
    torch.cuda.set_device(gpu)
    model = model.cuda(gpu)
    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[gpu])
```

 ✋ 파이썬의 멀티프로세싱 코드

```python
from multiprocessing import Pool

def f(x):
	return x*x

if __name__ == '__main__':
    with Pool(5) as p:
        print(p.map(f, [1, 2, 3]))
```

<br>

### Hyperparameter Tuning

모델의 성능을 높이는 데에는 크게 다음의 3가지 방법이 있습니다. 

1. 모델의 구조 개선: 현실적으로 큰 변화를 만들기 어렵다. 
2. 데이터 증강/보강: 가장 중요하면서 큰 효과를 볼 수 있다. 
3. 하이퍼파라미터 튜닝: 아주 큰 차이를 일으키지는 않지만 시도해 볼 만 하다. 

이 중 **하이퍼파라미터 튜닝**은 사실 이전에는 그 값에 의해 성능이 크게 좌우될 때도 있었지만, 요즘은 그렇지는 않다고 합니다. 

하지만 learning rate, 모델의 크기, batch size, optimizer 등 여러 하이퍼파라미터들을 튜닝하는 방법은 **마지막으로 모델의 성능을 조금 더 끌어올리고 싶을 때** 사용해 볼 만한 방법입니다. 

하이퍼 파라미터 튜닝 방법에는 전통적으로 사용되어 온 **grid search**와 **random search**가 있으며, 보통 random search로 튜닝을 하다가 성능이 좋은 부분이 발견되면 그 부분 부근에서 grid search를 수행하는 식으로 수행되었다고 합니다. 

![image-20220128115041782](https://user-images.githubusercontent.com/70505378/151489753-b580ccda-42df-428a-b598-dbe863baa8ec.png)

최근에는 두 방법 외에 **베이지안 기법**들이 주도하고 있습니다. 이에 대해 `BOHB(Baesian Optimization Hyper Band) 2018`이라는 논문을 읽어보면 도움이 될 것입니다. 

이번 포스팅에서는 이 하이퍼파라미터 튜닝 과정을 간소화 시켜주는 **Ray**라는 모듈에 대해 소개하고 사용하는 방법을 보려합니다. 

#### Ray

* Multi-node multi processing를 지원하며 ML/DL의 병렬 처리를 위해 개발된 모듈
* 기본적으로 현재의 분산 병렬 ML/DL 모듈의 표준
* Hyperparameter search를 위한 다양한 모듈 제공

```python
data_dir = os.path.abspath("./data")
load_data(data_dir)
# 1. config에 search space 지정
config = {
    "l1": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),
    "l2": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),
    "lr": tune.loguniform(1e-4, 1e-1),
    "batch_size": tune.choice([2, 4, 8, 16])
}
# 2. 학습 스케줄링 알고리즘 지정
scheduler = ASHAScheduler(
                metric="loss", mode="min", max_t=max_num_epochs, grace_period=1,
                reduction_factor=2)
# 3. 결과 출력 양식 지정
reporter = CLIReporter(metric_columns=["loss", "accuracy", "training_iteration"])
# 4. 병렬 처리 양식으로 학습 수행
result = tune.run(partial(train_cifar, data_dir=data_dir),
                resources_per_trial={"cpu": 2, "gpu": gpus_per_trial},
                config=config, num_samples=num_samples,
                scheduler=scheduler,
                progress_reporter=reporter)
```

Ray 모듈은 hyperparameter search를 수행할 때 처음에는 모든 경우로 학습을 시행하다가 성능이 좋지 않으면 해당 경우는 더 이상 학습을 시행하지 않고, 성능이 좋은 경우들로만 학습을 계속 수행하는 방식으로 리소스를 절약합니다. 

![image-20220128115742600](https://user-images.githubusercontent.com/70505378/151489755-8a135c5e-865f-4a99-98c1-4c29fdab9608.png)



또한 지난 포스팅에서 소개한 wandb 와 함께 사용하면 그 결과를 확인하기 더욱 좋기 때문에 두 모듈을 함께 활용하면 어렵지 않게 hyperparameter tuning을 수행할 수 있을 것으로 기대합니다. 

<br>

### PyTorch Troubleshooting

이 섹션에서는 모델 학습 과정에서 가장 많이 만나게 되는 에러이자, 해결하기 어려운 `OOM(Out of Memory)` 에러에 대해 얘기해보고자 합니다. 

**OOM이 해결하기 어려운 이유**

* 왜, 어디서 발생했는지 알기 어렵다. 
* Error backtracking이 이상한 데로 간다. 
* 메모리의 이전 상황의 파악이 어렵다. 

OOM을 해결하기 위해 가장 쉽게 시도해 볼 수 있는 방법으로는 **Batch size를 줄이는 시도**가 있습니다. 아, 그리고 batch size를 조정한 후에는 GPU clean(kernel restart) 과정을 해야 한다는 것을 잊지 마세요!

#### OOM 해결을 위한 방법들

**torch.cuda.empty_cache()**

empty_cache() 함수는 사용되지 않고 있는 GPU 상 cache를 정리합니다. (가비지 컬렉터를 호출하는 것으로 볼 수 있습니다)

이렇게 함으로써 가용 메모리를 확보할 수 있습니다. (메모리 주소의 참조를 끊는 del 과는 구분됩니다)

empty_cache() 함수는 학습 시작 전에 한 번 호출하는 것이 좋다고 합니다 ^_^

```python
import torch
from GPUtil import showUtilization as gpu_usage

print("Initial GPU Usage")
gpu_usage()
'''
Initial GPU Usage
| ID | GPU | MEM |
------------------
| 0 | 0% | 0% |
| 1 | 0% | 0% |
| 2 | 0% | 0% |
| 3 | 0% | 0% |
GPU Usage after allcoating a bunch of Tensors
'''
tensorList = []
for x in range(10):
	tensorList.append(torch.randn(10000000,10).cuda())
print("GPU Usage after allcoating a bunch of Tensors")
gpu_usage()
'''
GPU Usage after allcoating a bunch of Tensors
| ID | GPU | MEM |
------------------
| 0 | 0% | 40% |
| 1 | 0% | 0% |
| 2 | 0% | 0% |
| 3 | 0% | 0% |
'''
del tensorList
print("GPU Usage after deleting the Tensors")
gpu_usage()
'''
GPU Usage after deleting the Tensors
| ID | GPU | MEM |
------------------
| 0 | 0% | 40% |
| 1 | 0% | 0% |
| 2 | 0% | 0% |
| 3 | 0% | 0% |
'''
torch.cuda.empty_cache()
print("GPU Usage after emptying the cache")
gpu_usage()
'''
GPU Usage after emptying the cache
| ID | GPU | MEM |
------------------
| 0 | 0% | 5% |
| 1 | 0% | 0% |
| 2 | 0% | 0% |
| 3 | 0% | 0% |
'''
```

**training loop에 tensor로 축적되는 변수 확인**

tensor로 처리되는 변수들은 GPU 상에서 메모리를 사용하고, 해당 변수 loop 안에 연산이 있을 때 GPU에 computational graph를 생성하면서 메모리를 잠식해갑니다. 

따라서 이런 경우에는 1-d tensor의 경우 파이썬의 기본 객체(int, float, list 등)로 변환하여 처리할 것이 권장됩니다. 

```python
total_loss = 0

for x in range(10):
    # assume loss is computed
    iter_loss = torch.randn(3,4).mean()
    iter_loss.requires_grad = True
    # total_loss += iter_loss 대신, 
    iter_loss += iter_loss.item # 또는 float(iter_loss)
```

**del 명령어의 적절한 사용**

필요가 없어진 변수를 적절히 삭제하는 것도 방법이 될 수 있습니다. 

```python
for i in range(5):
    intermediate = f(input[i])
    result += g(intermediate)
    
del intermediate # del
output = h(result)
del result # del
return output
```

**배치 사이즈를 줄여보기(1로 해보기)**

**torch.no_grad()**

모델 추론 시점에는 역전파 과정이 필요 없으므로, `torch.no_grad()` context를 사용하여 backward 과정으로 인해 사용되는 메모리를 확보할 수 있습니다. 

```python
with torch.no_grad(): # torch.no_grad()
    for data, target in test_loader:
        output = network(data)
        test_loss += F.nll_loss(output, target, size_average=False).item()
        pred = output.data.max(1, keepdim=True)[1]
        correct += pred.eq(target.data.view_as(pred)).sum()
```

**tensor의 precision 줄이기**

tensor의 float precision을 8, 16bit 수준으로 줄이는 것도 하나의 방법입니다. 

그러나 이 방법은 모델의 성능에 직접적인 영향을 줄 수 있고, 매우 큰 모델을 돌리는 것이 아니라면 권장되지 않기 때문에 '최후의 수단' 정도로 생각해 두는 것이 좋을 듯 합니다. 

<br>

이외에도 **CUDNN_STATUS_NOT_INIT**이나 **device-side-assert** 등의 에러도 cuda와 관련하여 OOM의 일종이라고 할 수 있고, 역시 적절한 코드의 처리가 필요합니다. 이에 대해 참고할 만한 내용은 아래 참고자료 _GPU 에러 정리_ 에서 확인하실 수 있습니다. 

<br>

### 심화 과제: Transfer Learning & Hyperparameter Tuning

2주차 파이토치 심화 과제에서는 **전이 학습과 하이퍼파라미터 튜닝**에 관한 내용을 다루었는데요, 핵심 내용들만 정리해봅니다. 

#### Transfer Learning

* 대용량의 데이터(Source Task)로 학습된 이미 높은 성능을 보이는 모델을 나의 목적에 맞는 데이터(Target Task)로 재학습시켜 목적에 맞는 모델을 만드는 것
* Source task와 Target task에 정답(label) 유무에 따라 다양한 전이 학습 방법이 있는데, 그 중 두 task에서 모두 정답이 있는 경우에 **Fine-Tuning** 기법을 사용할 수 있습니다. 
  * Fine Tuning 방법에서는 가져온 모델을 전부 재학습 시킬 수도 있고, 특징 추출(Feature Extraction) 부분은 고정(frozen)시키고 분류 부분만 학습시킬 수도 있습니다.
  * 또는 epoch가 진행되면서 layer의 고정을 조금씩 풀어주는 방법도 있습니다.  

전이 학습 섹션에서는 ImageNet 데이터셋으로 학습된 ResNet18 모델을 Fashion MNIST 데이터셋으로 전이 학습시켰습니다. 

```python
imagenet_resnet18 = torchvision.models.resnet18(pretrained=True)
fashion_train = torchvision.datasets.FashionMNIST(root='./fashion', train=True, download=True)
fashion_test = torchvision.datasets.FashionMNIST(root='./fashion', train=False, download=True)
# 모델 구조 확인
print(imagenet_resnet18)
'''
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=512, out_features=1000, bias=True)
)
'''
```

<br>

전이학습을 시키기 위해 필수적으로 해야 하는 것이 2가지 있는데요, 이는 **모델의 입력/출력 layer 수정**과 **가중치 초기화**입니다. 

**모델 입력/출력 layer 수정**

ImageNet으로 학습된 ResNet18 모델의 입력 크기는 (3, 28, 28)이고, 우리의 목적인 Fashion MNIST의 크기는 (28, 28) 입니다. 여기서 채널 개수가 다르다는 것이 중요한데요, ImageNet의 채널 개수는 3이고 Fashion MNIST의 채널 개수는 1(grayscale)입니다. 

✋ 모델의 입력 채널 개수와 데이터 셋의 입력 채널 개수는 다음과 같이 확인할 수 있습니다. 

```python
'''CNN 모델의 입력 크기 확인하기'''
imagenet_resnet18.conv1.weight.shape # torch.Size([64, 3, 7, 7]) => (batch_size, channel, height, width)
imagenet_resnet18.conv1.weight.shape[1] # 채널 개수: 3
'''Fashion MNIST 데이터셋의 입력 크기 확인하기'''
fashion_train[0] # (<PIL.Image.Image image mode=L size=28x28 at 0x7F6608B19BD0>, 9)
np.array(fashion_train[0][0]).shape # (28, 28)
```



Convolution 연산 시 kernel의 channel 수는 input의 channel 수와 동일해야 하기 때문에 첫번째 convolution layer를 수정해야 합니다. 

```python
target_model = imagenet_resnet18

FASHION_INPUT_NUM = 1
target_model.conv1 = torch.nn.Conv2d(FASHION_INPUT_NUM, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
```

> [Con2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html?highlight=conv2d#torch.nn.Conv2d) 모듈의 인터페이스
>
> torch.nn.Conv2d(*in_channels*, *out_channels*, *kernel_size*, *stride=1*, *padding=0*, *dilation=1*, *groups=1*, *bias=True*, *padding_mode='zeros'*, *device=None*, *dtype=None*)

그리고 출력 layer를 우리 목적에 맞는 layer로 교체해주어야 합니다. 

Pretrained model의 출력층(Linear layer(FC layer))의 가중치의 개수는 (1000, 512)로, (out_features, in_features) 모양꼴이기 때문에 즉 output의 개수는 1000개 입니다. 이를 in_features는 동일하고 out_features는 target task, 즉 Fashion MNIST의 class의 개수와 일치하도록 교체해주어야 합니다. 

```python
FASHION_CLASS_NUM = 10
target_model.fc = torch.nn.Linear(in_features=512, out_features=FASHION_CLASS_NUM, bias=True)
```

<br>

**가중치 초기화**

이렇게 모델의 layer를 수정/교체해주었으면 초기화를 해줘야 합니다. 

보편적인 가중치 초기화 방법으로는 weight의 경우 Xavier Initialization으로, bias의 경우 in_features 크기를 n이라 했을 때 U(-1/root(n), 1/root(n))의 uniform distribution으로 해주는 방법이 있습니다. 

```python
  torch.nn.init.xavier_uniform(target_model.conv1.weight)
  torch.nn.init.xavier_uniform_(target_model.fc.weight)
  stdv = (1/target_model.fc.in_features)**(1/2)
  torch.nn.init.uniform_(target_model.fc.bias, -stdv, stdv)
```

> [torch.nn.init.xavier_uniform_()](https://pytorch.org/docs/stable/nn.init.html?highlight=uniform#torch.nn.init.xavier_uniform_)의 인터페이스
>
> torch.nn.init.xavier_uniform_(*tensor*, *gain=1.0*)
>
> [torch.nn.init.uniform_()](https://pytorch.org/docs/stable/nn.init.html?highlight=uniform#torch.nn.init.uniform_)의 인터페이스
>
> torch.nn.init.uniform_(*tensor*, *a=0.0*, *b=1.0*)

✋ 이외에도 카이밍 초기화([torch.nn.init.kaiming_uniform_()](https://pytorch.org/docs/stable/nn.init.html?highlight=uniform#torch.nn.init.kaiming_uniform_)), 정규 분포 초기화([torch.nn.init.normal_()](https://pytorch.org/docs/stable/nn.init.html?highlight=normal#torch.nn.init.normal_)), 상수 초기화([torch.nn.init.constant_()](https://pytorch.org/docs/stable/nn.init.html?highlight=torch%20init%20nn%20constant_#torch.nn.init.constant_)) 등 많은 방법이 있습니다. 

<br>

**모델 학습하기**

모델 학습에 대한 코드는 [PyTorch - Transfer Learning for Computer Vision Tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html#convnet-as-fixed-feature-extractor)에 자세하게 나와 있습니다. 

여기서 짚고 넘어갈 것은 전체 모델의 모든 layer를 재학습 시킬 수도 있고, feature extraction 부분은 고정시키고 classification 부분만 재학습 시킬 수도 있으며, 점차 layer의 고정을 풀어주는 식으로 재학습 시킬 수도 있는 여러 방법이 있다는 것입니다. 

```python
# 모델 가중치 고정시키기
for param in target_model.parameters():
    param.requires_grad = False
```







<br>

#### Hyperparameter Tuning

하이퍼파라미터 튜닝 섹션에서는 `ray`라는 모듈을 이용하여 튜닝을 수행하는 방법을 배웠습니다. ray 모듈은 Distributed application을 만들기 위한 프레임워크로, 분산 컴퓨팅 환경에서 많이 사용되고 있습니다. 그리고 ray 모듈 안에 있는 tune이라는 모듈을 이용하여 간단하게 하이퍼파라미터 튜닝을 수행할 수 있습니다. ([Tune Document](https://docs.ray.io/en/master/tune/index.html))

ray를 이용한 튜닝 방법을 코드 레벨에서 보기 전에, 튜닝을 할 때는 다음 2가지에 대해 생각해봅시다. 

1. Tuning의 목적(종속변인)
   * 이는 우리가 튜닝을 하는 목적에 해당합니다. 즉, **어떤 값을 최대화(최소화)할 것인지**를 정하는 것입니다. 
   * 여기서는 Fashion MNIST Test dataset의 Accuracy의 최대화를 목표로 합니다. 
2. Tuning할 Hyperparameter(조작변인, 통제변인)
   * 조작변인은 값을 조정하며 최적 값을 탐색할 변수에 해당하고, 통제변인은 값을 고정시킬 변수에 해당합니다. 
   * 여기서는 조작변인으로 **Epoch, Batch size, Learning rate**를, 통제변인으로 **모델 구조 ImageNet Pretrained ResNet18, All Not-Freeze Fine Tuning**을 지정합니다. 

**ray 모듈 설치하기**

아래 커맨드를 통해 ray 모듈을 설치할 수 있습니다. 

```python
print("Install ray")
!pip uninstall -y -q pyarrow
!pip install -q -U ray[tune]
!pip install -q ray[debug]
```

**통제변인**

```python
# 통제 변인
## 1. imagenet_resnet18 모델
def get_imagenet_pretrained_model():
  imagenet_resnet18 = torchvision.models.resnet18(pretrained=True)
  target_model = imagenet_resnet18
  FASHION_INPUT_NUM = 1
  FASHION_CLASS_NUM = 10
    
  imagenet_resnet18.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  in_features = imagenet_resnet18.fc.in_features
  imagenet_resnet18.fc = torch.nn.Linear(in_features, FASHION_CLASS_NUM, bias=True)
  torch.nn.init.xavier_uniform_(imagenet_resnet18.fc.weight)
  stdv = (1/imagenet_resnet18.fc.in_features)**(1/2)
  torch.nn.init.uniform_(imagenet_resnet18.fc.bias, -stdv, stdv)

  return target_model
```

**조작변인**

```python
# 조작 변인
## 1. Learning Rate
def get_adam_by_learningrate(model, learning_rate:float):
  return torch.optim.Adam(model.parameters(), lr=learning_rate)
## 2. Epoch 개수
def get_epoch_by_epoch(epoch:int):
  return epoch
## 3. BatchSize 크기에 따른 데이터 로더 생성
common_transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])
fashion_train_transformed = torchvision.datasets.FashionMNIST(root='./fashion', train=True, download=True, transform=common_transform)
fashion_test_transformed = torchvision.datasets.FashionMNIST(root='./fashion', train=False, download=True, transform=common_transform)

def get_dataloaders_by_batchsize(batch_size:int):
  # Mnist Dataset을 DataLoader에 붙이기
  BATCH_SIZE = batch_size
  fashion_train_dataloader = torch.utils.data.DataLoader(fashion_train_transformed, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)
  fashion_test_dataloader = torch.utils.data.DataLoader(fashion_test_transformed, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)

  dataloaders = {
      "train" : fashion_train_dataloader,
      "test" : fashion_test_dataloader
  }

  return dataloaders
```

**탐색 구간과 탐색기 정하기**

ray에서 사용할 수 있는 탐색기에는 여러 종류가 있습니다. 더 다양한 탐색기들에 대한 내용은 [여기](Optimizer들은 https://docs.ray.io/en/master/tune/api_docs/suggestion.html#bayesopt)에서 확인할 수 있습니다. 

```python
from ray import tune
# 탐색할 하이퍼파라미터 config 설정
config_space = {
    "NUM_EPOCH" : tune.choice([4,5,6,7,8,9]),
    "LearningRate" : tune.uniform(0.0001, 0.001),
    "BatchSize" : tune.choice([32,64,128]),
}

from ray.tune.suggest.hyperopt import HyperOptSearch
# 탐색기 Optimizer 설정
optim = HyperOptSearch(
    metric='accuracy', # hyper parameter tuning 시 최적화할 metric을 결정합니다.
    mode="max", # target objective를 maximize 하는 것을 목표로 설정합니다
)
```

**Training 함수 작성**

```python
def training(
    config # 조작 변인 learning rate, epoch, batchsize 정보
):
  # 통제 변인
  target_model = get_imagenet_pretrained_model() 

  device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu") # 학습 때 GPU 사용여부 결정. Colab에서는 "런타임"->"런타임 유형 변경"에서 "GPU"를 선택할 수 있음
  target_model.to(device)

  # 조작 변인
  NUM_EPOCH = get_epoch_by_epoch(config["NUM_EPOCH"])
  dataloaders = get_dataloaders_by_batchsize(config["BatchSize"])
  optimizer = get_adam_by_learningrate(target_model, config["LearningRate"])

  ### 학습 코드 시작
  ...
    
  # epoch 종료
  tune.report(accuracy=best_test_accuracy.item(), loss=best_test_loss)
```

**Tuning 수행**

```python
from ray.tune import CLIReporter
import ray

NUM_TRIAL = 10 # Hyper Parameter를 탐색할 때에, 실험을 최대 수행할 횟수를 지정합니다.

reporter = CLIReporter( # jupyter notebook을 사용하기 때문에 중간 수행 결과를 command line에 출력하도록 함
    parameter_columns=["NUM_EPOCH", "LearningRate", "BatchSize"],
    metric_columns=["accuracy", "loss"])

ray.shutdown() # ray 초기화 후 실행

analysis = tune.run(
    training,
    config=config_space,
    search_alg=optim,
    #verbose=1,
    progress_reporter=reporter,
    num_samples=NUM_TRIAL,
    resources_per_trial={'gpu': 1} # Colab 런타임이 GPU를 사용하지 않는다면 comment 처리로 지워주세요
)
```

**결과 확인**

```python
best_trial = analysis.get_best_trial('accuracy', 'max')
print(f"최고 성능 config : {best_trial.config}")
# 최고 성능 config : {'NUM_EPOCH': 9, 'LearningRate': 0.0009309039165529126, 'BatchSize': 32}
print(f"최고 test accuracy : {best_trial.last_result['accuracy']}")
# 최고 test accuracy : 0.9143999814987183
```

<br>

이로써 pretrained model을 가져와 transfer learning을 수행하고 hyperparameter tuning까지 수행하는 과정을 코드 레벨에서 공부했습니다. 

























<br>

<br>

## 참고 자료

* **Multi-GPU**
  * [PyTorch Lightning Multi GPU 학습](https://pytorch-lightning.readthedocs.io/en/stable/advanced/multi_gpu.html)
  * [DDP Tutorial](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)
* **Hyperparameter Tuning**
  * [Hyperparameter Tuning with Ray Tune](https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html)
* **PyTorch Troubleshooting**
  * [Pytorch에서 자주 발생하는 에러 질문들](https://pytorch.org/docs/stable/notes/faq.html)
  * [OOM시에 GPU 메모리 flush하기](https://discuss.pytorch.org/t/how-to-clean-gpu-memory-after-a-runtimeerror/28781)
  * [GPU 에러 정리](https://brstar96.github.io/shoveling/device_error_summary/)





<br>

<br>

## 회고

오늘로써 한 주 동안의 PyTorch에 대한 학습이 끝이 났습니다. 

파이토치 개요부터 기본적 연산들, nn.Module, Dataset과 DataLoader, Transfer learning과 Hyperparameter tuning, Multi-GPU, Trouble shooting 등 다양한 내용들을 다루는 것이 쉽지는 않았지만, 적절한 실습과 개념 문제들이 병행된 것 같아 많은 것을 배울 수 있었던 한 주였습니다. 

이전까지는 파이토치를 거의 사용해 본 경험이 없기 때문에 한 주 동안의 학습만으로는 많은 부분을 커버하기는 어렵겠지만, 앞으로 직접 모델을 만들고 학습 시키고, 성능을 개선하려는 일련의 과정과 노력을 통해 앞으로 더욱 더 익숙해지리라 생각합니다. 

무엇보다 각 내용들을 산발적으로 배우는 것이 아니라, 모델을 만들고 학습시키는 일련의 과정에 따라 순서대로 한 부분 씩 배우는 것이 전체 흐름을 이해하는 데에도 많은 도움이 되었습니다!!😁😁

한 주 간의 파이토치 모델링 과정을 배우면서 그 간의 흐름을 간단히 아래와 같이 정리해보았습니다. 

* **데이터 전처리(Dataset, Transform, Compose)**
* **데이터 불러오기(DataLoader, Sampler)**
* **신경망 구성(nn.Module, pretrained model)**
* **오차함수 및 최적화 기법 선택(Loss, Optimizer, metrics)**
* **학습 및 추론 옵션 설정(transfer learning, hyperparameter tuning, multi-gpu, monitoring)**
* **훈련, 검증(training, validating, troubleshooting)**

다음 주부터 진행될 내용들도 기대가 많이 됩니다 😊

앞으로 계속 지치지 않고 잘 나아갔으면 좋겠습니다 ㅎㅎ

<br>

### 마스터 클래스

#### FAQ

* AI에 대한 지식, 모델 개발 능력도 중요하지만 **'프로그래밍 역량'**이 계속해서 중요해질 것이다. 
* 웹 프로그래밍 
  * 서버-클라이언트 관계, 데이터베이스, 요청-응답 등에 대해 배울 수 있다. 
  * **시스템을 하나 만들어봐라!**
* AI의 길?
  * 엔지니어란 문제를 푸는 데 능력을 쓰는 사람
  * **행복하게 살 수 있는 길을 찾아라!**
* 효율적으로 공부할 수 있는 방법?
  * 효율보다는 일단 '양'을 늘려라. 양을 늘리다 보면 어느 순간 그 내용들이 연결되는 듯한 느낌이 온다. 
  * 그리고 그 느낌이 오면 지식의 습득이 빨라진다. 
  * MLOps: 리눅스(Shell Script), 빅데이터에 대한 공부가 기본!
* **하나의 시스템을 만들어서 인공지능을 삽입하는 연습**
  * 인공지능의 실제 프로덕트에서의 작동 과정에 대해 잘 공부할 수 있음

#### Data Centric AI

* 코딩을 못 하면 ML/DL 어렵나요?
  * 어렵다! 
* ML/DL 세계의 변화
  * using pre-trained model
  * 모델 개발/하이퍼파라미터 튜닝 싸움
* Research ML
  * 데이터는 준비 -> 모델 개발 -> 하이퍼파라미터 튜닝 -> 논문
* Project-Real World ML
  * ML Code 개발은 매우 작은 부분이다. 
  * 모델은 AWS, Google 등에서 제공하는 모델을 쓰고, 데이터 수집과 전처리/시스템 최적화 등에 신경을 쓴다. 
* Issues
  * Data
    * 양질의 데이터 확보가 관건
    * Production time 데이터와 Experiment 데이터가 다른 문제도 발생
    * 끊임없이 데이터를 관리하고 확보하려는 노력이 필요
      * User generated data(기존의 플랫폼 기업): inputs, clicks for recommendation
      * System generated data: logs, metadata, prediction
      * Data Flywheel: 사용자들의 참여로 데이터를 개선
      * Data augmentation: 데이터를 임의로 추가 확보
    * Data drift
      * 시간이 지나면서 데이터는 계속 바뀐다! (OTT 플랫폼)
    * Data Feedback Loop
      * 사용자로부터 오는 데이터를 자동화하여 모델에 피딩해주는 체계가 필요
      * ML/DL 코드 이상의 네트워크 하드웨어부터 데이터 플랫폼까지의 이해
      * 앞으로의 많은 ML/DL 엔지니어가 가져야 할 역량 중 하나
      * 특히 대용량 데이터를 다뤄본 경험이 중요할 것(Multi GPU, 데이터를 다루는 앞뒷단 부분)
  * 앞으로 알아야 할 것을
    * **ML Ops**
    * **당연히 데이터베이스**
    * **Cloud - AWS, GCP, Azure**
    * **Spark (+ Hadoop)**
    * **Linux, Docker**
    * **스케줄링 도구들(쿠브플로우, MLFlow, AirFlow)**
  * RAY, DASK, RAPIDS
  * Model, Algorithms, Metrics, Hyperparameter tuning



















<br>
