---

layout: single
title: "[AITech][Image Classification] 20220224 - Ensemble"
categories: ['AI', 'AITech', 'ImageClassification']
toc: true
toc_sticky: true
tag: []
---



<br>

**_본 포스팅은 번개장터의 '김태진' 강사 님의 강의를 바탕으로 제작되었습니다._** 

# 학습 내용

이번 포스팅에서는 모델링의 마지막 단계라고 할 수 있는 앙상블 기법에 대해 간단히 살펴보겠습니다. 

## Ensemble

여러 실험들을 하다보면 여러가지 모델로 여러 결과가 만들어지게 됩니다. `Ensemble` 기법은 싱글 모델보다 더 나은 성능을 위해 **서로 다른** 여러 학습 모델을 사용하는 것을 말합니다. 

### Model Averaging

각 모델이 예측한 결과를 종합하여 최종 결과를 내리는 방법을 `Voting`이라고 하고, Voting에는 **Hard Voting**과 **Soft Voting**이 있습니다. 

이렇게 병렬적으로 여러 개의 모델을 활용하는 것을 **Bagging** 기법이라고 합니다. 반대로, 이전 모델의 성능을 조금씩 개선해나가는 식으로 여러 개의 모델을 활용하는 것을 **Boosting** 기법이라고 합니다. 

![image-20220226183626684](https://user-images.githubusercontent.com/70505378/155838433-8c625f1d-f51d-48af-b19b-33b187b599d7.png)

### (Stratified) K-Fold Cross Validation

Train dataset을 k 등분하여 train/validation set으로 번갈아가며 모두 사용하는 것을 **k-fold cross validation**이라고 합니다. 여기에 각 split의 클래스 분포를 전체 dataset 클래스 분포와 비슷하게 가져가는 것을 **Stratify**라고 합니다. 

 ![image-20220226183933389](https://user-images.githubusercontent.com/70505378/155838435-3ed2ab20-d5b5-4b67-809d-b29ffb085397.png)

### TTA (Test Time Augmentation)

테스트 이미지를 Augmentation 후 모델 추론, 출력된 여러가지 결과를 앙상블

![image-20220226184054504](https://user-images.githubusercontent.com/70505378/155838436-2e82d86d-b831-45ee-88e8-67311a6e1a8a.png)

### 성능과 효율의 Trade-off

위에서 살펴본 것처럼 Model Ensemble에는 모델을 여러 개 사용하는 것 뿐 아니라, 데이터 차원에서 Ensemble을 하거나 Test 시에 dataset으로 Ensemble을 할 수도 있습니다. 

여기서 알아야 할 것은, 앙상블 효과는 확실히 있지만 그만큼 학습, 추론 시간이 배로 소모된다는 것입니다. 

<br>

<br>

# 참고 자료

* 





<br>
