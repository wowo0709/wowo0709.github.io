---
layout: single
title: "[AITech][Semantic Segmentation][P stage] 20220513 - Solutions"
categories: ['AI', 'AITech']
toc: true
toc_sticky: true
tag: []
---



<br>

# Solutions

**발표 1팀**

* 모델 선정
  * PapersWithCode -> HRNet _ OCR, SwinV2(UperNet), BEiT
  * Libraries: MMsegmentation, Detectron2(SeMask), smp(Unet++)
* Data cleansing
  * 전체 데이터 전수조사
  * 물체가 있음에도 인식하지 못한 경우, 라벨링이 잘못된 경우, mask 순서가 잘못된 경우, 비닐봉투 내부를 인식한 데이터
  * 결과를 구글 스프레드에 기록
  * 꽤 성능 향상이 있었음
* Data imbalance
  * Class 별 pixel 수 -> Battery가 압도적으로 적은 pixel을 차지
  * 모델 학습 과정에서는 오히려 battery는 잘 잡는 편이었다! -> 데이터 수가 적은 게 문제가 될까..?
  * 실제로 모델을 학습시켜보면서 확인 -> 오히려 general trach, plastic의 성능이 낮다!
  * 다른 쪽으로 접근하자!
* Data Remasking
  * 경계선의 픽셀값이 비어있는 이미지
  * Sliding Window 방식을 통하여 보정할 pixel인지 판단 (5x5 window, 3x3 window)
  * 큰 향상은 없었음
* Mask2Former 모델
  * 모델 설명
    * Pixel decoder를 통하여 뽑은 feature를 Transformer Decoder를 통하여 mask로 가공
    * Transformer Block 내부에 Masked Attention을 추가
    * **PointRend**: CG의 랜더링 기술 응용, 잘 분류하지 못하는 point을 sampling하여 선택된 point만으로 loss를 계산하여 연산량 감소
* Ensemble
  * Hard voting
  * Soft voting
* 후기 및 한계점
  * 저번 대회 때 사용했던 mmsegmentation 외에 detectron2, smp 등 사용
  * 한 번의 학습이 오래 걸리기 때문에 시간소비를 최소화할 수 있는 방법을 고안
  * 사용한 각각의 모델에 대해 시간적인 제약사항으로 인해 체계적인 실험을 하지 못 함
  * 모델 등에 대한 추가적인 학습 필요







<br>

**발표 2팀**

* Setup
  * mmsegmentation & smp
  * WandB (실험 관리)
  * Fiftyone (시각화)
* Data
  * Github issue를 통해 Train, Test 데이터에서 파악한 내용 공유
  * Annotation 수정 시도 -> 성능의 향상 없었음 -> 기존 그대로 유지
  * 좋은 Validation dataset을 찾기 위한 fold 실험
  * Geometry, Style Augmentation
    * RandomCrop이 많은 도움이 되었음 (global 정보 뿐 아니라 local한 특징을 파악하도록 함)
* Model
  * Framework 별로 다양성
  * Conv 베이스/Transformer 베이스
* Loss
  * CE+Dice loss
  * Decode Head LR에 가중치를 주는 trick을 사용
* Classification Head 사용
  * 큰 성능 향상
* Ensemble
  * Hard voting, Weighted Hard Voting
* Pseudo Labeling
  * 다양한 방식으로 시도
  * 가장 큰 성능 향상



<br>



**발표 3팀**

* Dataset
  * Validation score과 LB score가 align되는 validation set을 선택
* Model
  * SMP, MMSegmentation
* Augmentation
  * Data Imbalance
  * Copy-Paste Augmentation -> 오히려 성능 하락
  * 다양한 실험을 통해 custom augmentation 선정
* Pseudo Labeling
  * 성능의 큰 향상
* Ensemble
  * Hard voting, Soft voting
  * mIoU는 비슷하지만 모델 별로 각 클래스 별 IoU는 차이가 나는 경우가 많음
  * IoU가 높은 클래스에 가중치를 두어서 Soft Voting Ensemble 시도
  * 클래스에 대한 값들이 그대로 출력되도록 mmsegmentation의 segmentor 코드를 수정한 후 피클로 저장(255를 곱한 뒤 uint8로 캐스팅하여 파일 용량을 줄임)
  * 모델 소프트 보팅: 0.68 ~ 0.75 -> 0.8053
  * csv 파일 하드 보팅: 상위 5개 제출 파일 -> 0.8085

<br>

**마스터 피드백**

* segmentation의 특징: 모델 학습에 시간이 오래 걸리고, 데이터에 노이즈가 많다. 
* 지난 기수에 비해 점수가 향상되었고, 협업이 원활하게 진행되었다. 
* 작은 물체, 일부만 드러난 물체, 노이즈를 어떻게 다룰 지 등이 중요
* 발표 피드백
  * 1팀: 최신 모델 실험, 전체 데이터 전수조사, 문제점을 파악하고 해결하기 위한 노력
  * 2팀: 이미지 학습 중 시각화, correlation matrix 파악, layer나 loss마다 다른 가중치
  * 3팀: 실험 세팅, 효율적인 학습 및 실험, k-fold(val set), Ensemble 시 HPO























<br>

<br>

# 참고 자료

* 
