---
layout: single
title: "[AITech][Data Annotation] 20220413 - OCR 데이터 소개"
categories: ['AI', 'AITech']
toc: true
toc_sticky: true
tag: ['데이터 수집 방법', 'OCR 데이터셋', 'UFO format']
---



<br>

_**본 포스팅은 Upstage의 '이활석' 마스터 님의 강의를 바탕으로 작성되었습니다.**_

# OCR 데이터 소개

## Data Collection

강의에서는 아래 표와 같이 데이터셋 수집 방법을 분류했습니다. 

| 데이터셋        |                 |                     | 설명                                                         |
| --------------- | --------------- | ------------------- | ------------------------------------------------------------ |
| Public Dataset  |                 |                     | - 라벨링된 실제 이미지를 손쉽게 확보 가능 (가장 먼저 시도)<br />- 원하는 데이터가 없을 수 있다. <br />- 보통 수량이 적다. |
| Created Dataset | Synthetic Image |                     | - 라벨링 작업이 필요없다. <br />- 원하는 데이터를 빠르게 확보 가능<br />- 실제 데이터와 얼마나 다른 지 확인 필요 |
|                 | Real Image      | Crawled Image       | - 빠르게 이미지를 모을 수 있다. <br />- 고화질 이미지가 많지 않다. <br />- 다양한 샘플을 모으기 힘들다. <br />- 라이센스에 신경 써야 한다. |
|                 |                 | Crowd-sourced Image | - 비용이 크고 오래 걸린다. <br />- 원하는 고품질 데이터를 모을 수 있다. |









<br>

<br>

## Public Dataset

이번 섹션에서는 공개되어 있는 OCR dataset에는 무엇이 있는지 알아보겠습니다. 

먼저 public dataset을 빠르게 탐색해야 하는 이유는 아래와 같습니다. 

* 몇 장을 학습시키면 어느정도 성능이 나오는가?
* 어떤 경우가 일반적이고 어떤 경우가 희귀 케이스인가?
* 현재 최신 모델의 한계는 무엇인가?

위 질문들에 대한 답을 빠르게 얻기 위해 공개되어 있는 데이터셋으로 공개되어 있는 최신 모델을 학습시켜 성능을 분석하는 과정을 선행되어야 한다고 합니다. 

<br>

또한 강의에서는 데이터 검색 방법으로 크게 아래의 세가지 방법을 제시합니다. 

* 대회 데이터셋
  * Kaggle OCR 대회
  * RRC(Robust Reading Challenges): 2년 마다 열리는 OCR 전문 대회
* 논문
  * OCR 데이터셋 논문
  * Arxiv (ai 모든 논문), cvpr, iccv, aaai, icdar (OCR 전문 학회)
* 전문 사이트
  * Google Datasearch (데이터 전용 검색 플랫폼)
  * Zenodo.org
  * Datatang (데이터 유료 구매)

각 데이터셋은 언어, 용도(검출기/인식기/모두), 수량, 라이센스, 포맷 등에서 모두 다를 수 있기 때문에 이를 정확히 파악 및 비교하고 선택하는 것이 중요합니다. 

<br>

OCR 데이터셋에서 사용되는 용어에는 다음과 같은 것들이 있습니다. 

| 용어                | 설명                                               | 비고                                                         |
| ------------------- | -------------------------------------------------- | ------------------------------------------------------------ |
| Bounding Box (BBox) | 검출할 영역(글자, 단어 등)의 좌표                  | 직사각형 : (x, y, width, height), (x1,y1,x3,y3) <br />일반 사각형 : (x1,y1,x2,y2,x3,y3,x4,y4) <br />다각형 : 모든 점의 좌표들 등 다양한 형식 존재 |
| Text                | 영역 내의 존재하는 글자들의 시퀀스 (transcription) |                                                              |
| Don't care          | 검출해도 되고 안 해도 되는 영역                    | 학습 시에 사용하지 않음                                      |
| File name           | 이미지 파일의 이름                                 |                                                              |
| Image width         | 이미지 가로 길이                                   |                                                              |
| Image height        | 이미지 세로 길이                                   |                                                              |

<br>

시간이 지날수록 높은 성능을 요구하게 되고, 이에 따라 더욱 어려운 데이터셋들이 생성되었습니다. 처음에는 글자가 명확하게 보이는 이미지들 만을 데이터로 사용했다면, 요즘에는 우연히 찍힌 글자나 휘어지고 불규칙한 글자들에 대한 데이터셋들이 주를 이룹니다. 

### ICDAR 2015

* 총 1500장의 이미지와 그에 해당하는 gt text file
  * train 1000장, test 500장
* care, don't care로 구분하여 전사
  * care: 검출할 영역. 라틴 문자. 
  * don't care: 검출하지 않을 영역. 육안상 확인하기 어렵거나 라틴 문자가 아닌 글자. 

![image-20220420143239200](https://user-images.githubusercontent.com/70505378/164198239-f9e2a69b-ea1e-4480-96dc-9a4f5ed603a8.png)





### ICDAR 2017

* Multi-lingual Sence Text(MLT)
  * 9가지 언어: Chinese, Japanese, Korean, English, French, Arabic, Italian, German and Indian
  * 6가지 문자: "Arabic", "Latin", "Chinese", "Japanese", "Korean", "Bangla＂ + “symbols”, “mixed”
* 총 18000장
  * Train 9000 (각 언어별 1000장), test 9000
* Focused (Intentional) Scene Text
  * 우연히 찍힌 글자가 아닌, 글자 영역을 위주로 촬영된 이미지
  * 길거리 표지판, 광고판, 가게 간판, 지나가는 자동차 및 웹 microblog에 올라간 유저 사진 등
* gt파일 형식은 ICDAR 2015와 유사  

![image-20220420143407220](https://user-images.githubusercontent.com/70505378/164198181-9068c6c8-16e4-4930-86cc-c828c8b3972f.png)





### Recent Datasets

* ICDAR 2019, ArT (Arbitrary shaped Text)
* 기존에 존재하던 Total-Text, SCUT-CTW1500, Baidu Curved Scene Text 에 추가로 데이터를 수집 (3055장 + 7011장)
  * train: 5603장 , test: 4563장
* horizontal, multi-oriented, curved 등 다양한 형태 포함  

![image-20220420143938372](https://user-images.githubusercontent.com/70505378/164198191-8a26eb37-8377-47e0-904c-9a6fae02a846.png)





### AI HUB 야외 실제 촬영 한글 이미지

AI HUB에는 한글, 한자 인식을 위한 OCR 데이터셋들이 존재합니다. 

그 중 '야외 실제 촬영 한글 이미지'라는 데이터셋에 대해 살펴보겠습니다. 

![image-20220420144433925](https://user-images.githubusercontent.com/70505378/164198198-b8138627-dc68-4abd-b964-6fe18259c47f.png)

* 일상에서 접할 수 있는 다양한 한글 이미지를 촬영한 text-in-the-wild 이미지 데이터
  * 간판, 메뉴판, 책표지, 상품명 등을 촬영한 Focused Scene Text
* 총 약 50만 건
  * 1600*1200 해상도의 한글 이미지
  * 그 이미지와 1:1 쌍을 이루는 JSON 파일
* 특징
  * 이미지에 단어가 여러 개 있는 경우 글자 영역은 전체를 라벨링하되 transcription은 가장 잘 보이는 한글 위주로 최대 3개까지만 라벨링 작업을 실시함
  * 단어 기준 10자 이하만 transcription 라벨링
  * 인식대상을 제외한 한글의 경우 BBOX를 그린 후 don’t care처리(xxx 표기) 함  

![image-20220420144630963](https://user-images.githubusercontent.com/70505378/164198203-061a3a47-fe1c-4a1b-8f6c-fe6c2cdc6a5b.png)



<br>

여러 데이터셋들을 보면 알 수 있듯, 각 public dataset마다 annotation format이 다릅니다. 

이를 모델 학습에 사용하기 위해서는 일관된 하나의 데이터셋 포맷으로 변형하는 과정이 필요합니다. 

![image-20220420144729306](https://user-images.githubusercontent.com/70505378/164198208-69442ab3-3df5-48c7-acc7-daa1ba94c923.png)







<br>

<br>

## UFO Format

강의에서는 dataset들을 일관된 방식으로 읽어들이기 위한 방법으로 UFO format을 소개합니다. 

**UFO의 목적**

* 각각의 public dataset의 파일 형식 (json, txt, xml, csv 등)을 하나로 통합
* Detector, Recognizer, Parser 등 서로 다른 모듈에서 모두 쉽게 사용할 수 있어야 함
* 모델 개선을 위해 필요한 case에 대한 정보를 데이터에 포함시킬 수 있음
  * 예: 이미지 단위의 특징 (손글씨, blur 등), 글자 영역 단위의 특징 (가려짐, 글자 진행 방향 등)

**UFO 포맷의 특징**

* json 파일 안에서 element 탐색이 쉽게 graph structure를 기반으로 만들어짐
  * 하나의 이미지 내의 정보는 모두 parallel하게 존재

![image-20220420145049156](https://user-images.githubusercontent.com/70505378/164198211-7151ef30-c551-4178-afbd-6fc15d3a961c.png)

**Dataset 레벨**

한 데이터셋 내의 모든 이미지들에 관해 하나의 ufo 형식의 json 파일을 만든다. 

![image-20220420145122986](https://user-images.githubusercontent.com/70505378/164198216-23bacf16-1c54-4dac-9d4f-d3e2fe4debeb.png)

**Image 레벨**

* 각 이미지 별로 아래와 같은 정보가 담겨 있다. 
  * paragraphs, words, characters, image width, image height, image tag, annotation log, license tag
* 각각의 정보는 parallel하게 존재한다. 

![image-20220420145234437](https://user-images.githubusercontent.com/70505378/164198221-2dca1ae4-9588-450c-9cac-016043c4b86f.png)

**공통 요소**

* Ids: paragraph, image, character 레벨 각각에서 모두 id 넘버를 매김 (순서는 의미 없음)
* points: 각 라벨의 위치 좌표. 글자를 읽는 방향의 왼쪽 위에서부터 시계방향으로 x, y좌표를 nested list 형태로 기록함
  * 4points의 bbox가 기본적이나, 6, 8 등 2n 개의 points로 이루어진 polygon도 가능하다.
  * [[x1, y1], [x2, y2], [x3, y3], [x4, y4]]
* language: 사용된 언어
  * “en”, “ko", “others”, [“en”, “ko”] …
* tags: 성능에 영향을 주지만 별도로 기록하기 애매한 요소를 사전에 정의한 태그로 표시
  * 이미지 레벨의 image tag, 단어 레벨의 word tag
* confidence: ocr 모델이 예측한 pseudo-label의 경우 confidence score를 함께 표시  

**paragraph 레벨**

![image-20220420145719363](https://user-images.githubusercontent.com/70505378/164198222-8fbabdb2-5462-4cfd-bd3b-6ee3140cb86c.png)

**word 레벨**

![image-20220420145832483](https://user-images.githubusercontent.com/70505378/164198230-460a7b95-ce03-419c-b75e-f74b504f282e.png)

**character 레벨**

![image-20220420145941478](https://user-images.githubusercontent.com/70505378/164198233-385e9296-8eaf-4188-8422-bc4a1f73be89.png)

**Annotation log, License tag**

* Annotation log: 이슈 추적을 위한 정보 기록
  * Worker: 서버 내 로그인 아이디
  * Timestamp: 해당 파일이 만들어진 날짜와 시간
  * Tool_version: 작업자별 tool 정보
* License tag: 라이센스 정보
  * Usability: 데이터 사용 가능 여부
  * public: public dataset 여부
  * commercial: 상업적으로 사용 가능 여부
  * type: 라이센스의 구체적 종류
  * holder: 원본 데이터셋의 소유자  

![image-20220420150054241](https://user-images.githubusercontent.com/70505378/164198235-27defd83-2384-43a6-99b9-98040177e0bd.png)

**EDA**

* 이미지 당 단어 개수 분포
* Image width, height 분포
* 전체 단어 길이 분포
* 전체 BBOX 크기 분포
  * 넓이 기준
* 태그 개수 (histogram)
  * 언어
  * orientation
  * word tag
  * image tag
* Horizontal에 대한 aspect ratio (가로/세로)  





<br>

<br>

# 참고 자료

* 
