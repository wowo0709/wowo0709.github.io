---

layout: single
title: "[AITech] 20220209 - Attention&Transformer"
categories: ['AI', 'AITech']
toc: true
toc_sticky: true
tag: []
---



<br>

## í•™ìŠµ ë‚´ìš©

ì´ë²ˆ í¬ìŠ¤íŒ…ì—ì„œëŠ” ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ëŠ” `Transformer`ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ Encoder, Decoder, ê·¸ë¦¬ê³  ê·¸ ë‚´ë¶€ì— ìˆëŠ” Attentionì˜ êµ¬ì¡°ì™€ ë™ì‘ ë°©ì‹ì— ëŒ€í•´ ì•Œì•„ë³´ë ¤ í•©ë‹ˆë‹¤. 

(ì´ë²ˆ í¬ìŠ¤íŒ…ì—ì„œ ì‚¬ìš©í•˜ëŠ” ìë£Œì˜ ëŒ€ë¶€ë¶„ì€ [ì—¬ê¸°]([The Illustrated Transformer â€“ Jay Alammar â€“ Visualizing machine learning one concept at a time. (jalammar.github.io)](http://jalammar.github.io/illustrated-transformer/))ì—ì„œ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤)

### What is Transformer?

ê°„ë‹¨í•˜ê²Œ **Transformer**ê°€ ë¬´ì—‡ì¸ì§€ì— ëŒ€í•œ ì–˜ê¸°ë¶€í„° í•´ë´…ì‹œë‹¤. 

TransformerëŠ” ë³¸ë˜ ìì—°ì–´ ë²ˆì—­ì„ ìœ„í•œ ëª¨ë¸ë¡œ, **RNN êµ¬ì¡° ì—†ì´ Attentionì´ë¼ëŠ” ëª¨ë“ˆ**ì„ ë„ì…í•´ì„œ ë§¤ìš° ì„±ê³µì ì¸ performanceë¥¼ ë³´ì¸ ëª¨ë¸ì…ë‹ˆë‹¤. 

Transformerì˜ êµ¬ì¡°ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. 

![image-20220209144344432](https://user-images.githubusercontent.com/70505378/153200627-2499de50-375d-4fac-a5b3-5f80b429cb3b.png)

êµ¬ì¡°ë¥¼ ë³´ë©´ ì•„ë˜ì™€ ê°™ì€ ì •ë³´ë“¤ì„ ë°œê²¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

* TransformerëŠ” ê¸°ê³„ ë²ˆì—­ taskë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤. 
* TransformerëŠ” Encoder ë¶€ì™€ Decoder ë¶€ë¡œ ë‚˜ëˆ ì ¸ ìˆìŠµë‹ˆë‹¤. 
* Encoderì™€ Decoder ë¶€ëŠ” ê°ê° 6ê°œì˜ stacked êµ¬ì¡°ë¡œ ë˜ì–´ ìˆìŠµë‹ˆë‹¤. 

ê·¸ëŸ¼ ì´ì œ Transformerê°€ ë¬´ì—‡ì„ í•˜ëŠ” ë…€ì„ì´ê³ , ì–´ë–¤ êµ¬ì¡°ë¡œ ë˜ì–´ ìˆëŠ”ì§€ ë´¤ìœ¼ë‹ˆ, ê°ê°ì˜ ë¶€ë¶„ë“¤ì„ í•˜ë‚˜ì”© ëœ¯ì–´ë´…ì‹œë‹¤. 

<br>

### Encoder

Encoderì˜ êµ¬ì¡° ë‚´ë¶€ëŠ” ì•„ë˜ì™€ ê°™ì´ ìƒê²¼ìŠµë‹ˆë‹¤. 

![image-20220209144737148](https://user-images.githubusercontent.com/70505378/153200631-6800a280-6cf4-40c3-b46d-dff01aba50be.png)

ê·¸ëŸ¬ë©´ ì´ì œ ì € **Self-Attention** ëª¨ë“ˆì´ ë¬´ì—‡ì´ê³ , ì–´ë–»ê²Œ RNNì„ ëŒ€ì²´í–ˆëŠ”ì§€ì— ëŒ€í•´ ë´ì•¼ê² ì£ ?

#### **Attention**

**Attentionì´ ë¬´ì—‡ì¸ê°€?**

ìš°ì„  roughí•˜ê²Œ ë§í•˜ë©´, Attentionì€ Nê°œì˜ ë‹¨ì–´ë¡œ ì´ë£¨ì–´ì§„ ë¬¸ì¥ì„ ì²˜ë¦¬í•  ë•Œ **ì¬ê·€ì ìœ¼ë¡œ Në²ˆ ì²˜ë¦¬í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, í•œ ë²ˆì— Nê°œì˜ ë‹¨ì–´ë¥¼ ëª¨ë‘ ì´ìš©**í•©ë‹ˆë‹¤. ë‹¤ì‹œ ë§í•˜ë©´, 1ê°œì˜ ë‹¨ì–´ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ë‹¤ë¥¸ N-1ê°œì˜ ë‹¨ì–´ì— ëŒ€í•œ ì •ë³´ë¥¼ ë™ì‹œì— í™œìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. 

ì´ë ‡ê²Œ í•¨ìœ¼ë¡œì¨ ì–»ì„ ìˆ˜ ìˆëŠ” ì´ì ì€ ë¬´ì—‡ì„ê¹Œìš”? ì²«ì§¸, í•™ìŠµ ì‹œê°„ì„ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Në²ˆì˜ ê³¼ì •ì„ ê¸°ë‹¤ë ¤ì•¼ í–ˆë˜ RNNì— ë¹„í•´, ì´ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬í•˜ëŠ” TransformerëŠ” í›¨ì”¬ ë¹ ë¥¸ ëª¨ìŠµì„ ë³´ì…ë‹ˆë‹¤. ë‘˜ì§¸, ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì— ëŒ€í•œ ì •ë³´ë¥¼ ë” ì˜ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. RNN ëª¨ë¸ì—ì„œëŠ” ê¸´ ì‹œê³„ì—´ ë°ì´í„°ì— ëŒ€í•´ long term memoryì— ëŒ€í•œ í•œê³„ê°€ ìˆì—ˆë‹¤ë©´, Attention êµ¬ì¡°ì—ì„œëŠ” ë™ì‹œì— ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì— ëŒ€í•œ ì •ë³´ë¥¼ ì´ìš©í•˜ê¸° ë•Œë¬¸ì— ê·¸ëŸ° ë¬¸ì œê°€ ì—†ìŠµë‹ˆë‹¤. 

ë‹¤ë§Œ, ë‹¹ì—°í•˜ê²Œë„ ë§ì€ ì •ë³´ë¥¼ í•œ ë²ˆì— ì´ìš©í•˜ëŠ” Attention êµ¬ì¡°ëŠ” ë§ì€ Computational resourceë¥¼ ìš”êµ¬í•˜ê¸´ í•©ë‹ˆë‹¤. ì´ê²ƒì´ Transformerì˜ í•œê³„ë¡œ ì§€ì ë˜ê¸°ë„ í•˜ì£ . 

**Attentionì˜ ë™ì‘ êµ¬ì¡°**

ì–´ì°Œë˜ì—ˆë“ , Attentionì€ Nê°œì˜ ë‹¨ì–´ë¥¼ í•¨ê»˜ ê³ ë ¤í•˜ë©° ê° ë‹¨ì–´ì— ëŒ€í•´ ì²˜ë¦¬ë¥¼ í•©ë‹ˆë‹¤. ì•„ë˜ì²˜ëŸ¼ ë§ì´ì£ . 

![image-20220209145736590](https://user-images.githubusercontent.com/70505378/153200632-042c5e22-9608-4103-b857-5d83a3583665.png)



ìœ„ ê·¸ë¦¼ì„ ë³´ë©´ ê° ë‹¨ì–´ì— í•´ë‹¹í•˜ëŠ” xë²¡í„°ê°€ Attentionì˜ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°€, ì¶œë ¥ìœ¼ë¡œ zë²¡í„°ë“¤ì´ ë‚˜ì˜¤ëŠ”ë°, ê·¸ ê³¼ì •ì„ í•œ ë²ˆ ì‚´í´ë´…ì‹œë‹¤. 

Attentionì€ ì•„ë˜ì˜ ê³¼ì •ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. 

> 1. _`x`ì˜ Input featureë¥¼ embeddingí•˜ì—¬ Embedded vectorë¡œ ë³€í™˜_
> 2. _ê°ê°ì˜ embedded vectorì— ëŒ€í•´ **Query/Key/Value vector** í•œ ìŒì„ ìƒì„±_
>
> 3. _ê° ë‹¨ì–´ì— ëŒ€í•´ ìì‹ ì„ í¬í•¨í•œ ëª¨ë“  ë‹¨ì–´ë“¤ê³¼ Query vectorì™€ Key vectorë¥¼ ë‚´ì _ 
>    * ì´ ê°’ì„ Attention Scoreë¼ê³  í•¨
>    * ì´ Attention Scoreê°€ í•´ë‹¹ ë‹¨ì–´ì™€ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ê³¼ì˜ ê´€ê³„ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œë¼ê³  í•  ìˆ˜ ìˆìŒ
> 4. _ê°ê°ì˜ scoreë¥¼ root(d<sub>k</sub>)ë¡œ ë‚˜ëˆ„ê³  Softmaxë¥¼ ì ìš©_
>    * ì´ ë•Œì˜ d<sub>k</sub>ëŠ” dimension of key vector
>    * ì´ ê°’ì„ Attention weightë¼ê³  í•¨
> 5. _ë‚˜ëˆˆ ê°’ì— ê°ê°ì— í•´ë‹¹í•˜ëŠ” ë‹¨ì–´ì˜ Value vectorë¥¼ ìŠ¤ì¹¼ë¼ ê³±í•˜ê³  ëª¨ë‘ ë”í•¨ (Weighted Sum)_
>    * ì´ ê°’ì„ `z`ë¼ê³  í•¨

ì•„ë˜ëŠ” 2ê°œì˜ ë‹¨ì–´ë¡œ ì´ë£¨ì–´ì§„ ë¬¸ì¥(Thinking Machines)ì—ì„œ 'Thinking'ì´ë¼ëŠ” ë‹¨ì–´ë¡œë¶€í„° `z` ê°’ì„ ë„ì¶œí•˜ëŠ” ê³¼ì •ì„ ê·¸ë¦¼ìœ¼ë¡œ í‘œí˜„í•œ ê²ƒì…ë‹ˆë‹¤. 

![image-20220209151913555](https://user-images.githubusercontent.com/70505378/153200633-7a18bc5c-1431-429f-899b-80b5a5866ac5.png)

ìœ„ì™€ ê°™ì€ ê³¼ì •ìœ¼ë¡œ ê°ê°ì˜ xì— ëŒ€í•´ zë¥¼ êµ¬í•˜ëŠ”ë°ìš”, ì´ë¡œ ì¸í•´ RNNì—ì„œ Në²ˆì˜ ê³¼ì •ì„ ê±°ì³ì•¼ í–ˆë˜ ë³€í™˜ì€ Attentionì—ì„œëŠ” **ë‹¨ìˆœí•œ í–‰ë ¬ê³±**ìœ¼ë¡œ ëŒ€ì²´ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

ë°”ë¡œ ì•„ë˜ì™€ ê°™ì´, ë‹¨ì–´ë“¤ì˜ sequenceë¥¼ í–‰ë ¬ë¡œ ë‚˜íƒ€ë‚´ë©´ ê°ê° W<sup>Q</sup>, W<sup>K</sup>, W<sup>V</sup> í–‰ë ¬ê³¼ ê³±í•´ì„œ ë°”ë¡œ Q, K, V í–‰ë ¬ì„ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

![image-20220209152225437](https://user-images.githubusercontent.com/70505378/153200634-b0859c5b-469e-4480-b7e6-d5f73316a037.png)

ê·¸ë¦¬ê³  ìœ„ì—ì„œ êµ¬í•œ Q, K, V vectorë¥¼ ì´ìš©í•˜ë©´ ì•„ë˜ì™€ ê°™ì€ ê°„ë‹¨í•œ ìˆ˜ì‹ìœ¼ë¡œ `x`ì—ì„œ `z`ë¥¼ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

![image-20220209152816894](https://user-images.githubusercontent.com/70505378/153200640-55e73f43-53e1-45b2-a78f-312898ea3124.png)

ì´ëŸ° ê³¼ì •ì„ í†µí•´, Attentionì€ ìì—°ìŠ¤ëŸ½ê²Œ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ê³¼ì˜ ê´€ê³„ì„±ì„ í•™ìŠµí•˜ê²Œ ë˜ì£ . 

![image-20220209152449829](https://user-images.githubusercontent.com/70505378/153200636-833293b0-4dbf-47cf-9c6c-ba94578d4bb9.png)

#### MHA (Multi-Headed Attention)

ê·¸ëŸ°ë° ì‹¤ì œë¡œëŠ”, TransformerëŠ” ê° Encoder(ë˜ëŠ” Decoder)ë§ˆë‹¤ 8ê°œì˜ Attentionì„ ë³‘ë ¬ì ìœ¼ë¡œ í•¨ê»˜ ì‚¬ìš©í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ë‹¤ìŒê³¼ ê°™ì´ 8ê°œì˜ `z` ë²¡í„°ê°€ í•˜ë‚˜ì˜ Encoder ë‚´ì—ì„œ ìƒì„±ë©ë‹ˆë‹¤. 

![image-20220209152713155](https://user-images.githubusercontent.com/70505378/153200638-67f4c9bc-7bcb-468d-838a-5380c1964d6b.png)



ì´ë ‡ê²Œ í•˜ëŠ” ì´ìœ ëŠ”, ë°”ë¡œ ì•„ë˜ì™€ ê°™ì´ **ì—¬ëŸ¬ ê´€ì ì—ì„œ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ê³¼ì˜ ê´€ê³„ë¥¼ êµ¬í•˜ê¸° ìœ„í•¨**ì…ë‹ˆë‹¤. í•˜ë‚˜ì˜ Attentionë§Œì„ ì‚¬ìš©í•œë‹¤ë©´, ê·¸ Attentionì´ í•™ìŠµí•œ ì •ë³´ë°–ì—ëŠ” í™œìš©í•˜ì§€ ëª» í•˜ëŠ”ë° ë¹„í•´, ì—¬ëŸ¬ ê°œ Attentionì„ ì‚¬ìš©í•˜ë©´ ì—¬ëŸ¬ ê°œì˜ ê´€ì ìœ¼ë¡œ í•™ìŠµí•œ ì •ë³´ë“¤ì„ ëª¨ë‘ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ì£ . 

![image-20220209153259136](https://user-images.githubusercontent.com/70505378/153200642-526a1400-dcc5-4d5e-84f7-5aed81b08d1a.png)

#### ì¶œë ¥ í˜•íƒœ ë§ì¶°ì£¼ê¸°

ìœ„ì™€ ê°™ì€ ê³¼ì •ìœ¼ë¡œ MHAë¥¼ í†µê³¼í•˜ê³  ë‚˜ë©´, 8ê°œì˜ `Z` í–‰ë ¬ì´ ìƒì„±ë©ë‹ˆë‹¤. ì´ì œëŠ” ì´ `Z` í–‰ë ¬ì„ ì²˜ìŒ ì…ë ¥ í–‰ë ¬ì´ì—ˆë˜ `X`ì™€ ê°™ì€ í˜•íƒœë¡œ ë§ì¶°ì¤˜ì•¼ í•©ë‹ˆë‹¤. ì™œëƒí•˜ë©´ Encoder ë¶€ì—ëŠ” ì—¬ëŸ¬ ê°œì˜ stacked encoderë“¤ì´ ìˆê¸° ë•Œë¬¸ì— **ì´ë²ˆ encoderì˜ ì¶œë ¥ì€ ë‹¤ìŒ encoderë¡œì˜ ì…ë ¥ì´ ë©ë‹ˆë‹¤.** 

ì´ ê³¼ì •ì€ ë‹¨ìˆœíˆ í–‰ë ¬ `Wo`ì™€ í–‰ë ¬ê³±í•¨ìœ¼ë¡œì¨ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

![image-20220209154130564](https://user-images.githubusercontent.com/70505378/153200649-2b04cb46-dfa0-4722-902d-407d926315d7.png)

ê·¸ë˜ì„œ Self-Attention ëª¨ë“ˆì„ í†µê³¼í•˜ëŠ” ê³¼ì •ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. (Encoderì—ì„œëŠ” Self-Attention ëª¨ë“ˆ ì´í›„ì— FC-layerë¥¼ ì§€ë‚˜ì•¼ í•¨ì„ ìŠì§€ ë§ˆì„¸ìš”!)

![image-20220209154401681](https://user-images.githubusercontent.com/70505378/159019796-4fc6ab16-3b7c-413a-ae33-05ee1b60c94b.png)

#### Positional encoding & Residual connection

ì—¬ê¸°ê¹Œì§€ì˜ ê³¼ì •ì´ ì´í•´ë˜ì…¨ë‚˜ìš”? ì—¬ê¸°ì— 2ê°€ì§€ë§Œ ë” ì¶”ê°€í•´ë´…ì‹œë‹¤. í•˜ë‚˜ëŠ” **Positional encodding**ì´ê³  ë‹¤ë¥¸ í•˜ë‚˜ëŠ” **Residual connection**ì…ë‹ˆë‹¤. 

**Positional Encodding**

Sequential dataë¥¼ ë‹¤ë£¨ëŠ” ëª¨ë“  ëª¨ë¸ì—ì„œëŠ” **dataë“¤ì˜ ìˆœì„œ**ê°€ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤. ê·¸ëŸ°ë° ìœ„ì˜ ê³¼ì •ë§Œìœ¼ë¡œëŠ”, ê·¸ ë‹¨ì–´ë“¤ì˜ ìˆœì„œë¥¼ ì œëŒ€ë¡œ ê³ ë ¤í•´ì£¼ì§€ ëª»í•©ë‹ˆë‹¤. ë¬¸ì¥ì„ ì´ë£¨ëŠ” ë‹¨ì–´ë“¤ì´ ê°™ì•„ë„, ìˆœì„œê°€ ë‹¤ë¥´ë‹¤ë©´ ë‹¤ë¥¸ ì¶œë ¥ì´ ë‚˜ì˜¬ ìˆ˜ ìˆì–´ì•¼ í•˜ëŠ”ë°, ê·¸ëŸ¬ì§€ ëª»í•˜ëŠ” ê²ƒì´ì£ . 

ë°”ë¡œ ì´ ë‹¨ì–´ë“¤ì˜ ìˆœì„œë¥¼ ê³ ë ¤í•´ì£¼ê¸° ìœ„í•œ ê²ƒì´ positional encoddingì…ë‹ˆë‹¤. ê·¸ë¦¬ê³  ì´ëŠ” ë‹¨ìˆœíˆ Embedded vectorì— Positional encoddingì„ ìœ„í•œ í–‰ë ¬ì„ ë”í•´ì¤Œìœ¼ë¡œì¨ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

![image-20220209155752981](https://user-images.githubusercontent.com/70505378/153200653-b29bbf5c-9433-4cd7-af0b-f56f3348ee29.png)

ë…¼ë¬¸ì—ì„œëŠ” ì´ positional encoddingì— í•´ë‹¹í•˜ëŠ” ê°’ë“¤ì„ sin, cos ê°’ì„ ì´ìš©í•˜ì—¬ ìƒì„±í•˜ê³ , ì´ë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ ëŠ” ê·¸ ê°’ì´ -1~1ê¹Œì§€ì˜ ë²”ìœ„ë¥¼ ê°€ì§€ë©° ë‹¨ì–´ì˜ ê°œìˆ˜ì™€ ìƒê´€ì—†ì´ ëª‡ ê°œë“  ë§Œë“¤ì–´ë‚¼ ìˆ˜ ìˆëŠ” ì—°ì†í•¨ìˆ˜ì´ê¸° ë•Œë¬¸ì´ë¼ê³  í•©ë‹ˆë‹¤. 

**Residual Connection**

ë‘ë²ˆì§¸ë¡œ, Residual connectionì…ë‹ˆë‹¤. Transformerì˜ í•™ìŠµ ê³¼ì •ì—ì„œ backpropagationì´ ìˆ˜í–‰ë˜ë‹¤ ë³´ë©´, ìœ„ì—ì„œ ë³¸ positional encoddingì— ëŒ€í•œ ì •ë³´ê°€ ì†ì‹¤ë˜ê¸° ì‰½ìŠµë‹ˆë‹¤. ë°”ë¡œ ì´ ì •ë³´ë¥¼ ê²¬ê³ íˆ ìœ ì§€í•˜ê¸° ìœ„í•´ì„œ ì•„ë˜ì™€ ê°™ì´ Residual connectionì´ ì¡´ì¬í•©ë‹ˆë‹¤. 

(ì§„ì§œ ì§„ì§œ ë§ˆì§€ë§‰ìœ¼ë¡œ, residual connection ë‹¤ìŒì—ëŠ” layer normalizationì´ë¼ëŠ” ê²ƒì„ ì ìš©í•´ì„œ í•™ìŠµ íš¨ê³¼ë¥¼ ì¦ì§„ì‹œí‚µë‹ˆë‹¤ ğŸ˜Š)

![image-20220209160000883](https://user-images.githubusercontent.com/70505378/153200655-28d11568-334e-4444-bbf2-313f3360312d.png)

![image-20220209170248715](https://user-images.githubusercontent.com/70505378/153200660-1fe0dab1-144d-477a-aee1-93b65388fd75.png)

<br>

ìµœì¢… ì¸ì½”ë”ì˜ ì…ì¶œë ¥ í˜•íƒœëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. 

![image-20220209160018254](https://user-images.githubusercontent.com/70505378/153200658-bd799b2d-6ef2-46c2-956b-e8118b865dba.png)





<br>

### Encoder -> Decoder

ë‹¤ìŒìœ¼ë¡œ ì—¬ê¸°ì„œëŠ” Encoder ë¶€ë¥¼ ëª¨ë‘ í†µê³¼í•œ ì •ë³´ë“¤ì´ Decoder ë¶€ì— ì–´ë–»ê²Œ ì „ë‹¬ë˜ëŠ” ì§€ì— ëŒ€í•´ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤. ì—¬ê¸°ê¹Œì§€ ë”°ë¼ì˜¤ì…¨ë‹¤ë©´ ê±°ì˜ ë‹¤ ì™”ìŠµë‹ˆë‹¤!!!

ë§Œì•½ ìš°ë¦¬ê°€ 2ê°œì˜ ì¸ì½”ë”-ë””ì½”ë”ë¥¼ ì‚¬ìš©í•œë‹¤ë©´ ìµœì¢… Encoder ë¶€ì˜ ì¶œë ¥ì€ ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ Decoder ë¶€ì— ì „ë‹¬ë˜ëŠ”ë°ìš”, ê³¼ì—° **ì–´ë–¤ ì •ë³´ë“¤ì´ ì „ë‹¬ë˜ëŠ” ê²ƒì¼ê¹Œìš”?**

![image-20220209170756400](https://user-images.githubusercontent.com/70505378/153200662-291d94e1-a454-4647-a0cd-6b8a287f9a6d.png)

#### Encoderì—ì„œ K, Vë¥¼ Decoderì— ì „ë‹¬í•œë‹¤

Encoder ë¶€ì—ì„œ ì…ë ¥ `x`ê°€ ì¼ë ¨ì˜ encoderë“¤ì„ ëª¨ë‘ ì§€ë‚˜ê³  ë‚˜ë©´, ìœ„ì—ì„œ ë³¸ ê²ƒì²˜ëŸ¼ input xì™€ í˜•ìƒì´ ê°™ì€ output `z`ê°€ ì¶œë ¥ë  ê²ƒì…ë‹ˆë‹¤. ì´ ìµœì¢… ì¶œë ¥ `z`ë¥¼ ì´ìš©í•´ **Kì™€ V matrices**ë¥¼ ìƒì„±í•˜ê³ , ì´ ë‘ í–‰ë ¬ì„ **ê°ê°ì˜ Decoder**ì—ê²Œ ì „ë‹¬í•©ë‹ˆë‹¤. 

![](http://jalammar.github.io/images/t/transformer_decoding_1.gif)

ê·¸ë¦¬ê³  ì´ë ‡ê²Œ ì „ë‹¬ëœ Kì™€ V matrixëŠ” ê°ê°ì˜ Decoder ë‚´ì˜ **Encoder-Decoder Attention** ëª¨ë“ˆì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ Kì™€ VëŠ” **decoderê°€ input sequenceì—ì„œ ì–´ë–¤ ë¶€ë¶„ì— ì§‘ì¤‘í•´ì•¼ í•  ì§€**ë¥¼ ì•Œë ¤ì¤ë‹ˆë‹¤. 



<br>

### Decoder

ì, ì´ì œ Encoderì—ì„œ ì–´ë–»ê²Œ sequential dataë¥¼ ì²˜ë¦¬í•˜ê³ , Decoderì—ê²Œ ì–´ë–¤ ì •ë³´ë¥¼ ì–´ë–»ê²Œ ì „ë‹¬í•´ì£¼ëŠ” ì§€ê¹Œì§€ ë´¤ìŠµë‹ˆë‹¤. ì •ë§ ë‹¤ ì™”ìŠµë‹ˆë‹¤!

ì—¬ê¸°ì„œëŠ” ì´ì œ ë§ˆì§€ë§‰ìœ¼ë¡œ Decoder ë¶€ì—ì„œ ì–´ë–¤ ê³¼ì •ì„ í†µí•´ Transformerì˜ outputì„ ë§Œë“¤ì–´ë‚´ëŠ” ì§€ ì‚´í´ë³¼ ê²ƒì…ë‹ˆë‹¤. 

#### ê° stepì˜ outputì€ decoderì˜ ë‹¤ìŒ stepì˜ outputì„ ë§Œë“¤ê¸° ìœ„í•´ ì‚¬ìš©ëœë‹¤

Decoder ë¶€ëŠ” ì „ë‹¬ë°›ì€ `K`, `V`ì™€ ìì²´ì ìœ¼ë¡œ ìƒì„±í•œ `Q`ë¥¼ ì´ìš©í•˜ì—¬ í•œ stepë§ˆë‹¤ í•˜ë‚˜ì˜ outputì„ ë§Œë“¤ì–´ëƒ…ë‹ˆë‹¤. ì´ë ‡ê²Œ ë§Œë“¤ì–´ì§„ **ì´ì „ stepê¹Œì§€ì˜ outputë“¤ì€ ë‹¤ìŒ stepì˜ outputì„ ë§Œë“¤ê¸° ìœ„í•œ ì •ë³´ë¡œ ì‚¬ìš©**ë©ë‹ˆë‹¤. (ì—¬ê¸°ì„œ ì´ì „ stepê¹Œì§€ì˜ ì •ë³´ë“¤ë„ ë§ˆì°¬ê°€ì§€ë¡œ embeddingê³¼ positional encodingì´ ì ìš©ë©ë‹ˆë‹¤)

ê·¸ë¦¬ê³  ì´ëŠ” ë‹¤ë¥¸ ë§ë¡œ í•˜ë©´, **í˜„ì¬ step í›„ì˜ ì •ë³´ë“¤ì€ í˜„ì¬ stepì˜ outputì— ì˜í–¥ì„ ì£¼ì§€ ì•ŠëŠ”ë‹¤**ëŠ” ê²ƒì„ ë§í•©ë‹ˆë‹¤. ì´ëŠ” ëª¨ë“  ì •ë³´ë¥¼ í•¨ê»˜ ì‚¬ìš©í•˜ëŠ” Encoder ë¶€ì™€ ë‹¤ë¥¸ ì ì…ë‹ˆë‹¤. 

í˜„ì¬ step í›„ì˜ ì •ë³´ë“¤ì„ ì‚¬ìš©í•˜ì§€ ì•Šê¸° ìœ„í•´ decoderì˜ self-attention ëª¨ë“ˆì—ì„œëŠ” ë¯¸ë˜ì˜ ì •ë³´ë“¤ì„ masking(setting them to '-inf')í•©ë‹ˆë‹¤. ê·¸ë˜ì„œ decoderì˜ ì²«ë²ˆì§¸ self-attention ëª¨ë“ˆì€ masked-attention ëª¨ë“ˆì´ë¼ê³  ë¶ˆë¦½ë‹ˆë‹¤. 



![](http://jalammar.github.io/images/t/transformer_decoding_2.gif)

Decoderì˜ Encoder-Decoder Attention ëª¨ë“ˆì€ MHA(Multi-headed attention)ì™€ ë™ì¼í•˜ê²Œ ë™ì‘í•˜ë©°, ë‹¤ë¥¸ ì ì€ ì‚¬ìš©í•˜ëŠ” `K`, `V`ëŠ” Encoderë¡œë¶€í„° ì „ë‹¬ë°›ì€ ê°’ì„ ì‚¬ìš©í•˜ê³  `Q` ë˜í•œ ì§ì ‘ ìƒì„±í•´ë‚´ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì•„ë˜ layerì—ì„œ ìƒì„±ëœ ê°’ì„ ì‚¬ìš©í•œë‹¤ëŠ” ì ì…ë‹ˆë‹¤. 

#### ìµœì¢… Transformer ì¶œë ¥ ìƒì„±

ì¼ë ¨ì˜ stacked decoderë“¤ì„ ì§€ë‚˜ Decoderë¶€ì˜ ìµœì¢… ì¶œë ¥ì€ **vector of floats** ì…ë‹ˆë‹¤. ì´ê²ƒì„ ì–´ë–»ê²Œ ë‹¨ì–´ë“¤ë¡œ ë³€í™˜í•  ìˆ˜ ìˆì„ê¹Œìš”?

ê·¸ê²ƒì´ ë°”ë¡œ ìµœì¢…ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” **Linear layer & Softmax layer**ì˜ ì—­í• ì…ë‹ˆë‹¤. 

**Linear Layer**ëŠ” Decoder ë¶€ì˜ ìµœì¢… output vectorì— fully connect ì—°ì‚°ì„ ì ìš©í•˜ì—¬ **logits vector**ë¼ëŠ” ê²ƒì„ ìƒì„±í•©ë‹ˆë‹¤. ê·¸ë¦¬ê³  ì´ ë•Œì˜ **logits vectorì˜ í¬ê¸°ëŠ” Transformer ëª¨ë¸ì´ ì•Œê³  ìˆëŠ” ë‹¨ì–´ì˜ ìˆ˜(outputì´ ë  ìˆ˜ ìˆëŠ” ë‹¨ì–´ í›„ë³´ì˜ ìˆ˜)**ì™€ ê°™ìŠµë‹ˆë‹¤. 

ê·¸ë¦¬ê³  ì´ logits vectorë¥¼ **Softmax Layer**ë¥¼ ê±°ì³ ê°ê°ì˜ ê°’ì„ í™•ë¥  ê°’ìœ¼ë¡œ ë°”ê¾¸ê³ , **ê°€ì¥ í™•ë¥ ì´ ë†’ì€ ê°’ì— í•´ë‹¹í•˜ëŠ” ë‹¨ì–´ë¥¼ ì´ë²ˆ stepì˜ output**ìœ¼ë¡œ ì„ íƒí•˜ëŠ” ê²ƒì´ì£ . 

![image-20220209174758005](https://user-images.githubusercontent.com/70505378/153200664-43fa5d21-3825-4d82-91d6-91b0d8004964.png)



#### Label Smoothing

ì´ì œ Transformerì˜ ë™ì‘ ë°©ì‹ì— ëŒ€í•´ ëª¨ë‘ ì‚´í´ë´¤ìŠµë‹ˆë‹¤. ì—¬ê¸°ê¹Œì§€ ì˜¤ì‹  ë¶„ë“¤ ì¶•í•˜ë“œë¦½ë‹ˆë‹¤ ğŸ‘ğŸ‘

ê·¼ë° ìš°ë¦¬ í•˜ë‚˜ë§Œ ë” ë³´ê³  ê°‘ì‹œë‹¤. ë°”ë¡œ **Label Smoothing**ì´ë¼ëŠ” ê¸°ìˆ ì¸ë°ìš”, TransformerëŠ” ìµœì¢… ë‹¨ê³„ì— label smoothingì´ë¼ëŠ” ê²ƒì„ ì‚¬ìš©í•´ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ í•œì¸µ ë” ì¦ê°€ì‹œí‚µë‹ˆë‹¤. 

ì´ Label smoothingì—ëŠ” ì—¬ëŸ¬ ê¸°ë²•ë“¤ì´ ìˆëŠ”ë°ìš”, Transformerì—ì„œëŠ” Softmax layerì˜ ì¶œë ¥ìœ¼ë¡œ ë‚˜ì˜¨ probability vectorë¥¼ ê°€ì¥ ë†’ì€ í™•ë¥ ì€ ê°€ì§„ ì¸ë±ìŠ¤ì˜ ê°’ë§Œ 1ë¡œ ë§Œë“œëŠ” ì›-í•« ë°©ì‹ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, **ê°ê°ì˜ í™•ë¥  ê°’ì„ ì§ì ‘ ì´ìš©**í•˜ëŠ” ì‹ìœ¼ë¡œ ì´ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. 

ì´ê²Œ ë¬´ìŠ¨ ë§ì´ëƒ í•˜ë©´, ì˜ˆë¥¼ ë“¤ì–´ 'Thank you'ë¼ëŠ” ë‹¨ì–´ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­í•œë‹¤ê³  í•´ë´…ì‹œë‹¤. ì´ ë‹¨ì–´ëŠ” í•œêµ­ì–´ë¡œ 'ê³ ë§™ìŠµë‹ˆë‹¤' ë˜ëŠ” 'ê°ì‚¬í•©ë‹ˆë‹¤' ëª¨ë‘ë¡œ ë²ˆì—­ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ°ë° **ì •ë‹µì— í•´ë‹¹í•˜ëŠ” 'ê³ ë§™ìŠµë‹ˆë‹¤'ì— í•´ë‹¹í•˜ëŠ” ê°’ë§Œ 1ì´ë¼ë©´, ëª¨ë¸ì´ ê·¸ ê°’ì„ 'ê°ì‚¬í•©ë‹ˆë‹¤'ë¡œ ì˜ˆì¸¡í•˜ë“  'ì§œì¦ë‚©ë‹ˆë‹¤'ë¡œ ì˜ˆì¸¡í•˜ë“  ëª¨ë‘ ê·¸ëƒ¥ í‹€ë¦° ê²ƒì´ ë˜ë²„ë¦°ë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.**

ì´ ë•Œë¬¸ì— ì›-í•« ë°©ì‹ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  ëŒ€ì‹ ì— label smoothingì„ ì ìš©í•˜ì—¬ ê°ê°ì˜ í™•ë¥ ê°’ì— ìœ ì‚¬í•˜ê²Œ ì˜ˆì¸¡ì„ í•˜ë„ë¡ ìœ ë„í•¨ìœ¼ë¡œì¨ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ í•œì¸µ ë” ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ label smoothing ê¸°ë²•ì€ ë°ì´í„°ê°€ noisyí•œ ê²½ìš°, ì¦‰ ê°™ì€ ì…ë ¥ ê°’ì— ë‹¤ë¥¸ ì¶œë ¥ ê°’ì´ ë‚˜ì˜¤ëŠ” ë°ì´í„°ë“¤ì´ ë§ì„ìˆ˜ë¡ í¬ê²Œ ë„ì›€ì´ ëœë‹¤ê³  í•©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ ë” ì°¾ì•„ë³´ì‹œë©´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤. 

<br>

<br>

ì´ì œ ì •ë§ ëì…ë‹ˆë‹¤! Transformerë¥¼ ì´í•´í•˜ëŠ” ê²ƒì€ ì‰½ì§€ ì•Šì€ ê³¼ì •ì´ì§€ë§Œ, ì›Œë‚™ ë§ì´ ì‚¬ìš©ë˜ê³  ë– ì˜¤ë¥´ê³  ìˆëŠ” ê¸°ìˆ ì´ê¸° ë•Œë¬¸ì— ì´í•´í•´ë‘ë©´ ì•„ì£¼ ë„ì›€ì´ ë  ê²ƒì´ë¼ê³  ìƒê°í•©ë‹ˆë‹¤ ğŸ¤—ğŸ¤—

Transformer í™œìš©ì˜ ì˜ˆë¡œëŠ” Encoder ë¶€ë¶„ë§Œ ì‚¬ìš©í•´ì„œ ì´ë¯¸ì§€ë¥¼ ë¶„ë¥˜í•´ë‚´ëŠ” **Visual Transformer(ViT)**, í…ìŠ¤íŠ¸ì˜ ë‚´ìš©ì— í•´ë‹¹í•˜ëŠ” ì´ë¯¸ì§€ë¥¼ ìƒì„±í•´ë‚´ëŠ” **DALL-E** ë“±ì´ ìˆìŠµë‹ˆë‹¤. 

ì•„ë˜ëŠ” Attention ì—°ì‚°(Scaled Dot Product Attention, SDPA)ê³¼ MHA(Multi-Head Attention)ë¥¼ êµ¬í˜„í•œ ì½”ë“œì´ë‹ˆ, ì²œì²œíˆ ì½ì–´ë³´ì‹œë©´ì„œ Transformerì˜ ê³¼ì •ê³¼ ê·¸ ê³¼ì •ì´ ì½”ë“œë¡œëŠ” ì‹¤ì œë¡œ ì–´ë–»ê²Œ êµ¬í˜„ë˜ëŠ”ì§€, ê·¸ ê³¼ì •ì—ì„œ tensorì˜ sizeì— ëŒ€í•´ ìŒë¯¸í•´ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤. 

### MHA ì‹¤ìŠµ

* SDPA

  ```python
  class ScaledDotProductAttention(nn.Module):
      def forward(self, Q, K, V, mask=None):
          d_K = K.size()[-1] # key dimension
          scores = Q.matmul(K.transpose(-2,-1)) / np.sqrt(d_K)
          if mask is not None:
              scores = scores.masked_fill(mask==0, -1e9) # í˜„ì¬ step í›„ì˜ ê°’ masking
          attention = F.softmax(scores, dim=-1) # Softmax
          out = attention.matmul(V) # weighted sum
          return out, attention
  
  # ==============================================================================
  # Demo run of scaled dot product attention 
  SPDA = ScaledDotProductAttention()
  n_batch,d_K,d_V = 3,128,256 # d_K(=d_Q) does not necessarily be equal to d_V
  '''
  n_Q,n_K,n_V = 30,50,50
  - Q vectorì˜ ê°œìˆ˜ì™€ K, V vectorì˜ ê°œìˆ˜ëŠ” ë‹¬ë¼ë„ ë¨
  - K vectorì˜ ê°œìˆ˜ì™€ V vectorì˜ ê°œìˆ˜ëŠ” ê°™ì•„ì•¼ í•¨
      - Q * K.T: [n_Q, d_K]x[d_K, n_K] = [n_Q, n_K]
      - Softmax(Q*K.T/root(d_K)) * V: [n_Q, n_K]x[n_V, d_V](n_K==n_V) = [n_Q, d_V]
  '''
  n_Q,n_K,n_V = 30,50,50
  Q = torch.rand(n_batch,n_Q,d_K)
  K = torch.rand(n_batch,n_K,d_K)
  V = torch.rand(n_batch,n_V,d_V)
  out,attention = SPDA.forward(Q,K,V,mask=None)
  def sh(x): 
    return str(x.shape)[11:-1] 
  print ("SDPA: Q%s K%s V%s => out%s attention%s"%
         (sh(Q),sh(K),sh(V),sh(out),sh(attention)))
  # SDPA: Q[3, 30, 128] K[3, 50, 128] V[3, 50, 256] => out[3, 30, 256] attention[3, 30, 50]
  # ==============================================================================
  # It supports 'multi-head' attention
  n_batch,n_head,d_K,d_V = 3,5,128,256
  n_Q,n_K,n_V = 30,50,50 # n_K and n_V should be the same
  Q = torch.rand(n_batch,n_head,n_Q,d_K)
  K = torch.rand(n_batch,n_head,n_K,d_K)
  V = torch.rand(n_batch,n_head,n_V,d_V)
  out,attention = SPDA.forward(Q,K,V,mask=None)
  # out: [n_batch x n_head x n_Q x d_V]
  # attention: [n_batch x n_head x n_Q x n_K] 
  def sh(x): 
    return str(x.shape)[11:-1] 
  print ("(Multi-Head) SDPA: Q%s K%s V%s => out%s attention%s"%
         (sh(Q),sh(K),sh(V),sh(out),sh(attention)))
  # (Multi-Head) SDPA: Q[3, 5, 30, 128] K[3, 5, 50, 128] V[3, 5, 50, 256] => out[3, 5, 30, 256] attention[3, 5, 30, 50]
  ```

* MHA

  * Transformer ë…¼ë¬¸ì—ì„œëŠ” Attentionì—ì„œ Dropoutê³¼ ê´€ë ¨ëœ ì´ì•¼ê¸°ê°€ ì—†ìŠµë‹ˆë‹¤ë§Œ, ì‹¤ì œë¡œ êµ¬í˜„ ì‹œì—ëŠ” Dropoutì„ ì‚¬ìš©í•©ë‹ˆë‹¤. 
  * ë‹¨ì–´ì˜ featureì˜ ì°¨ì›ìˆ˜ `d_feat`ëŠ” `n_head`ê°œì˜ Headì—ê²Œ `d_head` ê°œì”© ë‚˜ëˆ ì ¸ì„œ ë³‘ë ¬ì ìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤. (d_head * n_head == d_feat)
  * Input ì˜ í˜•ìƒê³¼ outputì˜ í˜•ìƒì€ ì¼ì¹˜í•©ë‹ˆë‹¤. 

  ```python
  class MultiHeadAttention(nn.Module):
      def __init__(self, d_feat=128, n_head=5, actv=F.relu, USE_BIAS=True, dropout_p=0.1, device=None):
          """
          : param d_feat: feature dimension(ë‹¨ì–´ì˜ íŠ¹ì§• ì°¨ì›ìˆ˜)
          : param n_head: number of heads(Attention ê°œìˆ˜)
          : param actv: activation after each linear layer
          : param USE_BIAS: whether to use bias(linear layerì—ì„œ í¸í–¥ ì‚¬ìš© ì—¬ë¶€)
          : param dropout_p: dropout rate(ë…¼ë¬¸ì—ì„œëŠ” ë“œë¡­ì•„ì›ƒê³¼ ê´€ë ¨í•œ ì„¤ëª…ì´ ì—†ëŠ”ë° êµ¬í˜„ì—ì„œëŠ” ì‚¬ìš©)
          : device: which device to use (e.g. cuda:0)
          """
          super(MultiHeadAttention, self).__init__()
          # ë‹¨ì–´ì˜ íŠ¹ì§• ì°¨ì›ìˆ˜ëŠ” attention headì˜ ê°œìˆ˜ì˜ ë°°ìˆ˜ì—¬ì•¼ í•œë‹¤. ë§Œì•½ ë‹¨ì–´ì˜ íŠ¹ì§• ì°¨ì›ìˆ˜ê°€ 100ì´ë©´ ì´ê²ƒì„
          # í•˜ë‚˜ì˜ attentionì— í•œë²ˆì— ë„£ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, n_head ê°œì˜ attentionì— ë³‘ë ¬ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê¸° ë•Œë¬¸ì´ë‹¤.
          if (d_feat%n_head) != 0:
              raise ValueError("d_feat(%d) should be divisible by n_head(%d)"%(d_feat, n_head))
          self.d_feat = d_feat
          self.n_head = n_head
          self.d_head = self.d_feat // self.n_head
          self.actv = actv
          self.USE_BIAS = USE_BIAS
          self.dropout_p = dropout_p
  
          self.lin_Q = nn.Linear(self.d_feat,self.d_feat,self.USE_BIAS)
          self.lin_K = nn.Linear(self.d_feat,self.d_feat,self.USE_BIAS)
          self.lin_V = nn.Linear(self.d_feat,self.d_feat,self.USE_BIAS)
          self.lin_O = nn.Linear(self.d_feat,self.d_feat,self.USE_BIAS)
  
          self.dropout = nn.Dropout(p=self.dropout_p)
  
      def forward(self, Q, K, V, mask=None):
        """
        : param Q: [n_batch, n_Q, d_feat]
        : param K: [n_batch, n_K, d_feat]
        : param V: [n_batch, n_V, d_feat]
        : param mask
        """
        ### í•„ìš”í•œ feature ê°œìˆ˜ ê³„ì‚°
        n_batch = Q.shape[0]
        Q_feat = self.lin_Q(Q) # [n_batch, n_Q, d_feat]
        K_feat = self.lin_K(K) # [n_batch, n_K, d_feat]
        V_feat = self.lin_V(V) # [n_batch, n_V, d_feat]
  
        ### Multi-head split of Q, K, and V (d_feat = n_head*d_head)
        # [n_Q, d_head] í¬ê¸°ì˜ ê°€ì¤‘ì¹˜ í–‰ë ¬ì„ n_head ê°œë§Œí¼ n_batch ë°°ì¹˜ìˆ˜ë§Œí¼ ë§Œë“ ë‹¤. 
        Q_split = Q_feat.view(n_batch, -1, self.n_head, self.d_head).permute(0, 2, 1, 3) 
        K_split = K_feat.view(n_batch, -1, self.n_head, self.d_head).permute(0, 2, 1, 3)
        V_split = V_feat.view(n_batch, -1, self.n_head, self.d_head).permute(0, 2, 1, 3)
        # Q_split: [n_batch, n_head, n_Q, d_head]
        # K_split: [n_batch, n_head, n_K, d_head]
        # V_split: [n_batch, n_head, n_V, d_head]
  
        ### Multi-Headed Attention
        d_K = K.size()[-1] # key dimension
        scores = torch.matmul(Q_split, K_split.permute(0,1,3,2)) / np.sqrt(d_K) # [n_batch, n_head, n_Q, n_K]
        if mask is not None:
            scores = scores.masked_fill(mask==0, -1e9)
        attention = torch.softmax(scores, dim=-1)
        # dropout is NOT mentioned in the paper!
        x_raw = torch.matmul(self.dropout(attention), V_split) # [n_batch, n_head, n_Q, d_head] (n_K==n_V)
  
        ### Reshape x
        x_rsh1 = x_raw.permute(0,2,1,3).contiguous()   # [n_batch, n_Q, n_head, d_head]
        x_rsh2 = x_rsh1.view(n_batch, -1, self.d_feat) # [n_batch, n_Q, d_feat]
  
        ### Linear
        x = self.lin_O(x_rsh2) # [n_batch, n_Q, d_feat]
        out = {'Q_feat':Q_feat,'K_feat':K_feat,'V_feat':V_feat,
               'Q_split':Q_split,'K_split':K_split,'V_split':V_split,
               'scores':scores,'attention':attention,
               'x_raw':x_raw,'x_rsh1':x_rsh1,'x_rsh2':x_rsh2,'x':x}
        return out
  
  
  # ==============================================================================
  # Self-Attention Layer
  n_batch = 128
  n_src   = 32
  d_feat  = 200
  n_head  = 5
  src = torch.rand(n_batch,n_src,d_feat)
  self_attention = MultiHeadAttention(
      d_feat=d_feat,n_head=n_head,actv=F.relu,USE_BIAS=True,dropout_p=0.1,device=device)
  out = self_attention.forward(src,src,src,mask=None)
  
  Q_feat,K_feat,V_feat = out['Q_feat'],out['K_feat'],out['V_feat']
  Q_split,K_split,V_split = out['Q_split'],out['K_split'],out['V_split']
  scores,attention = out['scores'],out['attention']
  x_raw,x_rsh1,x_rsh2,x = out['x_raw'],out['x_rsh1'],out['x_rsh2'],out['x']
  
  # Print out shapes
  def sh(_x): 
    return str(_x.shape)[11:-1] 
  print ("Input src:\t%s  \t= [n_batch, n_src, d_feat]"%(sh(src)))
  print ()
  print ("Q_feat:   \t%s  \t= [n_batch, n_src, d_feat]"%(sh(Q_feat)))
  print ("K_feat:   \t%s  \t= [n_batch, n_src, d_feat]"%(sh(K_feat)))
  print ("V_feat:   \t%s  \t= [n_batch, n_src, d_feat]"%(sh(V_feat)))
  print ()
  print ("Q_split:  \t%s  \t= [n_batch, n_head, n_src, d_head](d_head * n_head == d_feat)"%(sh(Q_split)))
  print ("K_split:  \t%s  \t= [n_batch, n_head, n_src, d_head](d_head * n_head == d_feat)"%(sh(K_split)))
  print ("V_split:  \t%s  \t= [n_batch, n_head, n_src, d_head](d_head * n_head == d_feat)"%(sh(V_split)))
  print ()
  print ("scores:   \t%s  \t= [n_batch, n_head, n_src, n_src](Q_split * K_split)"%(sh(scores)))
  print ("attention:\t%s  \t= [n_batch, n_head, n_src, n_src]"%(sh(attention)))
  print ()
  print ("x_raw:    \t%s  \t= [n_batch, n_head, n_src, d_head](x_raw=Attention(src,Q,K,V))"%(sh(x_raw)))
  print ("x_rsh1:   \t%s  \t= [n_batch, n_src, n_head, d_head]"%(sh(x_rsh1)))
  print ("x_rsh2:   \t%s  \t= [n_batch, n_src, d_feat]"%(sh(x_rsh2)))
  print ()
  print ("Output x: \t%s  \t= [n_batch, n_src, d_feat](output shape == input shape)"%(sh(x)))
  
  '''
  Input src:	[128, 32, 200]  	= [n_batch, n_src, d_feat]
  
  Q_feat:   	[128, 32, 200]  	= [n_batch, n_src, d_feat]
  K_feat:   	[128, 32, 200]  	= [n_batch, n_src, d_feat]
  V_feat:   	[128, 32, 200]  	= [n_batch, n_src, d_feat]
  
  Q_split:  	[128, 5, 32, 40]  	= [n_batch, n_head, n_src, d_head](d_head * n_head == d_feat)
  K_split:  	[128, 5, 32, 40]  	= [n_batch, n_head, n_src, d_head](d_head * n_head == d_feat)
  V_split:  	[128, 5, 32, 40]  	= [n_batch, n_head, n_src, d_head](d_head * n_head == d_feat)
  
  scores:   	[128, 5, 32, 32]  	= [n_batch, n_head, n_src, n_src](Q_split * K_split)
  attention:	[128, 5, 32, 32]  	= [n_batch, n_head, n_src, n_src]
  
  x_raw:    	[128, 5, 32, 40]  	= [n_batch, n_head, n_src, d_head](x_raw=Attention(src,Q,K,V))
  x_rsh1:   	[128, 32, 5, 40]  	= [n_batch, n_src, n_head, d_head]
  x_rsh2:   	[128, 32, 200]  	= [n_batch, n_src, d_feat]
  
  Output x: 	[128, 32, 200]  	= [n_batch, n_src, d_feat](output shape == input shape)
  '''
  ```

  









<br>

<br>

## ì°¸ê³  ìë£Œ

* [Attention Is All You Need ë…¼ë¬¸](https://arxiv.org/pdf/1706.03762.pdf)
* [Transformer ì„¤ëª… ë¸”ë¡œê·¸(ì˜ì–´)](http://jalammar.github.io/illustrated-transformer)
* [Transformer ë…¼ë¬¸ ë¦¬ë·° ìœ íŠœë¸Œ ì˜ìƒ(í•œê¸€)](https://www.youtube.com/watch?v=mxGCEWOxfe8)

















<br>
