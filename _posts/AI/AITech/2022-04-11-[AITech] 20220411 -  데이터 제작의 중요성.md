---
layout: single
title: "[AITech][Data Annotation] 20220411 - 데이터 제작의 중요성"
categories: ['AI', 'AITech']
toc: true
toc_sticky: true
tag: []
---



<br>

_**본 포스팅은 Upstage의 '이활석' 마스터 님의 강의를 바탕으로 작성되었습니다.**_

# 데이터 제작의 중요성

이번 포스팅은 `데이터 제작의 중요성`에 대한 내용입니다. 

## Software 1.0 vs Software 2.0

`Software 1.0`의 개발 방법론은 다음의 개발 과정을 따릅니다. 

1. 문제 정의
2. 큰 문제를 작은 문제들의 집합으로 분해
3. 개별 문제 별로 알고리즘 설계
4. 솔루션들을 합쳐 하나의 시스템으로 구성

실제로 Software 1.0의 개발 철학이 담긴 시스템은 많은 곳에서 볼 수 있습니다. 

![image-20220411110957701](https://user-images.githubusercontent.com/70505378/162671505-f6a43a6a-1fa5-47b3-84c5-523fedb49822.png)

이미지 인식 기술(Classification, Detection 등)도 처음에는 Software 1.0 방식으로 개발되었습니다. 

하지만 Software 1.0 방식은 모든 부분에서 **사람의 개입**이 들어가기 때문에 그 발전 속도가 매우 느렸습니다. 실제로 2008년 Software 1.0 방식으로 개발된 object detection 모델이 정확도 21.0%를 기록한 뒤, 2014년 Software 1.0 방식으로 개발한 object detection SOTA 모델은 약 33.7%의 정확도를 보였습니다. 6년 동안 12.7%의 성능 향상 만이 이루어졌습니다. 

그런데 불과 몇 달 뒤 2014년에 나온 Software 2.0 방식의 RCNN 모델은 58.5%의 정확도를 보였고, 2020년에는 89.3%의 정확도를 기록했습니다. 

![image-20220411111746028](https://user-images.githubusercontent.com/70505378/162671508-2f36830b-9d1a-4f80-9bad-eb98f403bbab.png)

<br>

이러한 발전 속도의 차이가 나는 가장 큰 이유는 사람의 개입 정도의 차이입니다. Software 1.0 방식에서는 알고리즘 개발, 시스템 설계 등의 과정이 모두 휴리스틱하게 이루어졌기 때문에 시간이 오래 걸리고 그 정확도도 높지 않았습니다. 또한 사람들이 다른 부분에 신경 써야 할 것들이 많기 때문에, **데이터는 적은 양의 데이터를 활용할 뿐**이었습니다. 

![image-20220411114845010](https://user-images.githubusercontent.com/70505378/162671512-566c8a9b-eaed-4197-befd-8812607a5197.png)

`Software 2.0`에서는 이 패러다임이 완전히 변화한 것입니다.  Software 2.0 개발 방법론은 아래 과정을 따릅니다. 

1. 뉴럴넷의 구조에 의해 검색 영역 결정
2. 최적화를 통해 사람이 정한 목적에 가장 부합하는 연산의 집합을 탐색
3. 경로와 목적지는 데이터와 최적화 방법에 의해 결정

물론 모델의 구조를 설계하는 데도 사람의 개입이 들어가지만, 필요한 시간이 예전보다 훨씬 감소한 것입니다. 이로 인해 Software 2.0에서는 **최적화 방법과 데이터 수집에 더 많은 노력**이 들어갈 수 있게 되었습니다. 

실제 Software 2.0 방식의 개발이 도입된 후 object detection 영역의 발전 속도는 훨씬 빨라졌습니다. 

![image-20220411112255893](https://user-images.githubusercontent.com/70505378/162671510-730eb64e-be56-4563-812a-7b9bdac19c7b.png)

<br>

이를 요약하면 아래와 같습니다. 

* `Software 1.0`
  * 사람이 고민하여 프로그램을 설계
* `Software 2.0`
  * AI 모델의 구조로 프로그램의 검색 범위를 한정하고, 
  * **데이터**와 최적화 방법을 통해 최적의 프로그램을 탐색 및 설계
  * AI 모델의 성능 = (모델 구조 + 최적화 방법) + **데이터** = 코드 + **데이터**

이와 같이 Software 2.0 개발 방법론에 있어서 모델의 성능을 높이는 가장 기본적이자 필수적인 방법이 질 좋은 데이터를 제작하는 것입니다. 

<br>

<br>

## Lifecycle of an AI Project

다음으로 현업에서 AI Product Serving이 어떻게 이루어지는지 보면서 데이터가 중요한 이유에 대해 다시 한 번 이야기 해보겠습니다. 

서비스향 AI 모델 개발 과정은 크게 아래의 4가지 단계로 구성됩니다. 

![image-20220411133123902](https://user-images.githubusercontent.com/70505378/162671516-55471a93-fc84-4d1b-b235-4c8b540f22f8.png)

이 과정의 목표는 요구 사항을 충족시키는 모델을 지속적으로 확보하는 것이고, 여기에는 2가지 방법이 있습니다. 

* **Data-Centric**: 데이터만 수정하여 모델 성능 끌어올리기
* **Model-Centric**: 모델 구조만 수정하여 모델 성능 끌어올리기

Serving 단계에 따라 두 가지 접근 방식이 차지하는 비중이 달라집니다. 

첫번째로 현재 서비스를 개발하는 중이고 아직 첫 release를 하지 않은 상태에서는 데이터-중심 접근법과 모델-중심 접근법이 비슷한 비중을 이룹니다. 

![image-20220411133522077](https://user-images.githubusercontent.com/70505378/162671518-7952c775-be71-472b-87cf-23263208bf7f.png)

그런데 배포한 모델의 성능을 개선해야 할 때는 데이터-중심 접근법의 비중이 훨씬 높아지게 됩니다. 

![image-20220411133628931](https://user-images.githubusercontent.com/70505378/162671519-9340d76a-2f4a-4c87-8f91-fe832cf6ebbe.png)

서비스 출시 이후에는 모델의 정확도에 대한 성능 개선 요구가 대부분입니다. 이때 정확도 개선을 위해 모델 구조를 변경하는 것은 처리 속도, qps, 메모리 크기 등의 사양이 변화하여 이에 대한 요구 사항에 대한 검증을 다시 해야 하므로 그 비용이 큽니다. 

반면 더 많은, 더 좋은 데이터를 수집하여 모델을 학습시키는 것은 다른 부분의 변화없이 모델의 성능만 개선할 수 있습니다. 따라서 AI 모델 서비스를 제공할 때에도 데이터는 매우 중요한 비중을 차지하고 있음을 알 수 있습니다. 











<br>

<br>

## Data related tasks

데이터의 중요성은 익히 알려진 사실이지만, 아직 좋은 데이터를 충분히 모으기 위한 인프라가 구축되지 못 했습니다. 그렇기 때문에 이 데이터와 관련된 업무들에 투자하는 시간이 더욱 많아질 수 밖에 없는 것입니다. 

그렇다면 좋은 데이터를 많이 모으는 일이 어려운 이유는 무엇일까요?

**1. 어떻게 하면 좋을지에 대해서 알려져 있지 않다.**

첫번째 이유는 '무엇이 좋은 데이터인가'에 대한 명확한 정의가 없기 때문입니다. Task마다, dataset마다 '좋은 데이터'의 의미는 변할 수 있습니다. 

실제로 출간되는 논문의 비율을 보면 데이터와 관련된 논문은 모델과 관련된 논문보다 그 수가 훨씬 적습니다. 

![image-20220411141222840](https://user-images.githubusercontent.com/70505378/162671520-93c5b040-f5b6-4541-90c6-b2cd971c5b41.png)

이는 학계에서는 시간적 제약, 자원적 제약 등의 이유로 '데이터 제작'에 많은 투자를 할 수 없기 때문이죠. 따라서 좋은 데이터에 대한 정의는 아직 미지의 영역이라고 할 수 있습니다. 

**2. 일관된 라벨링 작업은 매우 어렵다.**

무작정 데이터를 많이 모은다고 해서 모델의 성능이 올라갈까요? 그건 아닐겁니다. 

'좋은 데이터'를 '골고루' 모아야 합니다. 여기서 **좋은 데이터**란 노이즈가 적은 데이터를 말하고, **골고루** 모은다는 것은 각 클래스에 해당하는 데이터 수가 비슷해야 한다는 것을 말합니다. 

실제로 노이즈 데이터는 모델의 학습에 악영향을 주게 되고, 모델이 노이즈 데이터를 무시하게 하려면 최소 2배 이상의 좋은 데이터(깨끗이 라벨링된 데이터)가 필요합니다. 

또한 데이터가 소수의 클래스에 편중되면 모델은 데이터 수가 적은 클래스에 대해 제대로 예측하지 못 할 것입니다. 

<br>

아래 그래프는 데이터 제작 시 샘플 수와 라벨링 노이즈 세기 사이의 상관관계를 나타낸 그래프입니다. 

![image-20220411141839216](https://user-images.githubusercontent.com/70505378/162671522-36a70b15-5119-4034-b627-c2c1a1e1bf9f.png)

데이터 제작(라벨링) 또한 결국 사람이 하는 일입니다. 양이 많고 직관적인 데이터는 라벨링하기도 쉽지만, 양이 적고 헷갈리는 데이터는 라벨링을 어떻게 해야 할 지 사람마다 생각이 다르기 때문에 노이즈가 많아지게 됩니다. 

**3. 데이터 불균형을 해소하기가 어렵다.**

따라서 **좋은 데이터를 위해서는 골고루 데이터를 모아야 하고, 일관된 라벨링 작업이 필요**합니다. 

![image-20220411142346503](https://user-images.githubusercontent.com/70505378/162671524-0724dd2e-2f36-4466-9898-9e67e4bc8688.png)

이는 어려운 일이 아닐 수 없습니다. 본 강의에서는 이를 위한 해법에는 두 가지가 있다고 말합니다. 

1\) 해당 task에 대한 지식이 풍부해야 한다. 

결국 도메인 지식이 많을수록 예외 경우에 대해서 미리 인지를 하고, 가이드라인을 세울 수 있습니다. 

테슬라의 경우 자율 주행에서의 예외 경우를 trigger라 지칭하고 221가지를 정의하여 세심히 관리한다고 합니다. 

![image-20220411142650999](https://user-images.githubusercontent.com/70505378/162671525-84a3c362-78c7-427b-bddd-bacf554ddba8.png)

2\) 반복적인 데이터 제작

하지만 완벽하게 모든 경우를 안 채로 데이터를 모으고 라벨링 가이드를 만드는 것은 불가능합니다. 이것이 불가능하다는 것을 인지하고, 데이터 제작을 반복적으로 수행하는 것이 필요합니다. 

여기서 반복적인 데이터 제작이란 가이드 라인을 계속해서 수정하고, 수정된 가이드 라인에 따라 데이터를 라벨링하고, 라벨링된 데이터를 확인하고, 모델의 성능을 개선하는 일련의 작업들을 반복하여 좋은 데이터를 만드는 것을 말합니다. 

![image-20220411142918001](https://user-images.githubusercontent.com/70505378/162671527-2a2c0955-e025-401b-9a6e-d3e724a7c4ab.png)

<br>

현재 VSC, PyCharm과 같은 Software 1.0 IDE 들은 많이 개발이 되어 상용화되었지만, Software 2.0 IDE 들은 자리잡지 못 했습니다. 

본 강의에서는 마지막으로 Software 2.0 시대에 필요한 IDE에게 요구되는 기능들에는 무엇이 있는지 이야기하며 강의를 마칩니다. 

* 데이터셋 시각화
  * 데이터/레이블 분포 시각화
  * 레이블 시각화
  * 데이터 별 예측값 시각화

![image-20220411143254678](https://user-images.githubusercontent.com/70505378/162671529-76fa6b3f-3787-4821-bccf-f3667b6f5e2e.png)

* 데이터 라벨링
  * 라벨링 UI
  * 테스트 특화 기능
  * 라벨링 일관성 확인
  * 라벨링 작업 효율 확인
  * 자동 라벨링

![image-20220411143325237](https://user-images.githubusercontent.com/70505378/162671531-1135dc09-74e7-48b4-b176-c72cc55a6101.png)

* 데이터셋 정제
  * 반복 데이터 제거
  * 라벨링 오류 수정

![image-20220411143346513](https://user-images.githubusercontent.com/70505378/162671534-1b151b57-cc57-4df5-b8f8-7dc866542e4d.png)

* 데이터셋 선별
  * 어떤 데이터를 사용해야 하나?

![image-20220411143419810](https://user-images.githubusercontent.com/70505378/162671535-13b04056-c79a-42db-8719-97f7eefc2f12.png)



















<br>

<br>

# 참고 자료

* 
