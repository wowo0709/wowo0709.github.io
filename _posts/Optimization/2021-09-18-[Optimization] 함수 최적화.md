---
layout: single
title: "[Search] 함수 최적화"
categories: ['AI', 'Optimization']
toc: true
toc_sticky: true
---

<br>

## 개요

지난 포스팅에서는 조합 최적화, 그 중 유전 알고리즘에 대해 자세히 알아보았습니다. 

이번 포스팅에서는 `함수 최적화`에 대해 다뤄보려 합니다. 

<br>

### 함수 최적화

`함수 최적화`는 어떤 **목적 함수**가 있을 때, 이 함수를 **최대**로 하거나 **최소**로 하는 **변수 값**을 찾는 최적화 문제입니다. 

<img src="https://user-images.githubusercontent.com/70505378/133869984-c7bb343b-06c9-4b81-83b7-ca1e58c54693.png" alt="image-20210918111639830" style="zoom:67%;" />

이번 포스팅에서는 수많은 함수 최적화 기법들 중 **제약조건 최적화**와 **회귀 문제에서의 최적화** 기법에 대해 알아보겠습니다. 

<br>

## 제약조건 최적화

`제약 조건 최적화`란 **제약 조건**을 만족시키면서 **목적 함수**를 **최적화**시키는 변수 값들을 찾는 문제입니다. 

<img src="https://user-images.githubusercontent.com/70505378/133869987-0f7de84b-7028-4587-8e9c-467ea0f530df.png" alt="image-20210918112026982" style="zoom:67%;" />

대표적으로 머신러닝 방법 중 SVM에서 이 제약 조건 최적화 기법을 사용합니다. 

<br>

### 라그랑주 함수

**제약조건 최적화** 문제에서는 `라그랑주 함수`라는 것을 사용합니다. 

**라그랑주 함수**란 제약 조건들과 목적함수를 결합한 함수입니다. 

위에서 본 문제의 **라그랑주 함수**는 다음과 같이 표현할 수 있습니다. 

![image-20210918112320247](https://user-images.githubusercontent.com/70505378/133870008-ee8e6172-560f-4aa5-8b55-07c0e6bd16e8.png)

<br>

여기서는 라그랑주 함수를 이용한 함수 최적화 방법에 대해서는 다루지 않으니, 이에 관심이 있으신 분들은 다른 자료들은 더 참고해보시기 바랍니다. 

<br>

<br>

## 회귀 문제에서의 최적화

`회귀 문제`란 주어진 데이터를 **가장 잘 근사**하는 함수를 찾는 문제입니다. 

회귀 문제에서는 이 **최적 함수**를 찾기 위해 여러가지 최적화 기법을 사용하는데, 그 중 몇가지에 대해 소개하겠습니다. 

<br>

### 최소 평균제곱법

`최소 평균 제곱법`이란 예측한 함수의 함수값과 실제 데이터의 차이를 제곱하여  평균한 함수(MSE, Mean Square Error)를 목적 함수로 사용하는 것입니다. 

이 목적함수를 인공지능에서는 **오차 함수** 또는 **에너지 함수**라고 부르며, 이 함수를 최소로 하는 함수를 찾는 것입니다. 

![image-20210918112921661](https://user-images.githubusercontent.com/70505378/133870011-c33f6446-f2f5-463f-9bab-db74a37b175e.png)

<br>

### 경사 하강법

인공지능에서 가장 기본적으로 사용되는 `경사 하강법`입니다. 

**경사 하강법**이란 함수의 최소값 위치를 찾는 문제에서 **오차 함수의 그레디언트(gradient, 각 파라미터에 대해 편미분한 벡터)의 반대 방향**으로 조금씩 움직여 최적의 파라미터를 찾으려는 방법입니다. 

여기서 **반대 방향**으로 움직이는 이유는 **오차 함수의 최소값**을 찾기 위함이죠. 이처럼 회귀 문제에서는 오차 함수라는 개념을 계속해서 사용합니다. 

<br>

함수의 그레디언트는 그 위치에서의 함수의 경사를 나타냅니다. 

아래로 볼록한 이차 함수의 경우를 생각해보죠. 이차 함수 위에서의 위치에 따라 경사(접선의 기울기)는 (+), (-), 0 중 하나로 나타납니다. 이 중 경사를 0으로 만드는 지점에 최대한 가깝게 가기 위해 현재 경사를 이용하여 이동합니다. 

* **`현재 경사가 (+)`**: 왼쪽, 즉 **(-)** 방향으로 이동해야 합니다. 
* **`현재 경사가 (-)`**: 오른쪽, 즉 (+) 방향으로 이동해야 합니다. 

따라서 경사 하강법에서는 반복적으로 **그레디언트 반대 방향**으로 이동하며 최적화를 수행하고, 그 식은 다음과 같이 나타낼 수 있습니다. 

![image-20210918113708964](https://user-images.githubusercontent.com/70505378/133870013-51071c03-aa10-45f1-8289-5cbc2e7bf11d.png)

**경사 하강법**은 회귀 모델, 신경망 등의 기본 학습 방법입니다. 

다만 이 최적화 기법은 국소해에 빠질 위험이 있기 때문에 이를 개선한 **모멘텀, adagrad, adadelta** 등의 여러 최적화 기법들이 파생되었습니다. 

<br>

<br>

## 정리

이번 포스팅에서는 **최적화** 기법 중 **함수 최적화** 기법에 대해 알아보았습니다. 

**제약 조건 최적화**에서는 목적 함수로 **라그랑주 함수**라는 것을 사용했습니다. 라그랑주 함수는 목적 함수와 제약 조건을 결합한 형태의 함수입니다. 

**회귀 문제 최적화**에서는 목적 함수(오차 함수)로 **MSE**를 포함해 여러 함수를 사용할 수 있으며, 이에 대한 최적화 기법으로 대표적인 **경사하강법**에 대해 살펴보았습니다. 











