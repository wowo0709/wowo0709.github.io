---
layout: single
title: "[TFLite] 4(2). 사전 학습 모델 이용"
categories: ['TFLite']
---



<br>

# 사전 학습 모델 이용

<br>

### 모델 선택

사전 학습 모델은 **이미 훈련이 완료된** 모델로, 복잡하고 오래 걸리는 학습 절차 없이 바로 이 모델을 이용하여 추론할 수 있습니다. 그러나 아직은 널리 사용되는 몇 가지 모델만이 존재합니다. 

사전 학습 모델은 텐서플로 라이트 모델로 제공되기도 하고 텐서플로 모델로 제공되기도 합니다. 텐서플로 라이트 모델은 설계, 학습, 변환, 최적화가 모두 완료되어 tflite 파일로 제공됩니다. 텐서플로 모델의 경우 TFLite 모델로 변환해야 안드로이드에서 사용할 수 있습니다. 

텐서플로 모델은 케라스 애플리케이션 모듈에서 제공하는 모델을 이용하거나 텐서플로 허브에서 제공하는 모델을 이용할 수 있습니다. 

텐서플로 라이트는 다음과 같은 문제를 해결할 수 있는 모델을 tflite 파일로 제공합니다. 

| 문제        | 설명                                                         | 모델                                                         |
| ----------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 이미지 분류 | 이미지가 사람, 동물, 사물, 활동, 장소 등 어떤 이미지에 속하는 지 예측 | Inception<br />ResNet<br />DenseNet<br />SqueezeNet<br />MobileNet<br />NasNet Mobile<br />MnasNet |
| 객체 탐지   | 이미지 안에서 객체의 영역 예측                               | COCO SSD MobileNet V1                                        |
| 이미지 분할 | 이미지의 각 픽셀이 어떤 클래스에 속하는 지 예측              | Deeplab v3                                                   |
| 자세 추정   | 신체의 주요 관절 위치를 추정하여 사람의 포즈 예측            | posenet                                                      |
| 스타일 변환 | 입력 이미지에 스타일 이미지를 합성하여 입력 이미지의 스타일 변환 | MobileNet (예측)<br />Style Transform Model(변환)            |
| 텍스트 분류 | 텍스트가 어떤 클래스에 속하는 지 예측                        | Mobile BERT                                                  |
| 질문과 답변 | 질문의 의도를 파악하여 주어진 자료에서 답변을 찾아 제공      | Mobile BERT                                                  |
| 스마트 변환 | 사용자와 대화를 하는 챗봇                                    | Smart Reply Model                                            |

<br>

### 모델 개발

여기서는 텐서플로 모델을 이용하는 방법에 대해 알아봅니다. 

ImageNet 데이터로 학습된 MobileNet V2를 사용합니다. MobileNet은 기존 합성곱 신경망의 합성곱 연산을 깊이 분할 합성곱 연산으로 변경하여 기존 합성곱 신경망 모델 대비 계산량을 낮춘 모델로 약 8~9 배의 높은 효율을 보입니다. 

테스트 데이터로는 [pixabay.com](pixabay.com)에서 다운로드 받은 이미지를 사용합니다. images 디렉터리 안에 저장되어 있습니다. 

<br>

##### 1. 테스트 데이터 준비


```python
from PIL import Image
import os
import numpy as np

# 이미지 경로 지정
data_dir = "./images/"
files = os.listdir(data_dir)

images = []
for file in files:
    path = os.path.join(data_dir, file)
    images.append(np.array(Image.open(path))) # 이미지를 np.array 형태로 가져옴
```


```python
print(images[0])
print(images[0].shape)
```

    [[[ 17  90 122]
      [  4  73 106]
      [ 18  83 115]
      ...
      [ 16  81 119]
      [  2  65  98]
      [ 13  74 105]]
    
     [[  0  69 101]
      [ 20  91 123]
      [ 30  95 127]
      ...
      [  5  70 108]
      [ 20  83 116]
      [  9  70 101]]
    
     [[ 13  86 119]
      [ 17  88 120]
      [ 27  93 125]
      ...
      [  0  62  98]
      [ 17  80 113]
      [  1  62  93]]
    
     ...
    
     [[204 223 230]
      [204 223 230]
      [206 223 231]
      ...
      [207 224 232]
      [201 218 226]
      [199 216 224]]
    
     [[204 223 230]
      [204 223 230]
      [205 222 230]
      ...
      [203 220 230]
      [204 221 231]
      [207 224 234]]
    
     [[205 224 231]
      [205 224 231]
      [206 223 231]
      ...
      [203 220 230]
      [195 212 222]
      [192 209 219]]]
    (1278, 1920, 3)

<br>

##### 2. 데이터 전처리

데이터를 모델에 입력하기 전에는 반드시 모델이 요구하는 입력 형태로 변환하는 전처리 과정이 필요합니다. 


```python
import tensorflow as tf

# MobileNet V2의 기본 입력 텐서 형태는 (224, 224, 3)
# 0으로 초기화된 array 생성
resized_images = np.array(np.zeros((len(images), 224, 224, 3)))
for i in range(len(images)):
    # 이미지 크기를 변환하여 array에 담는다. 
    resized_images[i] = tf.image.resize(images[i], (224, 224))

# mobilenet_v2 모듈에 포함된 preprocess_input() 메서드를 사용하여 입력값을 전처리
preprocessed_images = tf.keras.applications.mobilenet_v2.preprocess_input(resized_images)
```

<br>

##### 3. 모델 생성 및 추론


```python
# 모델 불러오기
# weights = "imagenet": 이미지넷으로 학습된 데이터 모델 불러오기
mobilenet_imagenet_model = tf.keras.applications.MobileNetV2(weights="imagenet")

# 예측 결과
y_pred = mobilenet_imagenet_model.predict(preprocessed_images)
# 결과값 해석
topK = 1
y_pred_top = tf.keras.applications.mobilenet_v2.decode_predictions(y_pred, top=topK)
```

- predict(): 케라스 애플리케이션 모델의 예측 메서드
- mobilenet_v2.decode_predictions(): 모바일 넷 추론 결과를 해석하기 위한 메서드
    - topK: 가장 확률이 높은 상위 n개의 클래스 반환


```python
y_pred.shape # 16개 이미지, 1000개 클래스
```




    (16, 1000)




```python
y_pred_top[0]
```




    [('n01622779', 'great_grey_owl', 0.4463733)]

<br>


```python
# 추론 결과 확인
from matplotlib import pyplot as plt
import numpy as np

for i in range(len(images)):
    plt.imshow(images[i])
    plt.show()
    
    for k in range(topK):
        print("{0}: {1} %".format(y_pred_top[i][k][1], round(y_pred_top[i][k][2] * 100, 1)))
```


![output_13_0](https://user-images.githubusercontent.com/70505378/128633652-f0d75107-cf6d-4a6d-ac69-5aa4f572ea6e.png)
    


    great_grey_owl: 44.6 %




![output_13_2](https://user-images.githubusercontent.com/70505378/128633658-be27fdb9-2bbd-4707-a7d0-3b55c5d44689.png)
    


    matchstick: 58.2 %




![output_13_4](https://user-images.githubusercontent.com/70505378/128633661-fdd8bb86-c3dd-4020-ab83-fcedaa52dde7.png)
    


    black-and-tan_coonhound: 74.8 %




![output_13_6](https://user-images.githubusercontent.com/70505378/128633663-730dc7de-656d-4271-b72f-210cbdfebf32.png)
    


    African_elephant: 20.3 %




![output_13_8](https://user-images.githubusercontent.com/70505378/128633665-3c321e7a-5e1a-4d65-8279-07e652197956.png)
    


    pot: 26.2 %




![output_13_10](https://user-images.githubusercontent.com/70505378/128633666-ddddf38a-9885-4cd7-be2f-93dd568499ba.png)
    


    soup_bowl: 31.4 %




![output_13_12](https://user-images.githubusercontent.com/70505378/128633668-3e7d1bd3-b9b4-4738-976d-d5a5ce18824a.png)
    


    lion: 83.5 %




![output_13_14](https://user-images.githubusercontent.com/70505378/128633669-d42e13a3-6441-4fbc-860c-303e6376ffcc.png)
    


    orange: 94.5 %




![output_13_16](https://user-images.githubusercontent.com/70505378/128633670-cf465d9b-bd72-475f-90c8-bf3c0c68f77c.png)
    


    plate: 34.6 %




![output_13_18](https://user-images.githubusercontent.com/70505378/128633671-8a47e7e1-d31f-489f-aea5-76bdbaafd221.png)
    


    macaw: 47.8 %




![output_13_20](https://user-images.githubusercontent.com/70505378/128633673-dcbfe4c2-5080-403a-bd14-a4faa1951f4b.png)
    


    carbonara: 91.9 %




![output_13_22](https://user-images.githubusercontent.com/70505378/128633674-7d15b9a1-5560-4c07-897e-67301697e2e5.png)
    


    soup_bowl: 61.4 %




![output_13_24](https://user-images.githubusercontent.com/70505378/128633675-79094e14-c597-49a7-b96d-4ceaaf8365e1.png)
    


    grey_fox: 26.6 %




![output_13_26](https://user-images.githubusercontent.com/70505378/128633678-a2abac0c-c934-446b-a2de-33406394e339.png)
    


    strawberry: 93.6 %




![output_13_28](https://user-images.githubusercontent.com/70505378/128633680-9d6b1738-1653-4640-acd2-888ab05dc539.png)
    


    hip: 12.2 %




![output_13_30](https://user-images.githubusercontent.com/70505378/128633682-5647bce2-a103-453e-9452-122bd8aeb2ff.png)
    


    plate: 18.6 %

